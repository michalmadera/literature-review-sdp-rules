
@article{bhushan_classifying_2021,
	title = {Classifying and resolving software product line redundancies using an ontological first-order logic rule based method},
	volume = {168},
	issn = {09574174},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099231625&doi=10.1016%2fj.eswa.2020.114167&partnerID=40&md5=d3d7a804f51c4eeecd20365e7e7d4ad4},
	doi = {10.1016/j.eswa.2020.114167},
	abstract = {Software product line engineering improves software quality and diminishes development cost and time by efficiently developing software products. Its success lies in identifying the commonalities and variabilities of a set of software products which are generally modeled using feature models. The success of software product lines heavily relies upon the quality of feature models to derive high quality products. However, there are various defects that reduce profits of software product line. One of such defect is redundancy. While the majority of research work focuses on the identification of redundancies, their causes and corrections have been poorly explored. Causes and corrections must be as accurate and comprehensible as possible in order to support the developer in resolving the cause of a redundancy. This research work classified redundancies in the form of a typology. An ontological first-order logic rule based method is proposed to deal with redundancies. A two-step process is presented for mapping model to ontology based on predicate logic. First-order logic based rules are developed and applied to the generated ontology for identifying redundancies, their causes and corrections to resolve redundancies. The proposed method is illustrated using a case study from software product lines online tools repository. The results of experiments performed on 35 models with varied sizes of real world models as well as automatically-generated models from the Software Product Line Online Tools repository and models created via {FeatureIDE} tool conclude that the method is accurate, efficient and scalable with {FM} up to 30,000 features. Thus, enables deriving redundancy free end products from the product line and ultimately, improves its quality. © 2020 Elsevier Ltd},
	journaltitle = {Expert Systems with Applications},
	author = {Bhushan, M. and Ángel Galindo Duarte, J. and Samant, P. and Kumar, A. and Negi, A.},
	date = {2021},
	note = {Publisher: Elsevier Ltd},
	keywords = {Automatically generated, Computer circuits, Computer software selection and evaluation, Cost engineering, Defects, Development costs, Engineering research, First order logic, Formal logic, High-quality products, Ontology, Redundancy, Software design, Software Product Line, Software product line engineerings, Software products, Software quality, Two-step process, xyes}
}

@article{malhotra_predicting_2021,
	title = {Predicting Software Defects for Object-Oriented Software Using Search-based Techniques},
	volume = {31},
	issn = {02181940},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101961196&doi=10.1142%2fS0218194021500054&partnerID=40&md5=81449e0a65f756bf7d3cb03fc70e1e95},
	doi = {10.1142/S0218194021500054},
	abstract = {Development without any defect is unsubstantial. Timely detection of software defects favors the proper resource utilization saving time, effort and money. With the increasing size and complexity of software, demand for accurate and efficient prediction models is increasing. Recently, search-based techniques ({SBTs}) have fascinated many researchers for Software Defect Prediction ({SDP}). The goal of this study is to conduct an empirical evaluation to assess the applicability of {SBTs} for predicting software defects in object-oriented ({OO}) softwares. In this study, 16 {SBTs} are exploited to build defect prediction models for 13 {OO} software projects. Stable performance measures-{GMean}, Balance and Receiver Operating Characteristic-Area Under Curve ({ROC}-{AUC}) are employed to probe into the predictive capability of developed models, taking into consideration the imbalanced nature of software datasets. Proper measures are taken to handle the stochastic behavior of {SBTs}. The significance of results is statistically validated using the Friedman test complied with Wilcoxon post hoc analysis. The results confirm that software defects can be detected in the early phases of software development with help of {SBTs}. This paper identifies the effective subset of {SBTs} that will aid software practitioners to timely detect the probable software defects, therefore, saving resources and bringing up good quality softwares. Eight {SBTs}-{sUpervised} Classification System ({UCS}), Bioinformatics-oriented hierarchical evolutionary learning ({BIOHEL}), {CHC}, Genetic Algorithm-based Classifier System with Adaptive Discretization Intervals ({GA}-{ADI}), Genetic Algorithm-based Classifier System with Intervalar Rule ({GA}-{INT}), Memetic Pittsburgh Learning Classifier System ({MPLCS}), Population-Based Incremental Learning ({PBIL}) and Steady-State Genetic Algorithm for Instance Selection ({SGA}) are found to be statistically good defect predictors. © 2021 World Scientific Publishing Company.},
	pages = {193--215},
	number = {2},
	journaltitle = {International Journal of Software Engineering and Knowledge Engineering},
	author = {Malhotra, R. and Jain, J.},
	date = {2021},
	note = {Publisher: World Scientific},
	keywords = {Defects, Software design, Software defect prediction, Adaptive systems, Computer aided diagnosis, Defect prediction models, Forecasting, Genetic algorithms, Learning classifier system, Learning systems, Object oriented programming, Object oriented software, Population based incremental learning, Predictive analytics, Receiver operating characteristics, Steady-state genetic algorithms, Stochastic systems, Supervised classification, xno}
}

@article{mahmood_mining_2021,
	title = {Mining software repository for cleaning bugs using data mining technique},
	volume = {69},
	issn = {15462218},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107792741&doi=10.32604%2fcmc.2021.016614&partnerID=40&md5=5f9bc74622b43439b6c231bbc7001e10},
	doi = {10.32604/cmc.2021.016614},
	abstract = {Despite advances in technological complexity and efforts, software repository maintenance requires reusing the data to reduce the effort and complexity. However, increasing ambiguity, irrelevance, and bugs while extracting similar data during software development generate a large amount of data from those data that reside in repositories. Thus, there is a need for a repository mining technique for relevant and bug-free data prediction. This paper proposes a fault prediction approach using a data-mining technique to find good predictors for high-quality software. To predict errors in mining data, the Apriori algorithm was used to discover association rules by fixing confidence at more than 40\% and support at least 30\%. The pruning strategy was adopted based on evaluation measures. Next, the rules were extracted from three projects of different domains; the extracted rules were then combined to obtain the most popular rules based on the evaluation measure values. To evaluate the proposed approach, we conducted an experimental study to compare the proposed rules with existing ones using four different industrial projects. The evaluation showed that the results of our proposal are promising. Practitioners and developers can utilize these rules for defect prediction during early software development. © 2021 Tech Science Press. All rights reserved.},
	pages = {873--893},
	number = {1},
	journaltitle = {Computers, Materials and Continua},
	author = {Mahmood, N. and Hafeez, Y. and Iqbal, K. and Hussain, S. and Aqib, M. and Jamal, M. and Song, O.-Y.},
	date = {2021},
	note = {Publisher: Tech Science Press},
	keywords = {Data mining, Software design, Forecasting, Apriori algorithms, Different domains, Evaluation measures, High-quality software, Industrial projects, Mining software repositories, Program debugging, Software repositories, Technological complexity, xyes}
}

@article{alattas_system_2021,
	title = {System Error Estimate Using Combination of Classification and Optimization Technique},
	volume = {17},
	issn = {15493636},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106663763&doi=10.3844%2fJCSSP.2021.319.329&partnerID=40&md5=7433913cc913d137bde1533a7a2a53be},
	doi = {10.3844/JCSSP.2021.319.329},
	abstract = {The representation of software must produce flawless significances without any inadequacies. Software imperfection evaluations scheme determines defective mechanisms in software. The eventual creation would have minor or negligible shortcomings to harvest great eminence software. Software quality metrics are a division of software metrics that spotlight the quality aspects of the product. The software flaw prediction system helps in the early discovery of flaws and contributes to talented removal and producing a quality software system through numerous metrics. The aim of the paper was to show how static model of data mining is used to extract defects and the {PSO} algorithm. Another aim of the research was to develop an optimized software flaw prophecy system on data mining techniques namely Association Rule mining, Decision Tree, Naive Bayes and Classification integrated with Particle Swarm Optimization technique. The proposed software flaw prediction system is deliberated through Data Mining techniques with Particle Swarm Optimization algorithm has been verified and compared the results. This proposed system is very useful to identify the relationships between the quality metrics and the potential defective modules. The optimized data mining systems have pragmatic perfect prediction of these defective modules. In the future, optimized data mining systems can be improved by the use of different platforms and particularly by improving data mining using {PSO} algorithms. It is necessary to develop algorithms that can identify faults in advance, which will minimize costs and promote the quality of developed software systems. Future optimized data mining systems will improve the relationship between quality metrics and the potential defective modules, which will lead to improved performances, productivity and lower operation costs. © 2021 Khalid Alattas. This open access article is distributed under a Creative Commons Attribution ({CC}-{BY}) 4.0 license.},
	pages = {319--329},
	number = {3},
	journaltitle = {Journal of Computer Science},
	author = {Alattas, K.},
	date = {2021},
	note = {Publisher: Science Publications},
	keywords = {xno},
	file = {Full Text:C\:\\Users\\michalm\\Zotero\\storage\\METAXWBT\\Alattas - 2021 - System Error Estimate using Combination of Classif.pdf:application/pdf;Full Text:C\:\\Users\\michalm\\Zotero\\storage\\5KV5VF8Y\\Alattas - 2021 - System Error Estimate using Combination of Classif.pdf:application/pdf}
}

@article{wu_limcr_2020,
	title = {{LIMCR}: Less-informative majorities cleaning rule based on naïve bayes for imbalance learning in software defect prediction},
	volume = {10},
	issn = {20763417},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096559362&doi=10.3390%2fapp10238324&partnerID=40&md5=0b46eefed4e930bc94d9faf859b5c9c4},
	doi = {10.3390/app10238324},
	abstract = {Software defect prediction ({SDP}) is an effective technique to lower software module testing costs. However, the imbalanced distribution almost exists in all {SDP} datasets and restricts the accuracy of defect prediction. In order to balance the data distribution reasonably, we propose a novel resampling method {LIMCR} on the basis of Naïve Bayes to optimize and improve the {SDP} performance. The main idea of {LIMCR} is to remove less-informative majorities for rebalancing the data distribution after evaluating the degree of being informative for every sample from the majority class. We employ 29 {SDP} datasets from the {PROMISE} and {NASA} dataset and divide them into two parts, the small sample size (the amount of data is smaller than 1100) and the large sample size (larger than 1100). Then we conduct experiments by comparing the matching of classifiers and imbalance learning methods on small datasets and large datasets, respectively. The results show the effectiveness of {LIMCR}, and {LIMCR}+{GNB} performs better than other methods on small datasets while not brilliant on large datasets. © 2020 by the authors. Licensee {MDPI}, Basel, Switzerland.},
	pages = {1--24},
	number = {23},
	journaltitle = {Applied Sciences (Switzerland)},
	author = {Wu, Y. and Yao, J. and Chang, S. and Liu, B.},
	date = {2020},
	note = {Publisher: {MDPI} {AG}},
	keywords = {xno},
	file = {Full Text:C\:\\Users\\michalm\\Zotero\\storage\\4Q3KYT3P\\Wu et al. - 2020 - LIMCR Less-Informative Majorities Cleaning Rule B.pdf:application/pdf;Full Text:C\:\\Users\\michalm\\Zotero\\storage\\CBEM3TC5\\Wu et al. - 2020 - LIMCR Less-Informative Majorities Cleaning Rule B.pdf:application/pdf}
}

@article{baum_gimo_2020,
	title = {{GIMO}: A multi-objective anytime rule mining system to ease iterative feedback from domain experts},
	volume = {8},
	issn = {25901885},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090830401&doi=10.1016%2fj.eswax.2020.100040&partnerID=40&md5=0c2a7764b294fd08cc3c164779f2860d},
	doi = {10.1016/j.eswax.2020.100040},
	abstract = {Data extracted from software repositories is used intensively in Software Engineering research, for example, to predict defects in source code. In our research in this area, with data from open source projects as well as an industrial partner, we noticed several shortcomings of conventional data mining approaches for classification problems: (1) Domain experts’ acceptance is of critical importance, and domain experts can provide valuable input, but it is hard to use this feedback. (2) Evaluating the quality of the model is not a matter of calculating {AUC} or accuracy. Instead, there are multiple objectives of varying importance with hard to quantify trade-offs. Furthermore, the performance of the model cannot be evaluated on a per-instance level in our case, because it shares aspects with the set cover problem. To overcome these problems, we take a holistic approach and develop a rule mining system that simplifies iterative feedback from domain experts and can incorporate the domain-specific evaluation needs. A central part of the system is a novel multi-objective anytime rule mining algorithm. The algorithm is based on the {GRASP}-{PR} meta-heuristic but extends it with ideas from several other approaches. We successfully applied the system in the industrial context. In the current article, we focus on the description of the algorithm and the concepts of the system. We make an implementation of the system available. © 2020 The Authors},
	journaltitle = {Expert Systems with Applications: X},
	author = {Baum, T. and Herbold, S. and Schneider, K.},
	date = {2020},
	note = {Publisher: Elsevier Ltd},
	keywords = {Data mining, Open source software, Software repositories, Rule mining, Conventional data mining, Economic and social effects, Electronic trading, Holistic approach, Industrial context, Industrial partners, Industrial research, Iterative methods, Knowledge acquisition, Mining machinery, Multiple-objectives, Open source projects, Rule mining algorithms, Explainable artificial intelligence, Human-in-the-loop, Interpretable artificial intelligence, Multi-objective, Set cover, xyes},
	file = {Full Text:C\:\\Users\\michalm\\Zotero\\storage\\MQY3F42D\\Baum et al. - 2020 - GIMO A multi-objective anytime rule mining system.pdf:application/pdf;Full Text:C\:\\Users\\michalm\\Zotero\\storage\\RKJXJ5TB\\Baum et al. - 2020 - GIMO A multi-objective anytime rule mining system.pdf:application/pdf}
}

@article{kuutti_numerical_2020,
	title = {Numerical assessment of the effects of microcrack interaction in {AM} components},
	volume = {184},
	issn = {09270256},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086632849&doi=10.1016%2fj.commatsci.2020.109882&partnerID=40&md5=d80c0eceffcc34dd0d17afc3ddd8569f},
	doi = {10.1016/j.commatsci.2020.109882},
	abstract = {A combined analytical–numerical method to study the effects of linear microcracks and their interaction in additively manufactured components is presented. The 2-D method combines an analytical technique to solve the interaction of microcracks and a numerical {RVE} type technique to represent the microcracking within a finite element framework using Abaqus finite element software. The method is applied to both a unit cell and test specimen type geometries containing defect patterns generated based on general trends reported for {AM} materials. The approach is able to determine the local stress intensity factors for each microcrack and their stiffness degradation effects in the continuum. Parallel defect patterns, such as co-oriented lack-of-fusion defects between build layers in {AM} materials, induce the greatest interaction effects while overall interaction effects in random patterns tend to cancel out. Stacked surface defects produce shielding effects on each other, which may cause a neighbouring subsurface defect to be more critical than the surface defects. The results show that the common geometrical interacting defect re-characterisation rules may provide an incorrect prediction of the failure origin. Finally, the applicability of the method is demonstrated with an example {AM} component. © 2020 Elsevier B.V.},
	journaltitle = {Computational Materials Science},
	author = {Kuutti, J. and Kolari, K.},
	date = {2020},
	note = {Publisher: Elsevier B.V.},
	keywords = {{ABAQUS}, Abaqus finite element software, Defect patterns, Interacting defects, Interaction effect, Micro-crack interactions, Microcracks, Numerical methods, Shielding effect, Stiffness degradation, Subsurface defect, Surface defects, xno}
}

@inproceedings{lima_understanding_2020,
	title = {Understanding and Detecting Harmful Code},
	isbn = {978-1-4503-8753-8},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099349813&doi=10.1145%2f3422392.3422420&partnerID=40&md5=f9cecd104b5052f5286e034b9e953337},
	doi = {10.1145/3422392.3422420},
	abstract = {Code smells typically indicate poor design implementation and choices that may degrade software quality. Hence, they need to be carefully detected to avoid such poor design. In this context, some studies try to understand the impact of code smells on the software quality, while others propose rules or machine learning-based techniques to detect code smells. However, none of those studies or techniques focus on analyzing code snippets that are really harmful to software quality. This paper presents a study to understand and classify code harmfulness. We analyze harmfulness in terms of {CLEAN}, {SMELLY}, {BUGGY}, and {HARMFUL} code. By {HARMFUL} {CODE}, we define a {SMELLY} code element having one or more bugs reported. These bugs may have been fixed or not. Thus, the incidence of {HARMFUL} {CODE} may represent a increased risk of introducing new defects and/or design problems during its fixing. We perform our study with 22 smell types, 803 versions of 13 open-source projects, 40,340 bugs and 132,219 code smells. The results show that even though we have a high number of code smells, only 0.07\% of those smells are harmful. The Abstract Function Call From Constructor is the smell type more related to {HARMFUL} {CODE}. To cross-validate our results, we also perform a survey with 60 developers. Most of them (98\%) consider code smells harmful to the software, and 85\% of those developers believe that code smells detection tools are important. But, those developers are not concerned about selecting tools that are able to detect {HARMFUL} {CODE}. We also evaluate machine learning techniques to classify code harmfulness: They reach the effectiveness of at least 97\% to classify {HARMFUL} {CODE}. While the Random Forest is effective in classifying both {SMELLY} and {HARMFUL} {CODE}, the Gaussian Naive Bayes is the less effective technique. Our results also suggest that both software and developers' metrics are important to classify {HARMFUL} {CODE}. © 2020 {ACM}.},
	pages = {223--232},
	booktitle = {{ACM} International Conference Proceeding Series},
	publisher = {Association for Computing Machinery},
	author = {Lima, R. and Souza, J. and Fonseca, B. and Teixeira, L. and Gheyi, R. and Ribeiro, M. and Garcia, A. and De Mello, R.},
	date = {2020},
	keywords = {Computer software selection and evaluation, Software quality, Open source software, Decision trees, Open source projects, Code smell, Design implementation, Design problems, Detection tools, Function calls, Machine learning, Machine learning techniques, Naive bayes, Odors, Open systems, xno}
}

@article{zhou_multi-agent_2020,
	title = {A Multi-Agent Simulation Method of Urban Land Layout Structure Based on {FPGA}},
	volume = {25},
	issn = {1383469X},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072178973&doi=10.1007%2fs11036-019-01361-0&partnerID=40&md5=d25d050e3bacfd8c023f7d0370775f20},
	doi = {10.1007/s11036-019-01361-0},
	abstract = {The unavoidable birth defects for current simulation method make the final simulation results cannot truly reflect the evolution rules of urban land layout. In this way, a multi-agent simulation method based on {FPGA} for urban land layout is proposed in this paper. The evolution rule of urban eco-land is explored by combination of cellular automata, dynamic reconfiguration and multi-agent methods. Relying on platform of {MATLAB} software and dynamic reconfiguration of {FPGA} logic resources, a regional {ANN}-{CA}-Agent model for evolution and prediction model of urban land layout is established. {MATLAB}, {FPGA}, and {ArcGIS} are interoperable by programming, and foreground is displayed by the repast tool. The input layer contains 18 data layers, and the output layer contains 6 data layers. A multi-agent model is established for studying the evolution of urban land layout structures. Finally, in order to make calculation results of the model more in line with actual situation and reflect uncertainty of urban system, random factors are added. Example analysis results show that simulation accuracy of the proposed land layout structure reaches 92.4\%, which is a high simulation accuracy. Moreover, conclusion shows that speed of urban expansion in the study area from 2007 to 2029 has gradually slowed down, and urban land-use pattern has changed from epitaxial expansion to intensive land-use. © 2019, Springer Science+Business Media, {LLC}, part of Springer Nature.},
	pages = {1572--1581},
	number = {4},
	journaltitle = {Mobile Networks and Applications},
	author = {Zhou, X. and Fu, W.},
	date = {2020},
	note = {Publisher: Springer},
	keywords = {Calculation results, Dynamic models, Dynamic re-configuration, Field programmable gate arrays ({FPGA}), Integrated circuit layout, Intensive land use, Land use, Layout structure, {MATLAB}, Multi agent, Multi agent simulation, Multi agent systems, Simulation, Simulation accuracy, Software agents, xno}
}

@article{abaei_fuzzy_2020,
	title = {A fuzzy logic expert system to predict module fault proneness using unlabeled data},
	volume = {32},
	issn = {13191578},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053032476&doi=10.1016%2fj.jksuci.2018.08.003&partnerID=40&md5=dc1ca5a715bad9a6287aa6e72f63f14d},
	doi = {10.1016/j.jksuci.2018.08.003},
	abstract = {Several techniques have been proposed to predict the fault proneness of software modules in the absence of fault data. However, the application of these techniques requires an expert assistant and is based on fixed thresholds and rules, which potentially prevents obtaining optimal prediction results. In this study, the development of a fuzzy logic expert system for predicting the fault proneness of software modules is demonstrated in the absence of fault data. The problem of strong dependability with the prediction model for expert assistance as well as deciding on the module fault proneness based on fixed thresholds and fixed rules have been solved in this study. In fact, involvement of experts is more relaxed or provides more support now. Two methods have been proposed and implemented using the fuzzy logic system. In the first method, the Takagi and Sugeno-based fuzzy logic system is developed manually. In the second method, the rule-base and data-base of the fuzzy logic system are adjusted using a genetic algorithm. The second method can determine the optimal values of the thresholds while recommending the most appropriate rules to guide the testing of activities by prioritizing the module's defects to improve the quality of software testing with a limited budget and limited time. Two datasets from {NASA} and the Turkish white-goods manufacturer that develops embedded controller software are used for evaluation. The results based on the second method show improvement in the false negative rate, f-measure, and overall error rate. To obtain optimal prediction results, developers and practitioners are recommended to apply the proposed fuzzy logic expert system for predicting the fault proneness of software modules in the absence of fault data. © 2018 The Authors},
	pages = {684--699},
	number = {6},
	journaltitle = {Journal of King Saud University - Computer and Information Sciences},
	author = {Abaei, G. and Selamat, A. and Al Dallal, J.},
	date = {2020},
	note = {Publisher: King Saud bin Abdulaziz University},
	keywords = {xyes}
}

@article{shao_software_2020,
	title = {Software defect prediction based on correlation weighted class association rule mining},
	volume = {196},
	issn = {09507051},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082463500&doi=10.1016%2fj.knosys.2020.105742&partnerID=40&md5=aafe1f60cfd93765b177edb265fe468d},
	doi = {10.1016/j.knosys.2020.105742},
	abstract = {Software defect prediction based on supervised learning plays a crucial role in guiding software testing for resource allocation. In particular, it is worth noticing that using associative classification with high accuracy and comprehensibility can predict defects. But owing to the imbalance data distribution inherent, it is easy to generate a large number of non-defective class association rules, but the defective class association rules are easily ignored. Furthermore, classical associative classification algorithms mainly measure the interestingness of rules by the occurrence frequency, such as support and confidence, without considering the importance of features, resulting in combinations of the insignificant frequent itemset. This promotes the generation of weighted associative classification. However, the feature weighting based on domain knowledge is subjective and unsuitable for a high dimensional dataset. Hence, we present a novel software defect prediction model based on correlation weighted class association rule mining ({CWCAR}). It leverages a multi-weighted supports-based framework rather than the traditional support-confidence approach to handle class imbalance and utilizes the correlation-based heuristic approach to assign feature weight. Besides, we also optimize the ranking, pruning and prediction stages based on weighted support. Results show that {CWCAR} is significantly superior to state-of-the-art classifiers in terms of Balance, {MCC}, and Gmean. © 2020 Elsevier B.V.},
	journaltitle = {Knowledge-Based Systems},
	author = {Shao, Y. and Liu, B. and Wang, S. and Li, G.},
	date = {2020},
	note = {Publisher: Elsevier B.V.},
	keywords = {Data mining, Defects, Software defect prediction, Forecasting, Apriori, Association rules, Associative classification, Attribute weighting, Class imbalance, Heuristic methods, Software testing, Association rule, xyes}
}

@article{thakur_spark_2020,
	title = {Spark and Rule-{KNN} based scalable machine learning framework for {EEG} deceit identification},
	volume = {58},
	issn = {17468094},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079317663&doi=10.1016%2fj.bspc.2020.101886&partnerID=40&md5=29f60031b0af8fca34f85fc220bc18c8},
	doi = {10.1016/j.bspc.2020.101886},
	abstract = {Brain computer interface ({BCI}) provides communication between the computer and the brain. It is the combination of hardware and software which provides non-muscular channel to send the various messages to control the computer. {BCI} is useful in various medical applications such as patients with neuromuscular injuries, locked-in syndrome ({LiS}) etc. {BCI} is not only useful in medical applications, but also useful in lie detection, entertainment, etc. In this paper, spark and rule-{KNN} based scalable framework has been presented using {BCI} with the {EEG} data collected on 20 subjects in which 10 are acted as innocent and 10 are acted as guilty. Using {BCI} P300, Deceit identification Test ({DIT}) is performed. To perform {DIT}, we classify the P300 signals which have a positive peak of 300 ms–1000 ms in one stimulus start. Data processing is performed with band pass filter to cut the frequency ranges and features are extracted using non-parametric weighted feature extraction followed by rule based discriminant classification. For training and testing, the data ratio selected as 80:20 and achieved the accuracy 92.46 \%. Proposed framework provides better results in comparison with existing models presented in literature. Hence this model is accurate, scalable and fault tolerant. © 2020 Elsevier Ltd},
	journaltitle = {Biomedical Signal Processing and Control},
	author = {Thakur, S. and Dharavath, R. and Edla, D.R.},
	date = {2020},
	note = {Publisher: Elsevier Ltd},
	keywords = {Data mining, Feature extraction, Machine learning, adult, Article, Bandpass filters, brain computer interface, Brain computer interface, cognitive function test, comparative study, controlled study, Data handling, deceit identification test, deception, Discriminant classifications, Eeg datum, electroencephalogram, event related potential, Extraction, feature extraction, Frequency ranges, Hardware and software, human, k nearest neighbor, Lithium compounds, Locked-in syndrome, Medical applications, Non-parametric, normal human, priority journal, Scalable machine learning, signal processing, Training and testing, xno}
}

@article{rao_novel_2020,
	title = {A novel under sampling strategy for efficient software defect analysis of skewed distributed data},
	volume = {11},
	issn = {18686478},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081312073&doi=10.1007%2fs12530-018-9261-9&partnerID=40&md5=1e2564b24c263195b3e640a10872bf1a},
	doi = {10.1007/s12530-018-9261-9},
	abstract = {The software quality development process is a continuous process which starts by identifying a reliable fault detection technique. The implementation of the effective fault detection technique depends on the properties of the dataset in terms of domain information, characteristics of input data, complexity, etc. The early detection of defective modules provide more time for the developers to allocate resources effectively to deliver the quality software in time. The class imbalance nature of the software defect datasets indicates that the existing techniques are unsuccessful for identifying all the defective modules. Misclassification of the defective modules in the software engineering datasets invites unexpected loses to the software developers. To classify the class imbalance software datasets in an efficient way, we have proposed a novel approach called as under sampling strategy. This proposed approach uses under sampling strategy to reduce the less prominent instances from majority subset. The experimental results confirm that the proposed approach can deliver more accuracy in predicting the modules which are error prone with less and simple rules. © 2019, Springer-Verlag {GmbH} Germany, part of Springer Nature.},
	pages = {119--131},
	number = {1},
	journaltitle = {Evolving Systems},
	author = {Rao, K.N. and Reddy, C.S.},
	date = {2020},
	note = {Publisher: Springer},
	keywords = {Computer software selection and evaluation, Defects, Software quality, Software defects, Classification (of information), Decision trees, Class imbalance learning, Continuous process, Development process, Domain informations, Fault detection, Fault detection techniques, Software developer, Under-sampling, xno}
}

@article{chen_modeling_2020,
	title = {Modeling and reasoning of {IoT} architecture in semantic ontology dimension},
	volume = {153},
	issn = {01403664},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079849963&doi=10.1016%2fj.comcom.2020.02.006&partnerID=40&md5=691e8881ffcbb75bad3e5a62acdfd69a},
	doi = {10.1016/j.comcom.2020.02.006},
	abstract = {The architecture for {IoT} is the primary foundation for designing and implementing the System of Internet of things. This paper discusses the theory, method, tools and practice of modeling and reasoning the architecture of the Internet of Things system from the dimension of semantic ontology. This paper breaks the way of static ontology modeling, and proposes an implementation framework for real-time and dynamic ontology modeling of the {IoT} system from the running system. According to the actual needs of the health cabin {IoT} system and the combination of theory and practice, the system architecture model of the semantic ontology dimension of {IoT} is built. Then, based on the reasoning rules of the ontology model, the model is reasoned by Pellet reasoning engine which injects the atom of the custom reasoning built-ins into the source code. In this way we have realized the automatic classification and attribute improvement of resources and behaviors of the {IoT} system, the real-time working state detection and fault diagnosis of the {IoT} system, and the automatic control of the {IoT} system and resources. © 2020 The Authors},
	pages = {580--594},
	journaltitle = {Computer Communications},
	author = {Chen, G. and Jiang, T. and Wang, M. and Tang, X. and Ji, W.},
	date = {2020},
	note = {Publisher: Elsevier B.V.},
	keywords = {Ontology, Computer aided diagnosis, Semantics, Fault detection, Architecture, Automatic classification, Automation, Computer architecture, Dynamic ontologies, Internet of things, Iot architectures, Models, Reasoning, Semantic ontology, System architectures, Theory and practice, Tools and practices, xno}
}

@article{barbez_machine-learning_2020,
	title = {A machine-learning based ensemble method for anti-patterns detection},
	volume = {161},
	issn = {01641212},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076559059&doi=10.1016%2fj.jss.2019.110486&partnerID=40&md5=ec8a0ff172eff58af69f033014cacec9},
	doi = {10.1016/j.jss.2019.110486},
	abstract = {Anti-patterns are poor solutions to recurring design problems. Several empirical studies have highlighted their negative impact on program comprehension, maintainability, as well as fault-proneness. A variety of detection approaches have been proposed to identify their occurrences in source code. However, these approaches can identify only a subset of the occurrences and report large numbers of false positives and misses. Furthermore, a low agreement is generally observed among different approaches. Recent studies have shown the potential of machine-learning models to improve this situation. However, such algorithms require large sets of manually-produced training-data, which often limits their application in practice. In this paper, we present {SMAD} ({SMart} Aggregation of Anti-patterns Detectors), a machine-learning based ensemble method to aggregate various anti-patterns detection approaches on the basis of their internal detection rules. Thus, our method uses several detection tools to produce an improved prediction from a reasonable number of training examples. We implemented {SMAD} for the detection of two well known anti-patterns: God Class and Feature Envy. With the results of our experiments conducted on eight java projects, we show that: (1) Our method clearly improves the so aggregated tools; (2) {SMAD} significantly outperforms other ensemble methods. © 2019},
	journaltitle = {Journal of Systems and Software},
	author = {Barbez, A. and Khomh, F. and Guéhéneuc, Y.-G.},
	date = {2020},
	note = {Publisher: Elsevier Inc.},
	keywords = {Computer software selection and evaluation, Software quality, Empirical studies, Learning systems, Machine learning, Anti-patterns, Detection approach, Ensemble methods, Machine learning models, Pattern recognition, Program comprehension, Software Quality, Training example, xno},
	file = {Submitted Version:C\:\\Users\\michalm\\Zotero\\storage\\8J6XU45L\\Barbez et al. - 2020 - A machine-learning based ensemble method for anti-.pdf:application/pdf}
}

@article{ali_software_2020,
	title = {Software defect prediction using variant based ensemble learning and feature selection techniques},
	volume = {12},
	issn = {20750161},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094560272&doi=10.5815%2fijmecs.2020.05.03&partnerID=40&md5=1c2a04219f7bd2ae863f7bba1d2cd376},
	doi = {10.5815/ijmecs.2020.05.03},
	abstract = {Testing is considered as one of the expensive activities in software development process. Fixing the defects during testing process can increase the cost as well as the completion time of the project. Cost of testing process can be reduced by identifying the defective modules during the development (before testing) stage. This process is known as “Software Defect Prediction”, which has been widely focused by many researchers in the last two decades. This research proposes a classification framework for the prediction of defective modules using variant based ensemble learning and feature selection techniques. Variant selection activity identifies the best optimized versions of classification techniques so that their ensemble can achieve high performance whereas feature selection is performed to get rid of such features which do not participate in classification and become the cause of lower performance. The proposed framework is implemented on four cleaned {NASA} datasets from {MDP} repository and evaluated by using three performance measures, including: F-measure, Accuracy, and {MCC}. According to results, the proposed framework outperformed 10 widely used supervised classification techniques, including: “Naïve Bayes ({NB}), Multi-Layer Perceptron ({MLP}), Radial Basis Function ({RBF}), Support Vector Machine ({SVM}), K Nearest Neighbor ({KNN}), {kStar} (K*), One Rule ({OneR}), {PART}, Decision Tree ({DT}), and Random Forest ({RF})”. © 2020 {MECS}.},
	pages = {29--40},
	number = {5},
	journaltitle = {International Journal of Modern Education and Computer Science},
	author = {Ali, U. and Aftab, S. and Iqbal, A. and Nawaz, Z. and Bashir, M.S. and Saeed, M.A.},
	date = {2020},
	note = {Publisher: Modern Education and Computer Science Press},
	keywords = {xno},
	file = {Full Text:C\:\\Users\\michalm\\Zotero\\storage\\74C7LISE\\Department of Computer Science, Virtual University of Pakistan, Lahore, Pakistan et al. - 2020 - Software Defect Prediction Using Variant based Ens.pdf:application/pdf}
}

@article{liu_programming_2020,
	title = {Programming logic modeling and cross-program defect detection method for object-oriented code},
	volume = {64},
	issn = {15462218},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090883454&doi=10.32604%2fCMC.2020.09659&partnerID=40&md5=4fa681e6f6c363ecc2deda4e85575a01},
	doi = {10.32604/CMC.2020.09659},
	abstract = {Code defects can lead to software vulnerability and even produce vulnerability risks. Existing research shows that the code detection technology with text analysis can judge whether object-oriented code files are defective to some extent. However, these detection techniques are mainly based on text features and have weak detection capabilities across programs. Compared with the uncertainty of the code and text caused by the developer’s personalization, the programming language has a stricter logical specification, which reflects the rules and requirements of the language itself and the developer’s potential way of thinking. This article replaces text analysis with programming logic modeling, breaks through the limitation of code text analysis solely relying on the probability of sentence/word occurrence in the code, and proposes an object-oriented language programming logic construction method based on method constraint relationships, selecting features through hypothesis testing ideas, and construct support vector machine classifier to detect class files with defects and reduce the impact of personalized programming on detection methods. In the experiment, some representative Android applications were selected to test and compare the proposed methods. In terms of the accuracy of code defect detection, through cross validation, the proposed method and the existing leading methods all reach an average of more than 90\%. In the aspect of cross program detection, the method proposed in this paper is superior to the other two leading methods in accuracy, recall and F1 value. © 2020 Tech Science Press. All rights reserved.},
	pages = {273--295},
	number = {1},
	journaltitle = {Computers, Materials and Continua},
	author = {Liu, Y. and Fang, W. and Wei, Q. and Zhao, Y. and Wang, L.},
	date = {2020},
	note = {Publisher: Tech Science Press},
	keywords = {Feature extraction, Computer circuits, Defects, Object oriented programming, Android applications, Codes (symbols), Construction method, Defect detection method, Detection capability, Logical specifications, Machine oriented languages, Modeling languages, Object detection, Object-oriented code, Software vulnerabilities, Support vector machine classifiers, Support vector machines, Text mining, xno}
}

@article{singh_aco_2020,
	title = {{ACO} based comprehensive model for software fault prediction},
	volume = {24},
	issn = {13272314},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083431176&doi=10.3233%2fKES-200029&partnerID=40&md5=904e8af117f073b904baa7e28d13e22e},
	doi = {10.3233/KES-200029},
	abstract = {The comprehensive models can be used for software quality modelling which involves prediction of low-quality modules using interpretable rules. Such comprehensive model can guide the design and testing team to focus on the poor quality modules, thereby, limited resources allocated for software quality inspection can be targeted only towards modules that are likely to be defective. Ant Colony Optimization ({ACO}) based learner is one potential way to obtain rules that can classify the software modules faulty and not faulty. This paper investigates {ACO} based mining approach with {ROC} based rule quality updation to constructs a rule-based software fault prediction model with useful metrics. We have also investigated the effect of feature selection on {ACO} based and other benchmark algorithms. We tested the proposed method on several publicly available software fault data sets. We compared the performance of {ACO} based learning with the results of three benchmark classifiers on the basis of area under the receiver operating characteristic curve. The evaluation of performance measure proves that the {ACO} based learner outperforms other benchmark techniques. © 2020 - {IOS} Press and the authors. All rights reserved.},
	pages = {63--71},
	number = {1},
	journaltitle = {International Journal of Knowledge-Based and Intelligent Engineering Systems},
	author = {Singh, P. and Verma, S.},
	date = {2020},
	note = {Publisher: {IOS} Press},
	keywords = {Computer software selection and evaluation, Software quality, Forecasting, Software testing, Software fault, Ant colony optimization, Ant Colony Optimization ({ACO}), Artificial intelligence, Benchmarking, Comprehensive model, Interpretable rules, Performance measure, Receiver operating characteristic curves, Software fault prediction, Software modules, xno}
}

@article{yangyuen_development_2020,
	title = {The development of dt-nb hybrid algorithms for classifying some defective dataset types for software quality prediction},
	volume = {10},
	issn = {20103689},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077164644&doi=10.18178%2fijiet.2020.10.1.1330&partnerID=40&md5=a60ba2288a06cdb75d65de1f51d3775e},
	doi = {10.18178/ijiet.2020.10.1.1330},
	abstract = {This research is the presenting the development of hybrid algorithms that is called {DT}-{NB} or Decision Tree – Naïve Bayesian to predict about the software quality. Besides, it can develop the new technique of data mining for software industry or the current software engineering. Then, these techniques to make the comparison are Decision Tree, Rule-Based and Naïve Bayesian. Similarly, according to the analysis result of making the comparison for quality planning, it was found that the technique {DT}-{NB} had the correctness result of96 percent. Additionally, it was found that according to the analysis result of making comparison for quality assurance, the technique of {DT}-{NB} had the correctness result with76percent. On the same way, according to the result analysis of making comparison of quality control, it was found that the technique of {DT}-{NB} had the correctness result of92 percent. © 2020 by the authors.},
	pages = {1--6},
	number = {1},
	journaltitle = {International Journal of Information and Education Technology},
	author = {Yangyuen, K.T. and Rattanapian, V.},
	date = {2020},
	note = {Publisher: International Journal of Information and Education Technology},
	keywords = {xno}
}

@article{ochodek_recognizing_2020,
	title = {Recognizing lines of code violating company-specific coding guidelines using machine learning: A Method and Its Evaluation},
	volume = {25},
	issn = {13823256},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075204546&doi=10.1007%2fs10664-019-09769-8&partnerID=40&md5=8cf3824cf1e4c7096ea29752f6a12917},
	doi = {10.1007/s10664-019-09769-8},
	abstract = {Software developers in big and medium-size companies are working with millions of lines of code in their codebases. Assuring the quality of this code has shifted from simple defect management to proactive assurance of internal code quality. Although static code analysis and code reviews have been at the forefront of research and practice in this area, code reviews are still an effort-intensive and interpretation-prone activity. The aim of this research is to support code reviews by automatically recognizing company-specific code guidelines violations in large-scale, industrial source code. In our action research project, we constructed a machine-learning-based tool for code analysis where software developers and architects in big and medium-sized companies can use a few examples of source code lines violating code/design guidelines (up to 700 lines of code) to train decision-tree classifiers to find similar violations in their codebases (up to 3 million lines of code). Our action research project consisted of (i) understanding the challenges of two large software development companies, (ii) applying the machine-learning-based tool to detect violations of Sun’s and Google’s coding conventions in the code of three large open source projects implemented in Java, (iii) evaluating the tool on evolving industrial codebase, and (iv) finding the best learning strategies to reduce the cost of training the classifiers. We were able to achieve the average accuracy of over 99\% and the average F-score of 0.80 for open source projects when using ca. 40K lines for training the tool. We obtained a similar average F-score of 0.78 for the industrial code but this time using only up to 700 lines of code as a training dataset. Finally, we observed the tool performed visibly better for the rules requiring to understand a single line of code or the context of a few lines (often allowing to reach the F-score of 0.90 or higher). Based on these results, we could observe that this approach can provide modern software development companies with the ability to use examples to teach an algorithm to recognize violations of code/design guidelines and thus increase the number of reviews conducted before the product release. This, in turn, leads to the increased quality of the final software. © 2019, The Author(s).},
	pages = {220--265},
	number = {1},
	journaltitle = {Empirical Software Engineering},
	author = {Ochodek, M. and Hebig, R. and Meding, W. and Frost, G. and Staron, M.},
	date = {2020},
	note = {Publisher: Springer},
	keywords = {Software design, Open source software, Learning systems, Decision trees, Industrial research, Open source projects, Machine learning, Open systems, Software developer, Codes (symbols), Action research, Code review, Decision tree classifiers, Industrial sources, Measurement, Medium-sized companies, Static code analysis, xno},
	file = {Full Text:C\:\\Users\\michalm\\Zotero\\storage\\G8YVFBJ4\\Ochodek et al. - 2020 - Recognizing lines of code violating company-specif.pdf:application/pdf;Full Text:C\:\\Users\\michalm\\Zotero\\storage\\YDFJT9HT\\Ochodek et al. - 2020 - Recognizing lines of code violating company-specif.pdf:application/pdf}
}

@article{shukla_formal_2019,
	title = {Formal modeling and verification of software-defined networks: A survey},
	volume = {29},
	issn = {10557148},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071243945&doi=10.1002%2fnem.2082&partnerID=40&md5=309aab14a5e3e12ab6b07b457090f14a},
	doi = {10.1002/nem.2082},
	abstract = {Unlike traditional networking devices, control and management plane are decoupled from data plane in software-defined networks ({SDN}). The logically centralized control and management plane facilitate dynamic orchestration of network resources, services, and policies by writing software programs. This provides much needed flexibility and programmability where networking rules and policies can be modified dynamically depending upon the application context. As the operation of network services entirely depends on a program, a small fault may induce several issues which can adversely affect the expected behavior of the network. Formal modeling and verification help in catching inconsistencies and existence of errors prior to the deployment of the programs that control the behavior of a network. In this paper, we provide a comprehensive survey of tools and techniques available in the literature for formal modeling and verification of {SDN}. These tools and techniques are classified based on their types, the components of {SDN} where they can be applied, and the design and development phase when they are utilized. In particular, their respective benefits and limitations are discussed in terms of ease of use, interfaces, and the ability to capture and verify intended network properties. © 2019 John Wiley \& Sons, Ltd.},
	number = {5},
	journaltitle = {International Journal of Network Management},
	author = {Shukla, N. and Pandey, M. and Srivastava, S.},
	date = {2019},
	note = {Publisher: John Wiley and Sons Ltd},
	keywords = {Surveys, Application contexts, Centralized control, Control and management, Design and Development, Formal modeling and verification, Network properties, Networking devices, Software defined networking, Tools and techniques, Verification, xno}
}

@article{khuat_binary_2019,
	title = {Binary teaching–learning-based optimization algorithm with a new update mechanism for sample subset optimization in software defect prediction},
	volume = {23},
	issn = {14327643},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053884634&doi=10.1007%2fs00500-018-3546-6&partnerID=40&md5=63c72345eda9a46d0b9730ad78e9765a},
	doi = {10.1007/s00500-018-3546-6},
	abstract = {Software defect prediction has gained considerable attention in recent years. A broad range of computational methods has been developed for accurate prediction of faulty modules based on code and design metrics. One of the challenges in training classifiers is the highly imbalanced class distribution in available datasets, leading to an undesirable bias in the prediction performance for the minority class. Data sampling is a widespread technique to tackle this problem. However, traditional sampling methods, which depend mainly on random resampling from a given dataset, do not take advantage of useful information available in training sets, such as sample quality and representative instances. To cope with this limitation, evolutionary undersampling methods are usually used for identifying an optimal sample subset for the training dataset. This paper proposes a binary teaching–learning- based optimization algorithm employing a distribution-based solution update rule, namely {BTLBOd}, to generate a balanced subset of highly valuable examples. This subset is then applied to train a classifier for reliable prediction of potentially defective modules in a software system. Each individual in {BTLBOd} includes two vectors: a real-valued vector generated by the distribution-based update mechanism, and a binary vector produced from the corresponding real vector by a proposed mapping function. Empirical results showed that the optimal sample subset produced by {BTLBOd} might ameliorate the classification accuracy of the predictor on highly imbalanced software defect data. Obtained results also demonstrated the superior performance of the proposed sampling method compared to other popular sampling techniques. © 2018, Springer-Verlag {GmbH} Germany, part of Springer Nature.},
	pages = {9919--9935},
	number = {20},
	journaltitle = {Soft Computing},
	author = {Khuat, T.T. and Le, M.H.},
	date = {2019},
	note = {Publisher: Springer Verlag},
	keywords = {Software defect prediction, Forecasting, Classification (of information), Accurate prediction, Classification accuracy, Distribution-based update, Imbalanced Learning, Learning algorithms, Optimization, Optimization algorithms, Prediction performance, Sampling technique, Set theory, Vectors, xno}
}

@inproceedings{cynthia_predicting_2019,
	title = {Predicting and classifying software faults: A data mining approach},
	isbn = {978-1-4503-7195-7},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073224084&doi=10.1145%2f3348445.3348453&partnerID=40&md5=f135d8b39a391adae485f81885aebc4e},
	doi = {10.1145/3348445.3348453},
	abstract = {In the field of software engineering, the detection of fault in the software has become a major topic to explore. With the help of data mining and machine learning approaches, this paper aims to denote whether a software is fault prone or not. In order to accomplish that this paper gives importance to compare between different machine learning approaches and by observing their performances we can conclude which models perform better to detect fault in the selected software modules. The dataset we have chosen to work on has imbalanced data. This paper research also worked with the imbalanced dataset and what results the imbalanced dataset gave when examined. The accuracy comparison, the performance of the different metrics can broadly help in software defect detection mechanism. © 2019 Association for Computing Machinery.},
	pages = {143--147},
	booktitle = {{ACM} International Conference Proceeding Series},
	publisher = {Association for Computing Machinery},
	author = {Cynthia, S.T. and Ripon, S.H.},
	date = {2019},
	keywords = {Data mining, Software defects, Forecasting, Machine learning, Association rules, Fault detection, Software fault, Support vector machines, Software modules, Accuracy comparisons, Adaptive boosting, Computer software, Imbalanced data, Imbalanced dataset, Machine learning approaches, Paper research, xno}
}

@article{jadhav_software_2019,
	title = {A software defect learning and analysis utilizing regression method for quality software development},
	volume = {8},
	issn = {22783091},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073326288&doi=10.30534%2fijatcse%2f2019%2f38842019&partnerID=40&md5=3fb3c2804f99f578ce08f7aefa2ed602},
	doi = {10.30534/ijatcse/2019/38842019},
	abstract = {The program is a complex object consisting of different units with variable degrees of defects. By predicting the effectiveness and frequency of program defects, program managers can make better use of manpower, cost, and time to obtain better quality assurance. It is always possible to have a set of defects that affect designed and predictable units in order to have close association with the subsidiaries. Most of the current defect prediction rating mechanism is derived from learning the previous project data, but it is not sufficient to predict the defect of the new project because the new design may contain a different type of parameter. This paper proposes a Software Defect Learning and Analysis utilizing Regression Method ({SDLA}-{RM}) to detect defects and plan a better maintenance strategy, which can support the prediction of a defective or nondefective software unit prior to deployment in any project programs. The {SDL}-{RM} mechanism extends Regression Analysis ({RAM}) to create an effective rulebased model for accurately classifying program faults. This approach improves the predictability of software defects, allowing software development to spend more time testing components that are expected to contain errors. The experimental evaluation is carried out across the {NASA}-{PROMISE} repository data sets, that outcome of the results in comparison with existing classifiers suggest the effectiveness and practical perspective in the software development. © 2019, World Academy of Research in Science and Engineering. All rights reserved.},
	pages = {1275--1282},
	number = {4},
	journaltitle = {International Journal of Advanced Trends in Computer Science and Engineering},
	author = {Jadhav, R.B. and Joshi, S.D. and Thorat, U.G. and Joshi, A.S.},
	date = {2019},
	note = {Publisher: World Academy of Research in Science and Engineering},
	keywords = {xno},
	file = {Full Text:C\:\\Users\\michalm\\Zotero\\storage\\H4IUCUX2\\Bharati Vidyapeeth (Deemed to be University) College of Engineering Pune, India and B. Jadhav - 2019 - A Software Defect Learning and Analysis Utilizing .pdf:application/pdf}
}

@article{chatterjee_fuzzy_2019,
	title = {A fuzzy rule-based generation algorithm in interval type-2 fuzzy logic system for fault prediction in the early phase of software development},
	volume = {31},
	issn = {0952813X},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058179117&doi=10.1080%2f0952813X.2018.1552315&partnerID=40&md5=39c1d24ec8744497e3521e77af9da471},
	doi = {10.1080/0952813X.2018.1552315},
	abstract = {Reliability, a measure of software, deals in total number of faults count up to a certain period of time. The present study aims at estimating the total number of software faults during the early phase of software life cycle. Such estimation helps in producing more reliable software as there may be a scope to take necessary corrective actions for improving the reliability within optimum time and cost by the software developers. The proposed interval type-2 fuzzy logic-based model considers reliability-relevant software metric and earlier project data as model inputs. Type-2 fuzzy sets have been used to reduce uncertainties in the vague linguistic values of the software metrics. A rule formation algorithm has been developed to overcome inconsistency in the consequent parts of large number of rules. Twenty-six software project data help to validate the model, and a comparison has been provided to analyse the proposed model’s performance. © 2018, © 2018 Informa {UK} Limited, trading as Taylor \& Francis Group.},
	pages = {369--391},
	number = {3},
	journaltitle = {Journal of Experimental and Theoretical Artificial Intelligence},
	author = {Chatterjee, S. and Maji, B. and Pham, H.},
	date = {2019},
	note = {Publisher: Taylor and Francis Ltd.},
	keywords = {Computer circuits, Software design, Software developer, Software reliability, Fuzzy inference, Fuzzy rules, Corrective actions, Early fault, Fuzzy logic, Fuzzy rule base, Generation algorithm, Interval type-2 fuzzy logic, Interval type-2 fuzzy logic systems, Life cycle, Software life cycles, xyes}
}

@article{chen_probabilistic_2019,
	title = {Probabilistic timing analysis of time-randomised caches with fault detection mechanisms},
	volume = {13},
	issn = {17518601},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065673504&doi=10.1049%2fiet-cdt.2018.5043&partnerID=40&md5=92250ec7af01af75cf71cdd217f336c4},
	doi = {10.1049/iet-cdt.2018.5043},
	abstract = {In the real-time systems domain, time-randomised caches have been proposed as a way to simplify software timing analysis, i.e. the process of estimating the probabilistic worst case execution time ({pWCET}) of an application. However, the technology scaling of the cache memory manufacturing process is rendering transient and permanent faults more and more likely. These faults, in turn, affect a system's timing behaviour and the complexity of its analysis. In this study, the authors propose a static probabilistic timing analysis approach for time-randomised caches that is able to account for the presence of faults- A nd their detection mechanisms-using a state-space modelling technique. Their experiments show that the proposed methodology is capable of providing tight {pWCET} estimates. In their analysis, the effects on the estimation of safe {pWCET} bounds of two online mechanisms for the detection and classification of faults, i.e. a rule-based system and dynamic hidden Markov models (D-{HMMs}), are compared. The experimental results show that different mechanisms can greatly affect safe {pWCET} margins and that, by using D-{HMMs}, the {pWCET} of the system can be improved with respect to rule-based detection. © 2019 Institution of Engineering and Technology. All rights reserved.},
	pages = {262--272},
	number = {3},
	journaltitle = {{IET} Computers and Digital Techniques},
	author = {Chen, C. and Panerati, J. and Li, M. and Beltrame, G.},
	date = {2019},
	note = {Publisher: Institution of Engineering and Technology},
	keywords = {Fault detection, Application programs, Cache memory, Different mechanisms, Fault-detection mechanisms, Hidden Markov models, Interactive computer systems, Manufacturing process, Real time systems, Rule based detection, Software timing analysis, State-space modelling, Timing circuits, Transient and permanent fault, Worst-case execution time, xno}
}

@article{mori_balancing_2019,
	title = {Balancing the trade-off between accuracy and interpretability in software defect prediction},
	volume = {24},
	issn = {13823256},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050694331&doi=10.1007%2fs10664-018-9638-1&partnerID=40&md5=24406dcf95de7405b06a8ce16df8182c},
	doi = {10.1007/s10664-018-9638-1},
	abstract = {Context: Classification techniques of supervised machine learning have been successfully applied to various domains of practice. When building a predictive model, there are two important criteria: predictive accuracy and interpretability, which generally have a trade-off relationship. In particular, interpretability should be accorded greater emphasis in the domains where the incorporation of expert knowledge into a predictive model is required. Objective: The aim of this research is to propose a new classification model, called superposed naive Bayes ({SNB}), which transforms a naive Bayes ensemble into a simple naive Bayes model by linear approximation. Method: In order to evaluate the predictive accuracy and interpretability of the proposed method, we conducted a comparative study using well-known classification techniques such as rule-based learners, decision trees, regression models, support vector machines, neural networks, Bayesian learners, and ensemble learners, over 13 real-world public datasets. Results: A trade-off analysis between the accuracy and interpretability of different classification techniques was performed with a scatter plot comparing relative ranks of accuracy with those of interpretability. The experiment results show that the proposed method ({SNB}) can produce a balanced output that satisfies both accuracy and interpretability criteria. Conclusions: {SNB} offers a comprehensible predictive model based on a simple and transparent model structure, which can provide an effective way for balancing the trade-off between accuracy and interpretability. © 2018, Springer Science+Business Media, {LLC}, part of Springer Nature.},
	pages = {779--825},
	number = {2},
	journaltitle = {Empirical Software Engineering},
	author = {Mori, T. and Uchihira, N.},
	date = {2019},
	note = {Publisher: Springer New York {LLC}},
	keywords = {Defects, Software defect prediction, Classification (of information), Decision trees, Economic and social effects, Bayesian networks, Classifiers, Ensemble learning, Interpretability, Mathematical transformations, Model approximations, Naive Bayes classifiers, Predictive accuracy, Regression analysis, Supervised learning, Trade-off analysis, Weights of evidences, xyes},
	file = {Mori and Uchihira - 2019 - Balancing the trade-off between accuracy and inter.pdf:C\:\\Users\\michalm\\Zotero\\storage\\MJIVB7Y2\\Mori and Uchihira - 2019 - Balancing the trade-off between accuracy and inter.pdf:application/pdf}
}

@article{juneja_fuzzy-filtered_2019,
	title = {A fuzzy-filtered neuro-fuzzy framework for software fault prediction for inter-version and inter-project evaluation},
	volume = {77},
	issn = {15684946},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061619023&doi=10.1016%2fj.asoc.2019.02.008&partnerID=40&md5=95a51edee31eacae45d0bea5ffbf7d64},
	doi = {10.1016/j.asoc.2019.02.008},
	abstract = {Fault Prediction is the most required measure to estimate the software quality and reliability. Several methods, measures, aspects and testing methodologies are available to evaluate the software fault. In this paper, a fuzzy-filtered neuro-fuzzy framework is introduced to predict the software faults for internal and external software projects. The suggested framework is split into three primary phases. At the earlier phase, the effective metrics or measures are identified, which can derive the accurate decision on prediction of software fault. In this phase, the composite analytical observation of each software attribute is calculated using Information Gain and Gain Ratio measures. In the second phase, these fuzzy rules are applied on these measures for selection of effective and high-impact features. In the last phase, the Neuro-fuzzy classifier is applied on fuzzy-filtered training and testing sets. The proposed framework is applied to identify the software faults based on inter-version and inter-project evaluation. In this framework, the earlier projects or project-versions are considered as training sets and the new projects or versions are taken as testing sets. The experimentation is conducted on nine open source projects taken from {PROMISE} repository as well as on {PDE} and {JDT} projects. The approximation is applied on internal version-specific fault prediction and external software projects evaluation. The comparative analysis is performed against Decision Tree, Random Tree, Random Forest, Naive Bayes and Multilevel Perceptron classifiers. This prediction result signifies that the proposed framework has gained the higher accuracy, lesser error rate and significant {AUC} and {GM} for inter-project and inter-version evaluations. © 2019 Elsevier B.V.},
	pages = {696--713},
	journaltitle = {Applied Soft Computing Journal},
	author = {Juneja, K.},
	date = {2019},
	note = {Publisher: Elsevier Ltd},
	keywords = {Computer software selection and evaluation, Open source software, Forecasting, Classification (of information), Decision trees, Software testing, Project management, Software reliability, Software fault prediction, Defect prediction, Fuzzy inference, Fuzzy systems, Comparative analysis, Fuzzy, Fuzzy sets, Inter project, Intra project, Multi-level perceptron, Neural networks, Neuro fuzzy classifier, xyes}
}

@article{agrawal_cross_2019,
	title = {Cross project defect prediction for open source software},
	issn = {25112104},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085313661&doi=10.1007%2fs41870-019-00299-6&partnerID=40&md5=a9a79b98b2b446f274e92299a20f483d},
	doi = {10.1007/s41870-019-00299-6},
	abstract = {Software defect prediction is the process of identification of defects early in the life cycle so as to optimize the testing resources and reduce maintenance efforts. Defect prediction works well if sufficient amount of data is available to train the prediction model. However, not always this is the case. For example, when the software is the first release or the company has not maintained significant data. In such cases, cross project defect prediction may identify the defective classes. In this work, we have studied the feasibility of cross project defect prediction and empirically validated the same. We conducted our experiments on 12 open source datasets. The prediction model is built using 12 software metrics. After studying the various train test combinations, we found that cross project defect prediction was feasible in 35 out of 132 cases. The success of prediction is determined via precision, recall and {AUC} of the prediction model. We have also analyzed 14 descriptive characteristics to construct the decision tree. The decision tree learnt from this data has 15 rules which describe the feasibility of successful cross project defect prediction. © 2019, Bharati Vidyapeeth's Institute of Computer Applications and Management.},
	journaltitle = {International Journal of Information Technology (Singapore)},
	author = {Agrawal, A. and Malhotra, R.},
	date = {2019},
	note = {Publisher: Springer Science and Business Media B.V.},
	keywords = {xyes}
}

@article{miholca_software_2019,
	title = {Software Defect Prediction Using a Hybrid Model Based on Semantic Features Learned from the Source Code},
	volume = {11775 {LNAI}},
	issn = {03029743},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081595392&doi=10.1007%2f978-3-030-29551-6_23&partnerID=40&md5=7f92b0dfd4e826e21fdd32e6e543d7b9},
	doi = {10.1007/978-3-030-29551-6_23},
	abstract = {Software defect prediction has extensive applicability thus being a very active research area in Search-Based Software Engineering. A high proportion of the software defects are caused by violated couplings. In this paper, we investigate the relevance of semantic coupling in assessing the software proneness to defects. We propose a hybrid classification model combining Gradual Relational Association Rules with Artificial Neural Networks, which detects the defective software entities based on semantic features automatically learned from the source code. The experiments we have performed led to results that confirm the interplay between conceptual coupling and software defects proneness. © Springer Nature Switzerland {AG} 2019.},
	pages = {262--274},
	journaltitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Miholca, D.-L. and Czibula, G.},
	date = {2019},
	note = {{ISBN}: 9783030295509
Publisher: Springer},
	keywords = {Defects, Software defect prediction, Software defects, Forecasting, Learning systems, Semantics, Software engineering, Neural networks, Software entities, Hybrid classification, Hybrid model, Search-based software engineering, Semantic couplings, Semantic features, xno}
}

@inproceedings{miholca_dyngrar_2019,
	title = {{DynGRAR}: A dynamic approach to mining gradual relational association rules},
	volume = {159},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076255283&doi=10.1016%2fj.procs.2019.09.155&partnerID=40&md5=5e9b30b5156c407fb1e81d944ab73c0a},
	doi = {10.1016/j.procs.2019.09.155},
	abstract = {Relational Association Rules ({RARs}) capture generic relations between attributes values in possibly large data sets. Due to their ability to uncover underlying semantically relevant patterns, they are of particular interest in data mining research and applicable in both unsupervised and supervised learning scenarios. With the aim of increasing the stability and expressiveness of the classical, non-gradual {RARs}, Gradual Relational Association Rules ({GRARs}) have been introduced. By generalizing the boolean relations to gradual relations, {GRARs} also capture the degrees to which generic relations are satisfied. In the current paper we introduce a new approach called {DynGRAR} (Dynamic Gradual Relational Association Rules Miner) for uncovering interesting {GRARs} in dynamic data sets which are incrementally extended with both new data instances and new data attributes. {DynGRAR} dynamically adjusts the set of all interesting {GRARs}. Through multiple experiments performed on publicly available software defect prediction data sets, we have evaluated {DynGRAR} versus applying the standard {GRARs} mining algorithm from scratch on the extended data. The results obtained emphasize the superior performance of the dynamic approach we propose. © 2019 The Author(s). Published by Elsevier B.V.},
	pages = {10--19},
	booktitle = {Procedia Computer Science},
	publisher = {Elsevier B.V.},
	author = {Miholca, D.-L. and Czibula, G.},
	date = {2019},
	note = {{ISSN}: 18770509},
	keywords = {Data mining, Software defect prediction, Association rules, 03B52, 68T35, Boolean relations, Dynamic approaches, Dynamic data sets, gradual relational association rule 2000 {MSC}: 68T05, Knowledge based systems, Unuspervised learning, xno}
}

@article{baarah_machine_2019,
	title = {Machine learning approaches for predicting the severity level of software bug reports in closed source projects},
	volume = {10},
	issn = {2158107X},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072269511&doi=10.14569%2fijacsa.2019.0100836&partnerID=40&md5=e2a0bcd27384aa873caffd025fa64dab},
	doi = {10.14569/ijacsa.2019.0100836},
	abstract = {In Software Development Life Cycle, fixing defect bugs is one of the essential activities of the software maintenance phase. Bug severity indicates how major or minor the bug impacts on the execution of the system and how rapidly the developer should fix it. Triaging a vast amount of new bugs submitted to the software bug repositories is a cumbersome and time-consuming process. Manual triage might lead to a mistake in assigning the appropriate severity level for each bug. As a consequence, a delay for fixing severe software bugs will take place. However, the whole process of assigning the severity level for bug reports should be automated. In this paper, we aim to build prediction models that will be utilized to determine the class of the severity (severe or non-severe) of the reported bug. To validate our approach, we have constructed a dataset from historical bug reports stored in {JIRA} bug tracking system. These bug reports are related to different closed-source projects developed by {INTIX} Company located in Amman, Jordan. We compare eight popular machine learning algorithms, namely Naive Bayes, Naive Bayes Multinomial, Support Vector Machine, Decision Tree (J48), Random Forest, Logistic Model Trees, Decision Rules ({JRip}) and K-Nearest Neighbor in terms of accuracy, F-measure and Area Under the Curve ({AUC}). According to the experimental results, a Decision Tree algorithm called Logistic Model Trees achieved better performance compared to other machine learning algorithms in terms of Accuracy, {AUC} and F-measure with values of 86.31, 0.90 and 0.91, respectively. © 2018 The Science and Information ({SAI}) Organization Limited.},
	pages = {285--294},
	number = {8},
	journaltitle = {International Journal of Advanced Computer Science and Applications},
	author = {Baarah, A. and Aloqaily, A. and Salah, Z. and Zamzeer, M. and Sallam, M.},
	date = {2019},
	note = {Publisher: Science and Information Organization},
	keywords = {xno},
	file = {Full Text:C\:\\Users\\michalm\\Zotero\\storage\\GN8MK3X5\\Baarah et al. - 2019 - Machine Learning Approaches for Predicting the Sev.pdf:application/pdf}
}

@article{chuanlei_analysis_2019,
	title = {Analysis and prediction of autistic children's game characteristics},
	volume = {61},
	issn = {09528091},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072188363&doi=10.1504%2fIJCAT.2019.102092&partnerID=40&md5=f193771d469c8431967ad08c10e1962f},
	doi = {10.1504/IJCAT.2019.102092},
	abstract = {Children with autism show great defects in the games they play. This research designed '{CePingBiao} of autistic children's game ability', and undertook a study on the game features of 130 children diagnosed with autism, and gave 69 children rehabilitation training game ability for four months. Results show that (1) autistic children's ability in game on the game type is with high and low points. Best body level of game development, followed by the structure of the game, again is a symbol of games, social games, and the lowest development level is the rules of the game; (2) age significantly influences the children with autism. With the increase in age, autism game levels increase, and 5 to 6 years old is in the rapid development period. (3) The training time influences the structure of the autistic children game development level. The language level obviously affects the development of children's game ability. Copyright © 2019 Inderscience Enterprises Ltd.},
	pages = {37--46},
	number = {1},
	journaltitle = {International Journal of Computer Applications in Technology},
	author = {Chuanlei, L. and Yuanfei, H. and Jiao, L.},
	date = {2019},
	note = {Publisher: Inderscience Publishers},
	keywords = {Software design, Assessment, Autistic children, Children with autisms, Diseases, Game development, Language levels, Patient rehabilitation, Play ability, Rehabilitation training, Training time, xno}
}

@article{kovalenko_development_2019,
	title = {Development of the procedure for integrated application of scenario prediction methods},
	volume = {2},
	issn = {17293774},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069044148&doi=10.15587%2f1729-4061.2019.163871&partnerID=40&md5=27133cff758a1c76cad616fcca58a2b2},
	doi = {10.15587/1729-4061.2019.163871},
	abstract = {The paper proposes a procedure for the integrated application of methods for scenario analysis and prediction, represented by graphs of the «tree» type. The task on analysis of risks in software projects has been considered, the cause of which are the possible programming errors that lead to failures in the operation of systems and software. The joint use of a failure tree and a probability tree makes it possible to generate the sequences of scripts for the implementation of an adverse event, whose main cause is possible defects or errors in software or data, as well as to assess the probabilities of their realization. Such an approach allows the identification of the overall result of the influence of certain risk-forming factors (defects) on the development of possible negative consequences (failures and malfunctions) or damage to the operation of complex software systems. This makes it possible to timely identify and propose effective mechanisms to manage software risk in order to reduce and eliminate them. A procedure has been proposed for aggregating individual probabilistic expert assessments of the occurrence of a risk event. Such an approach makes it possible to obtain group expert estimates assessing the feasibility of a risk event based on the constructed system of random events into a generalized expert assessment. The probabilities of the occurrence of a risk event, thus obtained, are used when constructing a probability tree and calculating the ratios of probabilistic inference using it. Aggregation of individual expert estimates is carried out by combining them based on a mathematical apparatus of the theory of evidence and the theory of plausible and paradoxical reasoning. It was established that in order to improve quality of the results of combining it is appropriate to establish an order for combining expert evidence and apply one of the rules of conflict redistribution as a combination rule. Numerical calculations of the proposed procedure for integrated application of a failure tree and a probability tree are provided. The results obtained make it possible to run a more in-depth analysis of the examined software systems and objects, and are aimed at improving the quality and effectiveness of managing risks in software projects caused by defects in programs and data. © 2019 Igor Kovalenko, Yevhen Davydenko, Alyona Shved.},
	pages = {31--38},
	number = {4},
	journaltitle = {Eastern-European Journal of Enterprise Technologies},
	author = {Kovalenko, I. and Davydenko, Y. and Shved, A.},
	date = {2019},
	note = {Publisher: Technology Center},
	keywords = {xno},
	file = {Full Text:C\:\\Users\\michalm\\Zotero\\storage\\IRG39JL3\\Kovalenko et al. - 2019 - Development of the procedure for integrated applic.pdf:application/pdf}
}

@inproceedings{tseng_efficient_2019,
	title = {Efficient search of layout hotspot patterns for matching {SEM} images using multilevel pixelation},
	volume = {10961},
	isbn = {978-1-5106-2569-3},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068672535&doi=10.1117%2f12.2514818&partnerID=40&md5=aa43b83a7059436336a38590dbec077f},
	doi = {10.1117/12.2514818},
	abstract = {Layout features become highly susceptible to lithography process fluctuations due to the widening subwavelength lithography gap. Problematic layout patterns incur poor printability even if they pass design rule checking. These hotspots should be detected and removed at early design phases to improve manufacturability. While existing studies mainly focus on hotspot detection and pattern classification, hotspot pattern library generation is rarely addressed in literature but crucial to the effectiveness and efficiency of hotspot detection. For an advanced process, in addition to yield-limiting patterns inherent from old processes and computation intensive lithography simulation, defect silicon images ({SEM} images) inspected from test wafers provide more realistic process-dependent hotspots. For facilitating hotspot pattern library generation, we raise a pattern matching problem of searching design layout patterns that may induce problematic {SEM} images. The key challenge is the various shape distortions between an {SEM} image and corresponding design layouts. Directly matching either feature points or shapes of both is thus not applicable. We observe that even with shape distortions, matched design layouts and the {SEM} image have similar density distribution. Therefore, in this paper, we propose an efficient multilevel pixilation framework to seek layout clips with similar density distribution from coarse-to finegranularities to an {SEM} image. The proposed framework possesses high parallelism. Our results show that the proposed method can effectively and efficiently identify matched layout pattern candidates. © 2019 {SPIE}.},
	booktitle = {Proceedings of {SPIE} - The International Society for Optical Engineering},
	publisher = {{SPIE}},
	author = {Tseng, S.S.-E. and Chang, W.-C. and Jiang, I.H.-R. and Zhu, J. and Shiely, J.P.},
	date = {2019},
	note = {{ISSN}: 0277786X},
	keywords = {Software design, Effectiveness and efficiencies, Hot spot, Lithography, Lithography Simulation, Multilevel framework, Pattern matching, Pattern matching problems, Pixelation, {SEM} image, Silicon wafers, Subwavelength lithography, Template matching, xno}
}

@article{iqbal_performance_2019,
	title = {Performance analysis of machine learning techniques on software defect prediction using {NASA} datasets},
	volume = {10},
	issn = {2158107X},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066739131&doi=10.14569%2fijacsa.2019.0100538&partnerID=40&md5=039cbe35e0f11a12f4eb5be407c9b4b2},
	doi = {10.14569/ijacsa.2019.0100538},
	abstract = {Defect prediction at early stages of software development life cycle is a crucial activity of quality assurance process and has been broadly studied in the last two decades. The early prediction of defective modules in developing software can help the development team to utilize the available resources efficiently and effectively to deliver high quality software product in limited time. Until now, many researchers have developed defect prediction models by using machine learning and statistical techniques. Machine learning approach is an effective way to identify the defective modules, which works by extracting the hidden patterns among software attributes. In this study, several machine learning classification techniques are used to predict the software defects in twelve widely used {NASA} datasets. The classification techniques include: Naïve Bayes ({NB}), Multi-Layer Perceptron ({MLP}). Radial Basis Function ({RBF}), Support Vector Machine ({SVM}), K Nearest Neighbor ({KNN}), {kStar} (K*), One Rule ({OneR}), {PART}, Decision Tree ({DT}), and Random Forest ({RF}). Performance of used classification techniques is evaluated by using various measures such as: Precision, Recall, F-Measure, Accuracy, {MCC}, and {ROC} Area. The detailed results in this research can be used as a baseline for other researches so that any claim regarding the improvement in prediction through any new technique, model or framework can be compared and verified. © 2018 The Science and Information ({SAI}) Organization Limited.},
	pages = {300--308},
	number = {5},
	journaltitle = {International Journal of Advanced Computer Science and Applications},
	author = {Iqbal, A. and Aftab, S. and Ali, U. and Nawaz, Z. and Sana, L. and Ahmad, M. and Husen, A.},
	date = {2019},
	note = {Publisher: Science and Information Organization},
	keywords = {xno},
	file = {Full Text:C\:\\Users\\michalm\\Zotero\\storage\\KJC5BWR2\\Iqbal et al. - 2019 - Performance Analysis of Machine Learning Technique.pdf:application/pdf}
}

@article{singh_fuzzy_2019,
	title = {A fuzzy rule-based approach for test case selection probability estimation in regression testing},
	volume = {11},
	issn = {17572657},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065140367&doi=10.1504%2fIJCAET.2019.099336&partnerID=40&md5=b27c15e17dc95ee1915268f0d9850b26},
	doi = {10.1504/IJCAET.2019.099336},
	abstract = {Regression testing is a very essential activity during maintenance of software. Due to constraints of time and cost, it is not possible to re-execute every test case with respect to every change occurred. Thus, a technique is required that selects and prioritises the test cases efficiently. This paper proposes a novel fuzzy rule-based approach for selecting and ordering a number of test cases from an existing test suite to predict the selection probability of test cases using multiple factors. The test cases, which have ability to find high fault detection rate with maximum coverage and minimum execution time to test are selected. The results specify the effectiveness of the proposed model for predicting the selection probability of individual test cases. Copyright © 2019 Inderscience Enterprises Ltd.},
	pages = {391--408},
	number = {3},
	journaltitle = {International Journal of Computer Aided Engineering and Technology},
	author = {Singh, L. and Singh, S.N. and Dawra, S. and Tuli, R.},
	date = {2019},
	note = {Publisher: Inderscience Publishers},
	keywords = {xno}
}

@article{shao_novel_2018,
	title = {A novel software defect prediction based on atomic class-association rule mining},
	volume = {114},
	issn = {09574174},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050891830&doi=10.1016%2fj.eswa.2018.07.042&partnerID=40&md5=2fcaad1a183db92a46af14f4bb2c2b67},
	doi = {10.1016/j.eswa.2018.07.042},
	abstract = {To ensure the rational allocation of software testing resources and reduce costs, software defect prediction has drawn notable attention to many “white-box” and “black-box” classification algorithms. Although there have been lots of studies on using software product metrics to identify defect-prone modules, defect prediction algorithms are still worth exploring. For instance, it is not easy to directly implement the Apriori algorithm to classify defect-prone modules across a skewed dataset. Therefore, we propose a novel supervised approach for software defect prediction based on atomic class-association rule mining ({ACAR}). It holds the characteristics of only one feature of the antecedent and a unique class label of the consequent, which is a specific kind of association rules that explores the relationship between attributes and categories. It holds the characteristics of only one feature of the antecedent and a unique class label of the consequent, which is a specific kind of association rules that explores the relationship between attributes and categories. Such association patterns can provide meaningful knowledge that can be easily understood by software engineers. A new software defect prediction model infrastructure based on association rules is employed to improve the prediction of defect-prone modules, which is divided into data preprocessing, rule model building and performance evaluation. Moreover, {ACAR} can achieve a satisfactory classification performance compared with other seven benchmark learners (the extension of classification based on associations ({CBA}2), Support Vector Machine, Naive Bayesian, Decision Tree, {OneR}, K-nearest Neighbors and {RIPPER}) on {NASA} {MDP} and {PROMISE} datasets. In light of software defect associative prediction, a comparative experiment between {ACAR} and {CBA}2 is discussed in details. It is demonstrated that {ACAR} is better than {CBA}2 in terms of {AUC}, G-mean, Balance, and understandability. In addition, the average {AUC} of {ACAR} is increased by 2.9\% compared with {CBA}2, which can reach 81.1\%. © 2018 Elsevier Ltd},
	pages = {237--254},
	journaltitle = {Expert Systems with Applications},
	author = {Shao, Y. and Liu, B. and Wang, S. and Li, G.},
	date = {2018},
	note = {Publisher: Elsevier Ltd},
	keywords = {Data mining, Defects, {NASA}, Software defect prediction, Forecasting, Learning systems, Classification (of information), Decision trees, Apriori, Association rules, Benchmarking, Classification algorithm, Black-box testing, Class association rules, Classification based on associations, Classification performance, Comparative experiments, Nearest neighbor search, Software product metrics, xyes},
	file = {Shao et al. - 2018 - A novel software defect prediction based on atomic.pdf:C\:\\Users\\michalm\\Zotero\\storage\\YQK7ZUBN\\Shao et al. - 2018 - A novel software defect prediction based on atomic.pdf:application/pdf}
}

@inproceedings{pak_software_2018,
	title = {Software Defect Prediction Using Propositionalization Based Data Preprocessing: An Empirical Study},
	isbn = {978-1-5386-8431-3},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061386917&doi=10.1109%2fICDSBA.2018.00021&partnerID=40&md5=0c174d9faf53d90bc911c4bb813b6073},
	doi = {10.1109/ICDSBA.2018.00021},
	abstract = {Data preprocessing can be used to improve classifier performance in classification problems. Software defect prediction is also one of classification problems, so it is needed to use data preprocessing for improving the performance of model. In this paper, we study about the software defect prediction using propositonalization based data preprocessing method. We proposed propositionalization using decision tree as data preprocessing method and made experiments by using common classifiers over 17 datasets from the {PROMISE} repository. We also used paired t-test to compare propositionalization using decision tree with attribute subset selection and principal component analysis. Results showed that Propostionalization using decision tree improved the performance of software defect prediction significantly and it was more effective than attribute subset selection and principal component analysis. There were no statistically significant differences between top 5 classifiers. © 2018 {IEEE}.},
	pages = {71--77},
	booktitle = {Proceedings - 2nd International Conference on Data Science and Business Analytics, {ICDSBA} 2018},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Pak, C. and Wang, T.T. and Su, X.H.},
	date = {2018},
	keywords = {Data mining, Defects, Software defect prediction, Empirical studies, Forecasting, Classification (of information), Decision trees, Association rules, Trees (mathematics), Advanced Analytics, Classifiers, Classifier performance, Data preprocessing, Principal component analysis, Propositionalization, propostionalization, Statistically significant difference, Subset selection, xno}
}

@inproceedings{chen_applications_2018,
	title = {Applications of psychological science for actionable analytics},
	isbn = {978-1-4503-5573-5},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058324185&doi=10.1145%2f3236024.3236050&partnerID=40&md5=35cabe5a4dba3791a11f73e0a25c6d7d},
	doi = {10.1145/3236024.3236050},
	abstract = {According to psychological scientists, humans understand models that most match their own internal models, which they characterize as lists of łheuristicžs (i.e. lists of very succinct rules). One such heuristic rule generator is the Fast-and-Frugal Trees ({FFT}) preferred by psychological scientists. Despite their successful use in many applied domains, {FFTs} have not been applied in software analytics. Accordingly, this paper assesses {FFTs} for software analytics. We ind that {FFTs} are remarkably efective in that their models are very succinct (5 lines or less describing a binary decision tree) while also outperforming result from very recent, top-level, conference papers. Also, when we restrict training data to operational attributes (i.e., those attributes that are frequently changed by developers), the performance of {FFTs} are not efected (while the performance of other learners can vary wildly). Our conclusions are two-fold. Firstly, there is much that software analytics community could learn from psychological science. Secondly, proponents of complex methods should always baseline those methods against simpler alternatives. For example, {FFTs} could be used as a standard baseline learner against which other software analytics tools are compared. © 2018 Association for Computing Machinery.},
	pages = {456--467},
	booktitle = {{ESEC}/{FSE} 2018 - Proceedings of the 2018 26th {ACM} Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
	publisher = {Association for Computing Machinery, Inc},
	author = {Chen, D. and Fu, W. and Krishna, R. and Menzies, T.},
	date = {2018},
	keywords = {Empirical studies, Decision trees, Software engineering, Defect prediction, Binary decision trees, Binary trees, Complex methods, Conference papers, heuristics, Internal models, psychological science, xno}
}

@inproceedings{walkinshaw_are_2018,
	title = {Are 20\% of files responsible for 80\% of defects?},
	isbn = {978-1-4503-5823-1},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061506473&doi=10.1145%2f3239235.3239244&partnerID=40&md5=f55dd5ef8cf54c8bab006adc09c21cb7},
	doi = {10.1145/3239235.3239244},
	abstract = {Background: Over the past two decades a mixture of anecdote from the industry and empirical studies from academia have suggested that the 80:20 rule (otherwise known as the Pareto Principle) applies to the relationship between source code files and the number of defects in the system: a small minority of files (roughly 20\%) are responsible for a majority of defects (roughly 80\%). Aims: This paper aims to establish how widespread the phenomenon is by analysing 100 systems (previous studies have focussed on between one and three systems), with the goal of whether and under what circumstances this relationship does hold, and whether the key files can be readily identified from basic metrics. Method: We devised a search criterion to identify defect fixes from commit messages and used this to analyse 100 active Github repositories, spanning a variety of languages and domains. We then studied the relationship between files, basic metrics (churn and {LOC}), and defect fixes. Results: We found that the Pareto principle does hold, but only if defects that incur fixes to multiple files count as multiple defects. When we investigated multi-file fixes, we found that key files (belonging to the top 20\%) are commonly fixed alongside other much less frequently-fixed files. We found {LOC} to be poorly correlated with defect proneness, Code Churn was a more reliable indicator, but only for extremely high values of Churn. Conclusions: It is difficult to reliably identify the "most fixed" 20\% of files from basic metrics. However, even if they could be reliably predicted, focussing on them would probably be misguided. Although fixes will naturally involve files that are often involved in other fixes too, they also tend to include other less frequently-fixed files. © 2018 {ACM}.},
	booktitle = {International Symposium on Empirical Software Engineering and Measurement},
	publisher = {{IEEE} Computer Society},
	author = {Walkinshaw, N. and Minku, L.},
	date = {2018},
	note = {{ISSN}: 19493770},
	keywords = {Defects, Empirical studies, Software engineering, Defect distribution, Defect proneness, Multiple defects, Pareto principle, Search criterion, Source codes, Surveying, xno}
}

@inproceedings{amasaki_cross-version_2018,
	title = {Cross-version defect prediction using cross-project defect prediction approaches: Does it work?},
	isbn = {978-1-4503-6593-2},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056731345&doi=10.1145%2f3273934.3273938&partnerID=40&md5=4e9a55dd5411a2ac5b51ea8893ac4538},
	doi = {10.1145/3273934.3273938},
	abstract = {Background: Specifying and removing defects before release deserve extra cost for the success of software projects. Long-running projects experience multiple releases, and it is a natural choice to adopt cross-version defect prediction ({CVDP}) that uses information from older versions. A past study shows that feeding multi older versions data may have a positive influence on the performance. The study also suggests that cross-project defect prediction ({CPDP}) may fit the situation but one {CPDP} approach was only examined. Aims: To investigate whether feeding multiple older versions data is effective for {CVDP} using {CPDP} approaches. The investigation also involves performance comparisons of the {CPDP} approaches under {CVDP} situation. Method: We chose a style of replication of the comparative study on {CPDP} approaches by Herbold et al. under {CVDP} situation. Results: Feeding multiple older versions had a positive effect for more than a half {CPDP} approaches. However, almost all of the {CPDP} approaches did not perform significantly better than a simple rule-based prediction. Although the best {CPDP} approach could work better than it and with-in project defect prediction, we found no effect of feeding multiple older versions for it. Conclusions: Feeding multiple older versions could improve {CPDP} approaches under {CVDP} situation. However, it did not work for the best {CPDP} approach in the study. © 2018 Association for Computing Machinery.},
	pages = {32--41},
	booktitle = {{ACM} International Conference Proceeding Series},
	publisher = {Association for Computing Machinery},
	author = {Amasaki, S.},
	date = {2018},
	keywords = {Defects, Forecasting, Predictive analytics, Software engineering, Defect prediction, Comparative studies, Feeding, Multiple release, Performance comparison, Rule based, Software project, xyes}
}

@article{watanabe_cross-validation-based_2018,
	title = {Cross-validation-based association rule prioritization metric for software defect characterization},
	volume = {E101D},
	issn = {09168532},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053838288&doi=10.1587%2ftransinf.2018EDP7020&partnerID=40&md5=9ec0a3efa0bc78a0699afdc104ea6de9},
	doi = {10.1587/transinf.2018EDP7020},
	abstract = {Association rule mining discovers relationships among variables in a data set, representing them as rules. These are expected to often have predictive abilities, that is, to be able to predict future events, but commonly used rule interestingness measures, such as support and confidence, do not directly assess their predictive power. This paper proposes a cross-validation-based metric that quantifies the predictive power of such rules for characterizing software defects. The results of evaluation this metric experimentally using four open-source data sets (Mylyn, {NetBeans}, Apache Ant and {jEdit}) show that it can improve rule prioritization performance over conventional metrics (support, confidence and odds ratio) by 72.8\%for Mylyn, 15.0\%for {NetBeans}, 10.5\%for Apache Ant and 0 for {jEdit} in terms of {SumNormPre}(100) precision criterion. This suggests that the proposed metric can provide better rule prioritization performance than conventional metrics and can at least provide similar performance even in the worst case. © 2018 The Institute of Electronics, Information and Communication Engineers.},
	pages = {2269--2278},
	number = {9},
	journaltitle = {{IEICE} Transactions on Information and Systems},
	author = {Watanabe, T. and Monden, A. and Yücel, Z. and Kamei, Y. and Morisaki, S.},
	date = {2018},
	note = {Publisher: Institute of Electronics, Information and Communication, Engineers, {IEICE}},
	keywords = {Data mining, Computer software selection and evaluation, Defects, Open source software, Association rules, Software Quality, Cross validation, Defect prediction, Precision criteria, Predictive abilities, Rule interestingness, Rule prioritization, Support and confidence, xno},
	file = {Full Text:C\:\\Users\\michalm\\Zotero\\storage\\C785N42S\\Watanabe et al. - 2018 - Cross-Validation-Based Association Rule Prioritiza.pdf:application/pdf}
}

@article{chatterjee_mahalanobis_2018,
	title = {A Mahalanobis distance based algorithm for assigning rank to the predicted fault prone software modules},
	volume = {70},
	issn = {15684946},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049096980&doi=10.1016%2fj.asoc.2018.06.032&partnerID=40&md5=ed49f211663087ac70467c0e841e7f11},
	doi = {10.1016/j.asoc.2018.06.032},
	abstract = {This article proposes a methodology based on Artificial Neural Network({ANN}) and type-2 fuzzy logic system ({FLS}) for detecting the fault prone software modules at early development phase. The present research concentrates on software metrics from requirement analysis and design phase of software life cycle. A new approach has been developed to sort out degree of fault proneness ({DFP}) of the software modules through type-2 {FLS}. {ANN} is used to prepare the rule base for inference engine. Furthermore, the proposed model has induced an order relation among the fault prone modules ({FPMs}) with the help of Mahalanobis distance ({MD}) metric. During software development process, a project manager needs to recognize the fault prone software modules with their {DFP}. Hence, the present study is of great importance to the project personnel to develop more cost-effective and reliable software. {KC}2 dataset of {NASA} has been applied for validating the model. Performance analysis clearly indicates the better prediction capability of the proposed model compared to some existing similar models. © 2018 Elsevier B.V.},
	pages = {764--772},
	journaltitle = {Applied Soft Computing Journal},
	author = {Chatterjee, S. and Maji, B.},
	date = {2018},
	note = {Publisher: Elsevier Ltd},
	keywords = {Software metrics, Computer circuits, Software design, {NASA}, Software modules, Fuzzy inference, Fuzzy logic, Interval type-2 fuzzy logic systems, Life cycle, Neural networks, Cost effectiveness, Mahalanobis distances, Performance analysis, Prediction capability, Software development process, Type-2 fuzzy logic system, Artificial neural network, Fault prone software module, Interval type-2 fuzzy logic system, Mahalanobis distance, xno}
}

@article{sharma_design_2018,
	title = {Design of testing framework for code smell detection ({OOPS}) using {BFO} algorithm},
	volume = {7},
	issn = {2227524X},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082344132&partnerID=40&md5=e133bf1f40f323e146d994e8b508c80a},
	abstract = {Detection of bad smells refers to any indication in the program code of a execution that perhaps designate a issue, maintain the software and software evolution. Code Smell detection is a main challenging for software developers and their informal classification direct to the designing of various smell detection methods and software tools. It appraises 4 code smell detection tool in software like as a in Fusion, {JDeodorant}, {PMD} and Jspirit. In this research proposes a method for detection the bad code smells in software is called as code smell. Bad smell detection in software, {OOSMs} are used to identify the Source Code whereby Plug-in were implemented for code detection in which position of program initial code the bad smell appeared so that software refactoring can then acquire position. Classified the code smell, as a type of codes: long method, {PIH}, {LPL}, {LC}, {SS} and {GOD} class etc. Detection of the code smell and as a result applying the correct detection phases when require is significant to enhance the Quality of the code or program. The various tool has been proposed for detection of the code smell each one featured by particular properties. The main objective of this research work described our proposed method on using various tools for code smell detection. We find the major differences between them and dissimilar consequences we attained. The major drawback of current research work is that it focuses on one particular language which makes them restricted to one kind of programs only. These tools fail to detect the smelly code if any kind of change in environment is encountered. The base paper compares the most popular code smell detection tools on basis of various factors like accuracy, False Positive Rate etc. which gives a clear picture of functionality these tools possess. In this paper, a unique technique is designed to identify {CSs}. For this purpose, various object-oriented programming ({OOPs})-based-metrics with their maintainability index are used. Further, code refactoring and optimization technique are applied to obtain low maintainability Index. Finally, the proposed scheme is evaluated to achieve satisfactory results. The results of the {BFOA} test defined that the lazy class caused framework defects in {DLS}, {DR}, and {SE}. However, the {LPL} caused no frame-work defects what so ever. The consequences of the connection rules test searched that the {LCCS} (Lazy Class Code Smell) caused structured defects in {DE} and {DLS}, which corresponded to the consequences of the {BFOA} test. In this research work, a proposed method is designed to verify the code smell. For this purpose, different {OOPs} based Software Metrics with their {MI} (Maintainability Index) are utilized. Further Code refactoring and optimization method id applied to attained the less maintainability index and evaluated to achieved satisfactory results. © 2018 Pratiksha Sharma, Er. Arshpreet Kaur.},
	pages = {161--166},
	number = {2},
	journaltitle = {International Journal of Engineering and Technology({UAE})},
	author = {Sharma, P. and Kaur, E.A.},
	date = {2018},
	note = {Publisher: Science Publishing Corporation Inc},
	keywords = {xno}
}

@article{pattnaik_empirical_2018,
	title = {Empirical analysis of software quality prediction using a {TRAINBFG} algorithm},
	volume = {7},
	issn = {2227524X},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067271067&partnerID=40&md5=bbf61e10f0de3da8953415cd3b3f9aaf},
	abstract = {Software quality plays a major role in software fault proneness. That's why prediction of software quality is essential for measuring the anticipated faults present in the software. In this paper we have proposed a Neuro-Fuzzy model for prediction of probable values for a predefined set of software characteristics by virtue of using a rule base. In course of it, we have used several training algorithms among which {TRAINBFG} algorithm is observed to be the best one for the purpose. There are various training algorithm available in {MATLAB} for training the neural network input data set. The prediction using fuzzy logic and neural network provides better result in comparison with only neural network. We find out from our implementation that {TRAINBFG} algorithm can provide better predicted value as com-pared to other algorithm in {MATLAB}. We have validated this result using the tools like {SPSS} and {MATLAB}. © 2018, Science Publishing Corporation Inc.},
	pages = {259--268},
	number = {2},
	journaltitle = {International Journal of Engineering and Technology({UAE})},
	author = {Pattnaik, S. and Pattanayak, B.K.},
	date = {2018},
	note = {Publisher: Science Publishing Corporation Inc},
	keywords = {xyes}
}

@article{packevicius_text_2018,
	title = {Text Semantics and Layout Defects Detection in Android Apps Using Dynamic Execution and Screenshot Analysis},
	volume = {920},
	issn = {18650929},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053611487&doi=10.1007%2f978-3-319-99972-2_22&partnerID=40&md5=fac01481f37328de02fe09ae87560298},
	doi = {10.1007/978-3-319-99972-2_22},
	abstract = {The paper presents classification of the text defects. It provides a list of user interface text defects and the method based on static/dynamic code analysis for detecting defects in Android applications. This paper proposes a list of static analysis rules for detecting every defect and the tool model implementing those rules. The method and the tool are based on the application of multiple Android application emulators, execution of the application through certain execution paths on multiple hardware and software configurations while taking application screen-shots. The defects are identified by running analysis rules on each taken screen-shot and searching for defect patterns. The results are presented by testing sample Android application. © 2018, Springer Nature Switzerland {AG}.},
	pages = {279--292},
	journaltitle = {Communications in Computer and Information Science},
	author = {Packevičius, Š. and Barisas, D. and Ušaniov, A. and Guogis, E. and Bareiša, E.},
	date = {2018},
	note = {{ISBN}: 9783319999715
Publisher: Springer Verlag},
	keywords = {Defects, Defect patterns, Semantics, Hardware and software, Android applications, Application programs, Android (operating system), Defects detection, Detecting defects, Dynamic execution, Execution paths, Static analysis, Testing samples, Text processing, User interfaces, xno}
}

@article{macdonald_software_2018,
	title = {Software defect prediction from code quality measurements via machine learning},
	volume = {10832 {LNAI}},
	issn = {03029743},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046366514&doi=10.1007%2f978-3-319-89656-4_35&partnerID=40&md5=8e02a1262a6a833277f2b80bb4bc281b},
	doi = {10.1007/978-3-319-89656-4_35},
	abstract = {Improvement in software development practices to predict and reduce software defects can lead to major cost savings. The goal of this study is to demonstrate the value of static analysis metrics in predicting software defects at a much larger scale than previous efforts. The study analyses data collected from more than 500 software applications, across 3 multi-year software development programs, and uses over 150 software static analysis measurements. A number of machine learning techniques such as neural network and random forest are used to determine whether seemingly innocuous rule violations can be used as significant predictors of software defect rates. © Springer International Publishing {AG}, part of Springer Nature 2018.},
	pages = {331--334},
	journaltitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {{MacDonald}, R.},
	date = {2018},
	note = {{ISBN}: 9783319896557
Publisher: Springer Verlag},
	keywords = {Defects, Software design, Software defect prediction, Software defects, Forecasting, Learning systems, Decision trees, Random forests, Machine learning techniques, Artificial intelligence, Defect prediction, Application programs, Static analysis, Software applications, Software development practices, Software development projects, xno}
}

@inproceedings{szczypinski_qmazda_2017,
	title = {{QMaZda} - Software tools for image analysis and pattern recognition},
	volume = {2017-September},
	isbn = {978-83-62065-30-1},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041504873&doi=10.23919%2fSPA.2017.8166867&partnerID=40&md5=09992260f6b97c545049c6f6442bd115},
	doi = {10.23919/SPA.2017.8166867},
	abstract = {Qmazda is a package of software tools for digital image analysis. They compute shape, color and texture attributes in arbitrary regions of interest, implement selected algorithms of discriminant analysis and machine learning, and enable texture based image segmentation. The algorithms generalize a concept of texture to three-dimensional data to enable analysis of volumetric images from magnetic resonance imaging or computed tomography scanners. The tools support a complete workflow - from image examples as an input to classification rules as an output. The extracted knowledge can be further used in custom made image analysis systems. Here we also present an application of {QMaZda} to identify defective barley kernels. The cereal seeds variability is high, therefore, characterization and discriminant analysis of such the biological objects is challenging and non-trivial. The software is available free of charge and open source, with executables for Windows, Linux and {OS} X platforms. © 2017 Division of Signal Processing and Electronic Systems, Poznan University of Technology.},
	pages = {217--221},
	booktitle = {Signal Processing - Algorithms, Architectures, Arrangements, and Applications Conference Proceedings, {SPA}},
	publisher = {{IEEE} Computer Society},
	author = {Szczypinski, P.M. and Klepaczko, A. and Kociolek, M.},
	date = {2017},
	note = {{ISSN}: 23260262},
	keywords = {Artificial intelligence, Biological objects, cereal grains, Cereal grains, Classification rules, Computed tomography scanners, Computer architecture, Computer operating systems, Computer software, Computerized tomography, {DH}-{HEMTs}, Digital image analysis, Discriminant analysis, feature extraction, Feature extraction, High definition video, Image analysis, Image analysis systems, Image processing, Image segmentation, Image texture, Integrated circuits, Learning systems, machine learning, Magnetic resonance imaging, Open source software, Open systems, Pattern recognition, Regions of interest, Signal processing, Signal processing algorithms, Three-dimensional data, xno}
}

@article{chatterjee_software_2017,
	title = {Software fault prediction using neuro-fuzzy network and evolutionary learning approach},
	volume = {28},
	issn = {09410643},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976253647&doi=10.1007%2fs00521-016-2437-y&partnerID=40&md5=cbd4f26d3720adad0b1ca215e84179e8},
	doi = {10.1007/s00521-016-2437-y},
	abstract = {In the real world, a great deal of information is provided by human experts that normally do not conform to the rules of physics, but describe the complicated systems by a set of incomplete or vague statements. The need of conducting uncertainty analysis in software reliability for the large and complex system is demanding. For large complex systems made up of many components, the uncertainty of each individual parameter amplifies the uncertainty of the total system reliability. In this paper, to overcome with the problem of uncertainty in software development process and environment, a neuro-fuzzy modeling has been proposed for software fault prediction. The training of the proposed neuro-fuzzy model has been done with genetic algorithm and back-propagation learning algorithm. The proposed model has been validated using some real software failure data. The efficiency of the two learning algorithms has been compared with various fuzzy and statistical time series-based forecasting algorithms on the basis of their prediction ability. © 2016, The Natural Computing Applications Forum.},
	pages = {1221--1231},
	journaltitle = {Neural Computing and Applications},
	author = {Chatterjee, S. and Nigam, S. and Roy, A.},
	date = {2017},
	note = {Publisher: Springer London},
	keywords = {Software design, Forecasting, Genetic algorithms, Software engineering, Reliability analysis, Software reliability, Complex networks, Software fault prediction, Learning algorithms, Computer software, Fuzzy inference, Fuzzy logic, Reliability, Software development process, Algorithms, Backpropagation, Backpropagation algorithms, Backpropagation learning algorithm, Evolutionary Learning, Faulting, Forecasting algorithm, Fuzzy neural networks, Large complex systems, Large scale systems, Neuro-fuzzy network, Total system reliability, Uncertainty analysis, xno}
}

@article{li_what_2017,
	title = {What Are They Talking About? Analyzing Code Reviews in Pull-Based Development Model},
	volume = {32},
	issn = {10009000},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037378589&doi=10.1007%2fs11390-017-1783-2&partnerID=40&md5=27a059207de0baa870b8a5232ac49601},
	doi = {10.1007/s11390-017-1783-2},
	abstract = {Code reviews in pull-based model are open to community users on {GitHub}. Various participants are taking part in the review discussions and the review topics are not only about the improvement of code contributions but also about project evolution and social interaction. A comprehensive understanding of the review topics in pull-based model would be useful to better organize the code review process and optimize review tasks such as reviewer recommendation and pull-request prioritization. In this paper, we first conduct a qualitative study on three popular open-source software projects hosted on {GitHub} and construct a fine-grained two-level taxonomy covering four level-1 categories (code correctness, pull-request decision-making, project management, and social interaction) and 11 level-2 subcategories (e.g., defect detecting, reviewer assigning, contribution encouraging). Second, we conduct preliminary quantitative analysis on a large set of review comments that were labeled by {TSHC} (a two-stage hybrid classification algorithm), which is able to automatically classify review comments by combining rule-based and machine-learning techniques. Through the quantitative study, we explore the typical review patterns. We find that the three projects present similar comments distribution on each subcategory. Pull-requests submitted by inexperienced contributors tend to contain potential issues even though they have passed the tests. Furthermore, external contributors are more likely to break project conventions in their early contributions. © 2017, Springer Science+Business Media, {LLC} \& Science Press, China.},
	pages = {1060--1075},
	number = {6},
	journaltitle = {Journal of Computer Science and Technology},
	author = {Li, Z.-X. and Yu, Y. and Yin, G. and Wang, T. and Wang, H.-M.},
	date = {2017},
	note = {Publisher: Springer New York {LLC}},
	keywords = {Open source software, Learning systems, Machine learning techniques, Open systems, Software engineering, Project management, Codes (symbols), Code review, Hybrid classification, Decision making, Open source software projects, pull-request, Qualitative study, Quantitative study, Social interactions, Social sciences, xno}
}

@article{mahela_recognition_2017,
	title = {Recognition of power quality disturbances using S-transform based ruled decision tree and fuzzy C-means clustering classifiers},
	volume = {59},
	issn = {15684946},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020696918&doi=10.1016%2fj.asoc.2017.05.061&partnerID=40&md5=8fe15d579c48b349b0ed989a9a961e05},
	doi = {10.1016/j.asoc.2017.05.061},
	abstract = {A method based on Stockwell's transform and Fuzzy C-means ({FCM}) clustering initialized by decision tree has been proposed in this paper for detection and classification of power quality ({PQ}) disturbances. Performance of this method is compared with S-transform based ruled decision tree. {PQ} disturbances are simulated in conformity with standard {IEEE}-1159 using {MATLAB} software. Different statistical features of {PQ} disturbance signals are obtained using Stockwell's transform based multi-resolution analysis of signals. These features are given as input to the proposed techniques such as rule-based decision tree and {FCM} clustering initialized by ruled decision tree for classification of various {PQ} disturbances. The {PQ} disturbances investigated in this study include voltage swell, voltage sag, interruption, notch, harmonics, spike, flicker, impulsive transient and oscillatory transient. It has been observed that the efficiency of classification based on ruled decision tree deteriorates in the presence of noise. However, the classification based on Fuzzy C-means clustering initialized by decision tree gives results with high accuracy even in the noisy environment. Validity of simulation results has been verified through comparisons with results in real time obtained using the Real Time Digital Simulator ({RTDS}) in hardware synchronization mode. The proposed algorithm is established effectively by results of high accuracy to detect and classify various electrical power quality disturbances. © 2017 Elsevier B.V.},
	pages = {243--257},
	journaltitle = {Applied Soft Computing Journal},
	author = {Mahela, O.P. and Shaik, A.G.},
	date = {2017},
	note = {Publisher: Elsevier Ltd},
	keywords = {Decision trees, {MATLAB}, Pattern recognition, Electric fault currents, Fuzzy C means clustering, Oscillatory transients, Power quality, Power quality disturbances, Real time digital simulator, Statistical features, Fuzzy systems, Mathematical transformations, Disturbance signals, Electrical power quality, Rule-based decision trees, xno}
}

@article{rathore_towards_2017,
	title = {Towards an ensemble based system for predicting the number of software faults},
	volume = {82},
	issn = {09574174},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018510580&doi=10.1016%2fj.eswa.2017.04.014&partnerID=40&md5=fff8aa3402a4ce72c295edb2460e4e42},
	doi = {10.1016/j.eswa.2017.04.014},
	abstract = {Software fault prediction using different techniques has been done by various researchers previously. It is observed that the performance of these techniques varied from dataset to dataset, which make them inconsistent for fault prediction in the unknown software project. On the other hand, use of ensemble method for software fault prediction can be very effective, as it takes the advantage of different techniques for the given dataset to come up with better prediction results compared to individual technique. Many works are available on binary class software fault prediction (faulty or non-faulty prediction) using ensemble methods, but the use of ensemble methods for the prediction of number of faults has not been explored so far. The objective of this work is to present a system using the ensemble of various learning techniques for predicting the number of faults in given software modules. We present a heterogeneous ensemble method for the prediction of number of faults and use a linear combination rule and a non-linear combination rule based approaches for the ensemble. The study is designed and conducted for different software fault datasets accumulated from the publicly available data repositories. The results indicate that the presented system predicted number of faults with higher accuracy. The results are consistent across all the datasets. We also use prediction at level l (Pred(l)), and measure of completeness to evaluate the results. Pred(l) shows the number of modules in a dataset for which average relative error value is less than or equal to a threshold value l. The results of prediction at level l analysis and measure of completeness analysis have also confirmed the effectiveness of the presented system for the prediction of number of faults. Compared to the single fault prediction technique, ensemble methods produced improved performance for the prediction of number of software faults. Main impact of this work is to allow better utilization of testing resources helping in early and quick identification of most of the faults in the software system. © 2017 Elsevier Ltd},
	pages = {357--382},
	journaltitle = {Expert Systems with Applications},
	author = {Rathore, S.S. and Kumar, S.},
	date = {2017},
	note = {Publisher: Elsevier Ltd},
	keywords = {Empirical studies, Forecasting, Genetic algorithms, Software testing, Software fault prediction, Computer software, Average relative error, Ensemble-based systems, Genetic programming, Gradient boosting, Heterogeneous ensembles, Linear combination rules, Linear regression, Promise repository, xno}
}

@article{menzies_are_2017,
	title = {Are delayed issues harder to resolve? Revisiting cost-to-fix of defects throughout the lifecycle},
	volume = {22},
	issn = {13823256},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994709940&doi=10.1007%2fs10664-016-9469-x&partnerID=40&md5=c8ae911a0e1c66129a142dc8e1dd79b8},
	doi = {10.1007/s10664-016-9469-x},
	abstract = {Many practitioners and academics believe in a delayed issue effect ({DIE}); i.e. the longer an issue lingers in the system, the more effort it requires to resolve. This belief is often used to justify major investments in new development processes that promise to retire more issues sooner. This paper tests for the delayed issue effect in 171 software projects conducted around the world in the period from 2006–2014. To the best of our knowledge, this is the largest study yet published on this effect. We found no evidence for the delayed issue effect; i.e. the effort to resolve issues in a later phase was not consistently or substantially greater than when issues were resolved soon after their introduction. This paper documents the above study and explores reasons for this mismatch between this common rule of thumb and empirical data. In summary, {DIE} is not some constant across all projects. Rather, {DIE} might be an historical relic that occurs intermittently only in certain kinds of projects. This is a significant result since it predicts that new development processes that promise to faster retire more issues will not have a guaranteed return on investment (depending on the context where applied), and that a long-held truth in software engineering should not be considered a global truism. © 2016, Springer Science+Business Media New York.},
	pages = {1903--1935},
	number = {4},
	journaltitle = {Empirical Software Engineering},
	author = {Menzies, T. and Nichols, W. and Shull, F. and Layman, L.},
	date = {2017},
	note = {Publisher: Springer New York {LLC}},
	keywords = {Software engineering, Software testing, Development process, Software project, Costs, Economics, Empirical data, Paper documents, Paper tests, Phase delay, Rule of thumb, Software economics, xno}
}

@article{dhanalaxmi_rule-based_2017,
	title = {A rule-based prediction method for defect detection in software system},
	volume = {95},
	issn = {19928645},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026770283&partnerID=40&md5=5b4b377b1bb3a7cec928c6c88d3fbef8},
	abstract = {Software is a complex object that consists of different modules with changing degrees of defect occurrence. By efficiently and appropriate predicting the frequency of defects in software, software project managers can better utilize their workforce, cost and time to obtain better quality assurance. This paper proposes a rule-based prediction ({RBP}) method for defect detection and for planing the better maintenance strategy, which can support in the forecast a defective or non-defective software module before it can deploy for any software project. The {RBP} extends the Ripple-down rule ({RDR}) classifier method to construct an effective rule-basedmodel for accurately classifying the software defects. The method will enhance the software defect prediction so that software testers can spend more time in testing those components which are expected to contain errors. The experiment evaluation is performed over a software repository datasets and the obtained results showa satisfactory improvement. © 2005 – ongoing {JATIT} \& {LLS}.},
	pages = {3403--3412},
	number = {14},
	journaltitle = {Journal of Theoretical and Applied Information Technology},
	author = {Dhanalaxmi, B. and Appa Rao Naidu, G. and Anuradha, K.},
	date = {2017},
	note = {Publisher: Asian Research Publishing Network},
	keywords = {xyes}
}

@article{venkatesh_pragmatic_2017,
	title = {Pragmatic investigation on performance of instance-based learning algorithms},
	volume = {517},
	issn = {21945357},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027095691&doi=10.1007%2f978-981-10-3174-8_53&partnerID=40&md5=135e179eb44712e404cb994e00439c1d},
	doi = {10.1007/978-981-10-3174-8_53},
	abstract = {In the recent past, there has been increasing usage of machine learning algorithms for classifying the data for various real-world applications such as weather prediction, medical diagnosis, network intrusion detection, software fault detection, etc. The instance-based learning algorithms play a major role in the classification process since they do not learn the data until the need of the developing the classification model. Therefore, these learning algorithms are called as lazy learning algorithms and implemented in various applications. However, there is a pressing need among the researchers to analyze the performance of various types of the instance-based classifier. Therefore, this chapter presents a pragmatic investigation on performance of the instance-based learning algorithms. In order to conduct this analysis, different instance-based classifier namely instance-based one ({IB}1), instance-based K ({IBK}), Kstar, lazy learning of Bayesian rules ({LBR}), locally weighted learning ({LWL}) are adopted. The performance of these algorithms is evaluated in terms of sensitivity, specificity, and accuracy on various real-world datasets. © Springer Nature Singapore Pte Ltd. 2017.},
	pages = {633--642},
	journaltitle = {Advances in Intelligent Systems and Computing},
	author = {Venkatesh, B. and Singh, D.A.A.G. and Leavline, E.J.},
	date = {2017},
	note = {{ISBN}: 9789811031731
Publisher: Springer Verlag},
	keywords = {Computer aided diagnosis, Learning systems, Fault detection, Artificial intelligence, Learning algorithms, Application programs, Intrusion detection, Software fault detection, Classification models, Classification process, Diagnosis, Evolutionary algorithms, Instance based learning, Instance-based classifiers, Kstar, Locally weighted learning, Network intrusion detection, xno}
}

@article{goyal_fuzzy_2017,
	title = {Fuzzy inferencing to identify degree of interaction in the development of fault prediction models},
	volume = {29},
	issn = {13191578},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006791416&doi=10.1016%2fj.jksuci.2014.12.008&partnerID=40&md5=3e0f7210b23221ef38cf0b4799ed0bdf},
	doi = {10.1016/j.jksuci.2014.12.008},
	abstract = {The software fault prediction models, based on different modeling techniques have been extensively researched to improve software quality for the last three decades. Out of the analytical techniques used by the researchers, fuzzy modeling and its variants are bringing out a major share of the attention of research communities. In this work, we demonstrate the models developed through data driven fuzzy inference system. A comprehensive set of rules induced by such an inference system, followed by a simplification process provides deeper insight into the linguistically identified level of interaction. This work makes use of a publicly available data repository for four software modules, advocating the consideration of compound effects in the model development, especially in the area of software measurement. One related objective is the identification of influential metrics in the development of fault prediction models. A fuzzy rule intrinsically represents a form of interaction between fuzzified inputs. Analysis of these rules establishes that Low and {NOT} (High) level of inheritance based metrics significantly contributes to the F-measure estimate of the model. Further, the Lack of Cohesion of Methods ({LCOM}) metric was found insignificant in this empirical study. © 2015 The Authors},
	pages = {93--102},
	number = {1},
	journaltitle = {Journal of King Saud University - Computer and Information Sciences},
	author = {Goyal, R. and Chandra, P. and Singh, Y.},
	date = {2017},
	note = {Publisher: King Saud bin Abdulaziz University},
	keywords = {Software fault prediction, Object oriented metrics, Fuzzy inference system, Influential metrics, xyes}
}

@article{azar_combined_2016,
	title = {A combined ant colony optimization and simulated annealing algorithm to assess stability and fault-proneness of classes based on internal software quality attributes},
	volume = {14},
	issn = {09740635},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984844840&partnerID=40&md5=823cfb7db80cb132bf6d3f9eda762275},
	abstract = {Several machine learning algorithms have been used to assess external quality attributes of software systems. Given a set of metrics that describe internal software attributes (cohesion, complexity, size, etc.), the purpose is to construct a model that can be used to assess external quality attributes (stability, reliability, maintainability, etc.) based on the internal ones. Most of these algorithms result in assessment models that are hard to generalize. As a result, they show a degradation in their assessment performance when used to estimate quality of new software modules. This paper presents a hybrid heuristic to construct software quality estimation models that can be used to predict software quality attributes of new unseen systems prior to re-using them or purchasing them. The technique relies on two heuristics: simulated annealing and ant colony optimization. It learns from the data available in a particular domain guidelines and rules to achieve a particular external software quality. These guidelines are presented as rule-based logical models. We validate our technique on two software quality attributes namely stability and fault-proneness - a subattribute of maintainability. We compare our technique to two state-of-the-art algorithms: Neural Networks ({NN}) and C4.5 as well as to a previously published Ant Colony Optimization algorithm. Results show that our hybrid technique out-performs both C4.5 and {ACO} in most of the cases. Compared to {NN}, our algorithm preserves the white-box nature of the predictive models hence, giving not only the classification of a particular module but also guidelines for software engineers to follow in order to reach a particular external quality attribute. Our algorithm gives promising results and is generic enough to apply to any software quality attribute. © 2016 [International Journal of Artificial Intelligence].},
	pages = {137--156},
	number = {2},
	journaltitle = {International Journal of Artificial Intelligence},
	author = {Azar, D. and Fayad, K. and Daoud, C.},
	date = {2016},
	note = {Publisher: {CESER} Publications},
	keywords = {xno}
}

@article{chatterjee_new_2016,
	title = {A new fuzzy rule based algorithm for estimating software faults in early phase of development},
	volume = {20},
	issn = {14327643},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84932188500&doi=10.1007%2fs00500-015-1738-x&partnerID=40&md5=7befbdbcf21dcc50b99c0587eaeacc49},
	doi = {10.1007/s00500-015-1738-x},
	abstract = {Estimation of reliability and the number of faults present in software in its early development phase, i.e., requirement analysis or design phase is very beneficial for developing reliable software with optimal cost. Software reliability prediction in early phase of development is highly desirable to the stake holders, software developers, managers and end users. Since, the failure data are unavailable in early phase of software development, different reliability relevant software metrics and similar project data are used to develop models for early software fault prediction. The proposed model uses the linguistic values of software metrics in fuzzy inference system to predict the total number of faults present in software in its requirement analysis phase. Considering specific target reliability, weightage of each input software metrics and size of software, an algorithm has been proposed here for developing general fuzzy rule base. For model validation of the proposed model, 20 real software project data have been used here. The linguistic values from four software metrics related to requirement analysis phase have been considered as model inputs. The performance of the proposed model has been compared with two existing early software fault prediction models. © 2015, Springer-Verlag Berlin Heidelberg.},
	pages = {4023--4035},
	number = {10},
	journaltitle = {Soft Computing},
	author = {Chatterjee, S. and Maji, B.},
	date = {2016},
	note = {Publisher: Springer Verlag},
	keywords = {Software metrics, Software design, Forecasting, Reliability analysis, Software reliability, Computer software, Fuzzy inference, Fuzzy rules, Fuzzy systems, Early fault, Fuzzy rule base, Fuzzy inference systems, Reliability, Algorithms, Computational linguistics, Linguistics, Requirement analysis, xyes},
	file = {Chatterjee and Maji - 2016 - A new fuzzy rule based algorithm for estimating so.pdf:C\:\\Users\\michalm\\Zotero\\storage\\X46IHY9V\\Chatterjee and Maji - 2016 - A new fuzzy rule based algorithm for estimating so.pdf:application/pdf}
}

@article{dickson_identifying_2016,
	title = {Identifying the controls on coastal cliff landslides using machine-learning approaches},
	volume = {76},
	issn = {13648152},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951567957&doi=10.1016%2fj.envsoft.2015.10.029&partnerID=40&md5=f4d8e51b015891ff0c208023e4c773c7},
	doi = {10.1016/j.envsoft.2015.10.029},
	abstract = {Transformations are underway in our ability to collect and interrogate remotely sensed data. Here we explore the utility of three machine-learning methods for identifying the controls on coastal cliff landsliding using a dataset from Auckland, New Zealand. Models were built using all available data with a resampling approach used to evaluate uncertainties. All methods identify two dominant landslide predictors (unfailed cliff slope angle and fault proximity). This information could support a range of management approaches, from the development of 'rules-of-thumb' to detailed models that incorporate all predictor information. In our study all statistical approaches correctly predict a high proportion ({\textgreater}85\%) of cases. Similar 'success' has been shown in other studies, but important questions should be asked about possible error sources, particularly in regard to absence data. In coastal landslide studies sign decay is a vexing issue, because sites prone to landsliding may also be sites of rapid evidence removal. © 2015 Elsevier Ltd..},
	pages = {117--127},
	journaltitle = {Environmental Modelling and Software},
	author = {Dickson, M.E. and Perry, G.L.W.},
	date = {2016},
	note = {Publisher: Elsevier Ltd},
	keywords = {Learning systems, Artificial intelligence, Machine learning approaches, Auckland, cliff, Cliffs, coastal zone, Erosion, erosion control, Landforms, landslide, Landslides, Landsliding, machine learning, Maxent, New Zealand, North Island, Re-sampling approach, regression analysis, Regression trees, Remotely sensed data, software, Three machine learning methods, xno}
}

@article{ampatzoglou_quality_2016,
	title = {Quality rule violations in sharepoint applications: An empirical study in industry},
	volume = {10027 {LNCS}},
	issn = {03029743},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84998979464&doi=10.1007%2f978-3-319-49094-6_35&partnerID=40&md5=3f4f96bcded4c4dfa06e5324b36fb1d9},
	doi = {10.1007/978-3-319-49094-6_35},
	abstract = {In this paper, we focus on source code quality assessment for Share‐ Point applications, which is a powerful framework for developing software by combining imperative and declarative programming. In particular, we present an industrial case study conducted in a software consulting/development company in Netherlands, which aimed at: identifying the most common {SharePoint} quality rule violations and their severity. The results indicate that the most frequent rule violations are identified in the {JavaScript} part of the applications, and that the most severe ones are related to correctness, security and deployment. The aforementioned results can be exploited by both researchers and practitioners, in terms of future research directions, and to inform the quality assurance process. © Springer International Publishing {AG} 2016.},
	pages = {495--505},
	journaltitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Ampatzoglou, A. and Avgeriou, P. and Koenders, T. and Van Alphen, P. and Stamelos, I.},
	date = {2016},
	note = {{ISBN}: 9783319490939
Publisher: Springer Verlag},
	keywords = {Quality assurance, Defect prediction, Computer programming, Application programs, Declarative Programming, Future research directions, Industrial case study, Process engineering, Quality assessment, Quality assurance process, Sharepoint, Source code qualities, xno}
}

@article{abdi_hybrid_2015,
	title = {A hybrid one-class rule learning approach based on swarm intelligence for software fault prediction},
	volume = {11},
	issn = {16145046},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945477440&doi=10.1007%2fs11334-015-0258-2&partnerID=40&md5=55244e135378604d70e7ef15669e8675},
	doi = {10.1007/s11334-015-0258-2},
	abstract = {Software testing is a fundamental activity in the software development process aimed to determine the quality of software. To reduce the effort and cost of this process, defect prediction methods can be used to determine fault-prone software modules through software metrics to focus testing activities on them. Because of model interpretation and easily used by programmers and testers some recent studies presented classification rules to make prediction models. This study presents a rule-based prediction approach based on kernel k-means clustering algorithm and Distance based Multi-objective Particle Swarm Optimization ({DSMOPSO}). Because of discrete search space, we modified this algorithm and named it {DSMOPSO}-D. We prevent best global rules to dominate local rules by dividing the search space with kernel k-means algorithm and by taking different approaches for imbalanced and balanced clusters, we solved imbalanced data set problem. The presented model performance was evaluated by four publicly available data sets from the {PROMISE} repository and compared with other machine learning and rule learning algorithms. The obtained results demonstrate that our model presents very good performance, especially in large data sets. © 2015, Springer-Verlag London.},
	pages = {289--301},
	number = {4},
	journaltitle = {Innovations in Systems and Software Engineering},
	author = {Abdi, Y. and Parsa, S. and Seyfari, Y.},
	date = {2015},
	note = {Publisher: Springer-Verlag London Ltd},
	keywords = {Clustering algorithms, Software design, Forecasting, Learning systems, Classification (of information), Software engineering, Software testing, Fault prediction, Particle swarm optimization ({PSO}), Artificial intelligence, Learning algorithms, Classification rules, Algorithms, Imbalanced Data-sets, Kernel k-means, Multi objective particle swarm optimization, Multiobjective optimization, xno}
}

@article{czibula_detecting_2015,
	title = {Detecting software design defects using relational association rule mining},
	volume = {42},
	issn = {02191377},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891894965&doi=10.1007%2fs10115-013-0721-z&partnerID=40&md5=b53495712645fce717363a3a8717fdb7},
	doi = {10.1007/s10115-013-0721-z},
	abstract = {In this paper, we are approaching, from a machine learning perspective, the problem of automatically detecting defective software entities (classes and methods) in existing software systems, a problem of major importance during software maintenance and evolution. In order to improve the internal quality of a software system, identifying faulty entities such as classes, modules, methods is essential for software developers. As defective software entities are hard to identify, machine learning-based classification models are still developed to approach the problem of detecting software design defects. We are proposing a novel method based on relational association rule mining for detecting faulty entities in existing software systems. Relational association rules are a particular type of association rules and describe numerical orderings between attributes that commonly occur over a dataset. Our method is based on the discovery of relational association rules for identifying design defects in software. Experiments on open source software are conducted in order to detect defective classes in object-oriented software systems, and a comparison of our approach with similar existing approaches is provided. The obtained results show that our method is effective for software design defect detection and confirms the potential of our proposal. © 2014, Springer-Verlag London.},
	pages = {545--577},
	number = {3},
	journaltitle = {Knowledge and Information Systems},
	author = {Czibula, G. and Marian, Z. and Czibula, I.G.},
	date = {2015},
	note = {Publisher: Springer London},
	keywords = {Data mining, Defects, Software design, Open source software, Turing machines, Learning systems, Object oriented programming, Machine learning, Open systems, Association rules, Software developer, Object detection, Software systems, Software entities, Classification models, Defect detection, Internal quality, Object-oriented software systems, Software maintenance and evolution, xyes}
}

@inproceedings{lacaille_influence_2015,
	title = {An influence gauge to detect and explain relations between measurements and a performance indicator},
	isbn = {978-1-936263-20-2},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016096495&partnerID=40&md5=5b925db9c89df65246cdaf06dffe01b4},
	abstract = {What about a software tool that behaves like a gauge able to estimate the quantity of information contained in a group of measurements? Then if we have a performance indicator or a defect rate, how may we compute the maximum performance explanation contained in our dataset? The first question may be answered by entropy and the second with mutual information. The present paper recalls a simple way to use those mathematical tools in an application one wants to launch each time a new dataset has to be studied. Often the {PHM} team in Snecma is asked to participate in special workforces to analyze sudden crisis. This methodology helps at the very beginning of the process to identify our mathematical capability to build an explanation model. This was the case during a small engine start crisis when some spark plugs were not working. Another time we used this tool to identify the flying condition when a gearbox was heating. This methodology was first developed for industry purposes like the optimization of machine tools or process recipes. Its success is in the simplifications of the computations that enlighten the interpretability of the results. Each signal is quantified in a way that improves the mutual information with the performance indicator. This is done signal by signal, but also for any small subsets of multivariate measurements until the confidence given by the quantity and quality of the data reaches its maximum. The segmentation of the data helps and boosts the computation of the integrals. Moreover, as this methodology uses quantified data as inputs it works as well with any sort of inputs such as continuous, discrete ordered and even categorized measurements. Once a best subset of measurements is selected a simple non-linear model is built using a relaxation algorithm. This model is a set of hypercubes that classifies the input space in a very simple and interpretable way. The methodology given below is a rough approach and may be replaced by more efficient regression algorithms if one only have continuous measurements but it has some advantages like a way to search a "best rule" according to some constraints and a graphic navigation tool very efficient to correct recipes. © 2015, Prognostics and Health Management Society. All rights reserved.},
	pages = {298--306},
	booktitle = {Proceedings of the Annual Conference of the Prognostics and Health Management Society, {PHM}},
	publisher = {Prognostics and Health Management Society},
	author = {Lacaille, J.},
	date = {2015},
	note = {{ISSN}: 23250178},
	keywords = {Systems engineering, Air navigation, Benchmarking, Optimization, Continuous measurements, Gages, Machine tools, Mathematical tools, Mutual informations, Navigation tools, Non-linear model, Performance indicators, Regression algorithms, Relaxation algorithm, Spark plugs, xno}
}

@article{jin_novel_2015,
	title = {A novel rule base representation and its inference method using the evidential reasoning approach},
	volume = {87},
	issn = {09507051},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992236535&doi=10.1016%2fj.knosys.2015.06.018&partnerID=40&md5=f718eef9dceaa05f257e054f66c9037f},
	doi = {10.1016/j.knosys.2015.06.018},
	abstract = {In this paper, a novel rule base, Certainty Rule Base ({CeRB}), and its inference method are proposed. This rule base is firstly designed with certainty degrees embedded in the antecedent terms as well as in the consequent terms. {CeRB} is shown to be capable of capturing vagueness, incompleteness, uncertainty, and nonlinear causal relationships in an integrated way. Secondly, the {CeRB} inference method using the evidential reasoning approach is provided. The overall representation and inference framework offer a further improvement and a great extension of the recently uncertainty inference methods. Namely, the knowledge is represented by {CeRB} and the evidential reasoning approach is applied to the rule combination. In the end, two case studies including a numerical example and a software defect prediction are provided to illustrate the proposed {CeRB} representation, generation and inference procedure as well as demonstrate its high performance by comparing with some existing approaches. © 2015 Elsevier B.V.},
	pages = {80--91},
	journaltitle = {Knowledge-Based Systems},
	author = {Jin, L. and Liu, J. and Xu, Y. and Fang, X.},
	date = {2015},
	note = {Publisher: Elsevier},
	keywords = {Software engineering, Artificial intelligence, Knowledge based systems, Evidential reasoning approaches, Inference methods, Representation of knowledge, Rule base, Similarity measure, xyes},
	file = {Jin et al. - 2015 - A novel rule base representation and its inference.pdf:C\:\\Users\\michalm\\Zotero\\storage\\HYC49AJY\\Jin et al. - 2015 - A novel rule base representation and its inference.pdf:application/pdf}
}

@article{han_residual_2015,
	title = {Residual defect prediction using multiple technologies},
	volume = {10},
	issn = {19750080},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941051136&doi=10.14257%2fijmue.2015.10.8.01&partnerID=40&md5=65afabcbb0453293432c2e7f2e6543b3},
	doi = {10.14257/ijmue.2015.10.8.01},
	abstract = {Finding defects in a software system is not easy. Effective detection of software defects is an important activity of software development process. In this paper, we propose an approach to predict residual defects, which applies machine learning algorithms (classifiers) and defect distribution model. This approach includes two steps. Firstly, use machine learning Algorithms and Association Rules to get defect classification table, then confirm the defect distribution trend referring to several distribution models. Experiment results on a {GUI} project show that the approach can effectively improve the accuracy of defect prediction and be used for test planning and implementation. © 2015 {SERSC}.},
	pages = {1--12},
	number = {8},
	journaltitle = {International Journal of Multimedia and Ubiquitous Engineering},
	author = {Han, W.-J. and Jiang, L.-X. and Lu, T.-B. and Zhang, X.-Y.},
	date = {2015},
	note = {Publisher: Science and Engineering Research Support Society},
	keywords = {Defects, Software design, Software defects, Forecasting, Learning systems, Software engineering, Artificial intelligence, Defect prediction, Learning algorithms, Classifiers, Defect distribution, Software development process, Defect classification, Distribution models, Multiple technology, Residual defects, xyes}
}

@article{han_software_2015,
	title = {Software defect model based on similarity and association rule},
	volume = {10},
	issn = {19750080},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938590184&doi=10.14257%2fijmue.2015.10.7.01&partnerID=40&md5=0ec914701879865dc39b740ae967168d},
	doi = {10.14257/ijmue.2015.10.7.01},
	abstract = {In order to detect defects efficiently and improve the quality of products, this paper puts forward the concept about defect classification model and defect association model by a lot of defect data. The technology of similarity is applied to defect classification model, and the idea of Knowledge Discovery in Database is applied to defect association model. Defect classification model can analyze the defect efficiently and provides the basis of solving problems quickly while defect association model can be used to detect early and prevent problem, which can make effective improvements for testing and development. This paper summed up {GUI} defect model based on a large number of interface defects. The model is useful to improve the accuracy of forecast and be used for test planning and implementation through the practice of several projects. © 2015 {SERSC}.},
	pages = {1--10},
	number = {7},
	journaltitle = {International Journal of Multimedia and Ubiquitous Engineering},
	author = {Han, W.J. and Jiang, H.Y. and Lu, T.B. and Zhang, X.Y. and Li, W.},
	date = {2015},
	note = {Publisher: Science and Engineering Research Support Society},
	keywords = {Defects, Software defects, Classification (of information), Defect model, Association rules, Defect classification, Defect associations, Interface defects, Knowledge discovery in database, Quality of product, Similarity, xyes}
}

@inproceedings{dhanalaxmi_adaptive_2015,
	title = {Adaptive {PSO} based association rule mining technique for software defect classification using {ANN}},
	volume = {46},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84931310946&doi=10.1016%2fj.procs.2015.02.041&partnerID=40&md5=e2417b86844d4cc674f50fa8a7065e0b},
	doi = {10.1016/j.procs.2015.02.041},
	abstract = {The proposed system categorizes various defects by using association rule mining dependent problem classification approach, which is applied to collect the actual defects using recognition. Association rule mining algorithm at times results in useless policies. To avoid this kind of concerns, the principles prior to classification determined by assistance as well as confidence value has to be optimized. In this exploration, Adaptive Particle Swarm ({APSO}) optimization algorithm is used. This can discover the best assistance and confidence value to have the best policies. And finally Artificial Neural Network ({ANN}) can be used to classify the actual defects determined. © 2015 Published by Elsevier B.V.},
	pages = {432--442},
	booktitle = {Procedia Computer Science},
	publisher = {Elsevier B.V.},
	author = {Dhanalaxmi, B. and Apparao Naidu, G. and Anuradha, K.},
	date = {2015},
	note = {{ISSN}: 18770509},
	keywords = {Data mining, Defects, Software defects, Rule mining algorithms, Association rules, Software testing, Particle swarm optimization ({PSO}), Optimization, Optimization algorithms, Neural networks, Algorithms, Adaptive particle swarm optimization algorithm, Confidence values, Defect prevention, Problem classification, Rule mining techniques, xno}
}

@article{tiwari_design_2015,
	title = {Design and implementation of rough set co-processor on {FPGA}},
	volume = {11},
	issn = {13494198},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924651054&partnerID=40&md5=11ae313b9cae6f239288f503551080d8},
	abstract = {Rough set theory is a mathematical approach to process and interpret in¬complete information system. Several researchers have dealt with the problem of finding reduct from the set of attributes, cores, and rules from databases using different soft¬ware deployed on multiprocessor system. Recently researchers have started using Field Programmable Gate Array ({FPGA}) implementation as an alternate option. Software approach is versatile but slow as compared to hardware implementation. The goal of this work is to design an exemplary rough set co-processor based on rough set theory and map it on {FPGA}. This paper gives an insight of a rough set co-processor’s modules. The theory of dealing with large databases is studied. With the usage of dual port {RAM} and pipelining in design, a considerable time is saved thus making it suitable for real time applications. The application for rough set co-processor is explained with the case study of a typical fault dictionary of a Very Large Scale Integrated ({VLSI}) chip. It can be used as a Built-in-Self-Test controller for testing {VLSI} chip. Simulation results show that proposed hardware is significantly faster than algorithms running on general-purpose processor. The rough set co-processor can also be used as hardware classifier unit in per¬sonal computer. © 2015 {ISSN} 1349-4198.},
	pages = {641--656},
	number = {2},
	journaltitle = {International Journal of Innovative Computing, Information and Control},
	author = {Tiwari, K.S. and Kothari, A.G.},
	date = {2015},
	note = {Publisher: {IJICIC} Editorial Office},
	keywords = {Classification (of information), Field programmable gate arrays ({FPGA}), Set theory, Computer hardware, Hardware, Rough set theory, Algorithms, Built-in self test, Co-processors, Computer hardware description languages, Design, Discernibility matrix, General purpose computers, Hardware accelerators, {HDL}, Integrated circuit testing, Reduct, Rules, Testability, xno}
}

@article{he_empirical_2015,
	title = {An empirical study on software defect prediction with a simplified metric set},
	volume = {59},
	issn = {09505849},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921027598&doi=10.1016%2fj.infsof.2014.11.006&partnerID=40&md5=8d9c3dd0f911ea02f763bf4d5b6480e0},
	doi = {10.1016/j.infsof.2014.11.006},
	abstract = {Context Software defect prediction plays a crucial role in estimating the most defect-prone components of software, and a large number of studies have pursued improving prediction accuracy within a project or across projects. However, the rules for making an appropriate decision between within- and cross-project defect prediction when available historical data are insufficient remain unclear. Objective The objective of this work is to validate the feasibility of the predictor built with a simplified metric set for software defect prediction in different scenarios, and to investigate practical guidelines for the choice of training data, classifier and metric subset of a given project. Method First, based on six typical classifiers, three types of predictors using the size of software metric set were constructed in three scenarios. Then, we validated the acceptable performance of the predictor based on Top-k metrics in terms of statistical methods. Finally, we attempted to minimize the Top-k metric subset by removing redundant metrics, and we tested the stability of such a minimum metric subset with one-way {ANOVA} tests. Results The study has been conducted on 34 releases of 10 open-source projects available at the {PROMISE} repository. The findings indicate that the predictors built with either Top-k metrics or the minimum metric subset can provide an acceptable result compared with benchmark predictors. The guideline for choosing a suitable simplified metric set in different scenarios is presented in Table 12. Conclusion The experimental results indicate that (1) the choice of training data for defect prediction should depend on the specific requirement of accuracy; (2) the predictor built with a simplified metric set works well and is very useful in case limited resources are supplied; (3) simple classifiers (e.g., Naïve Bayes) also tend to perform well when using a simplified metric set for defect prediction; and (4) in several cases, the minimum metric subset can be identified to facilitate the procedure of general defect prediction with acceptable loss of prediction precision in practice. © 2014 Elsevier B.V. All rights reserved.},
	pages = {170--190},
	journaltitle = {Information and Software Technology},
	author = {He, P. and Li, B. and Liu, X. and Chen, J. and Ma, Y.},
	date = {2015},
	note = {Publisher: Elsevier},
	keywords = {Software metrics, Computer software selection and evaluation, Defects, Software defect prediction, Open source software, Forecasting, Classification (of information), Open source projects, Software Quality, Defect prediction, Set theory, Acceptable performance, Metric set simplification, Prediction precision, xno}
}

@article{moeyersoms_comprehensible_2015,
	title = {Comprehensible software fault and effort prediction: A data mining approach},
	volume = {100},
	issn = {01641212},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919361139&doi=10.1016%2fj.jss.2014.10.032&partnerID=40&md5=2eab3043a0a68f06cf9b0418d024f57f},
	doi = {10.1016/j.jss.2014.10.032},
	abstract = {Software fault and effort prediction are important tasks to minimize costs of a software project. In software effort prediction the aim is to forecast the effort needed to complete a software project, whereas software fault prediction tries to identify fault-prone modules. In this research both tasks are considered, thereby using different data mining techniques. The predictive models not only need to be accurate but also comprehensible, demanding that the user can understand the motivation behind the model's prediction. Unfortunately, to obtain predictive performance, comprehensibility is often sacrificed and vice versa. To overcome this problem, we extract trees from well performing Random Forests ({RFs}) and Support Vector Machines for regression ({SVRs}) making use of a rule extraction algorithm {ALPA}. This method builds trees (using C4.5 and {REPTree}) that mimic the black-box model ({RF}, {SVR}) as closely as possible. The proposed methodology is applied to publicly available datasets, complemented with new datasets that we have put together based on the Android repository. Surprisingly, the trees extracted from the black-box models by {ALPA} are not only comprehensible and explain how the black-box model makes (most of) its predictions, but are also more accurate than the trees obtained by working directly on the data. © 2014 Elsevier Inc. All rights reserved.},
	pages = {80--90},
	journaltitle = {Journal of Systems and Software},
	author = {Moeyersoms, J. and Junqué De Fortuny, E. and Dejaeger, K. and Baesens, B. and Martens, D.},
	date = {2015},
	note = {Publisher: Elsevier Inc.},
	keywords = {Data mining, Forecasting, Decision trees, Forestry, Effort prediction, Extraction, Software fault prediction, Computer software, Rule extraction, Comprehensibility, Computer Programs, Data Processing, Fault-prone modules, Predictive performance, Rule extraction algorithms, Software effort prediction, Trees, Software fault and effort prediction, xyes},
	file = {Accepted Version:C\:\\Users\\michalm\\Zotero\\storage\\VYDKZISE\\Moeyersoms et al. - 2015 - Comprehensible software fault and effort predictio.pdf:application/pdf}
}

@inproceedings{gopinath_code_2014,
	title = {Code coverage for suite evaluation by developers},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994173220&doi=10.1145%2f2568225.2568278&partnerID=40&md5=b571e6bc90035457b76e063f99096006},
	doi = {10.1145/2568225.2568278},
	abstract = {One of the key challenges of developers testing code is determining a test suite's quality - its ability to find faults. The most common approach is to use code coverage as a measure for test suite quality, and diminishing returns in coverage or high absolute coverage as a stopping rule. In testing research, suite quality is often evaluated by a suite's ability to kill mutants (artificially seeded potential faults). Determining which criteria best predict mutation kills is critical to practical estimation of test suite quality. Previous work has only used small sets of programs, and usually compares multiple suites for a single program. Practitioners, however, seldom compare suites - - they evaluate one suite. Using suites (both manual and automatically generated) from a large set of real-world open-source projects shows that evaluation results differ from those for suite-comparison: statement (not block, branch, or path) coverage predicts mutation kills best. © 2014 {ACM}.},
	pages = {72--82},
	booktitle = {Proceedings - International Conference on Software Engineering},
	publisher = {{IEEE} Computer Society},
	author = {Gopinath, R. and Jensen, C. and Groce, A.},
	date = {2014},
	note = {{ISSN}: 02705257
Issue: {CONFCODENUMBER}},
	keywords = {Automatically generated, Open source software, Evaluation results, Open source projects, Software engineering, Software testing, Codes (symbols), Quality control, Ability testing, Code coverage, Coverage criteria, Potential faults, Practical estimation, Statistical methods, Test framework, xno},
	file = {Submitted Version:C\:\\Users\\michalm\\Zotero\\storage\\GCNHZ78A\\Gopinath et al. - 2014 - Code coverage for suite evaluation by developers.pdf:application/pdf}
}

@article{rahman_using_2014,
	title = {Using bayesian networks to model and analyze software product line feature model},
	volume = {8875},
	issn = {03029743},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84911923947&doi=10.1007%2f978-3-319-13365-2_20&partnerID=40&md5=f247724b792424998d68b6b432b4d439},
	doi = {10.1007/978-3-319-13365-2_20},
	abstract = {Proper management of requirements plays a significant role in the successful development of any software product family. Application of {AI}, Bayesian Network ({BN}) in particular, is gaining much interest in Software Engineering, mainly in predicting software defects and software reliability. Feature analysis and its associated decision making is a suitable target area where {BN} can make remarkable effect. In {SPL}, a feature tree portrays various types of features as well as captures the relationships among them. This paper applies {BN} in modeling and analyzing features in a feature tree. Various feature analysis rules are first modeled and then verified in {BN}. The verification confirms the definition of the rules and thus these rules can be used in various decision making tages in {SPL}. ©Springer International Publishing Switzerland 2014.},
	pages = {220--231},
	journaltitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Rahman, M. and Ripon, S.},
	date = {2014},
	note = {Publisher: Springer Verlag},
	keywords = {Software Product Line, Software defects, Forestry, Models, Software reliability, Application programs, Bayesian networks, Decision making, Computer Programs, Dead feature, False optional, Feature analysis, Feature tree, Management, Software product family, xno}
}

@article{ma_investigating_2014,
	title = {Investigating associative classification for software fault prediction: An experimental perspective},
	volume = {24},
	issn = {02181940},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902316977&doi=10.1142%2fS021819401450003X&partnerID=40&md5=24b92bd4a35a5c9e854ed011c73d58ec},
	doi = {10.1142/S021819401450003X},
	abstract = {It is a recurrent finding that software development is often troubled by considerable delays as well as budget overruns and several solutions have been proposed in answer to this observation, software fault prediction being a prime example. Drawing upon machine learning techniques, software fault prediction tries to identify upfront software modules that are most likely to contain faults, thereby streamlining testing efforts and improving overall software quality. When deploying fault prediction models in a production environment, both prediction performance and model comprehensibility are typically taken into consideration, although the latter is commonly overlooked in the academic literature. Many classification methods have been suggested to conduct fault prediction; yet associative classification methods remain uninvestigated in this context. This paper proposes an associative classification ({AC})-based fault prediction method, building upon the {CBA}2 algorithm. In an empirical comparison on 12 real-world datasets, the {AC}-based classifier is shown to achieve a predictive performance competitive to those of models induced by five other tree/rule-based classification techniques. In addition, our findings also highlight the comprehensibility of the {AC}-based models, while achieving similar prediction performance. Furthermore, the possibilities of cross project prediction are investigated, strengthening earlier findings on the feasibility of such approach when insufficient data on the target project is available. © 2014 World Scientific Publishing Company.},
	pages = {61--90},
	number = {1},
	journaltitle = {International Journal of Software Engineering and Knowledge Engineering},
	author = {Ma, B. and Zhang, H. and Chen, G. and Zhao, Y. and Baesens, B.},
	date = {2014},
	note = {Publisher: World Scientific Publishing Co. Pte Ltd},
	keywords = {Computer software selection and evaluation, Forecasting, Learning systems, Classification (of information), Associative classification, Software testing, Budget control, Artificial intelligence, Software fault prediction, Prediction performance, comprehensibility, cross project validation, xno}
}

@inproceedings{maurio_method_2014,
	title = {Method for generating a diverse set of requirements for safety-critical systems},
	volume = {28},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897978749&doi=10.1016%2fj.procs.2014.03.057&partnerID=40&md5=0412352a23e6f02f5a6fda6a6c26b0c0},
	doi = {10.1016/j.procs.2014.03.057},
	abstract = {Automatic digital safety-critical systems are often architected with redundant hardware in order to combat the effects of a single failure that could prevent the system from performing its safety function Additionally, diverse hardware and software are typically employed to guard against any potential common-cause failures that would likewise cause an inability of the system to carry out its safety function An all digital (processor or programmable logic-based) implementation usually requires the development of two digital systems by two separate software (and frequently hardware) teams which operate in parallel to provide the safety function Strict rules are applied to the development process to ensure that the separate teams do not share information or influence each other's designs Even though this technique provides a means to develop a diverse set of digital safety-critical equipment, the system design still begins with a single set of requirements Therefore, it is conceivable that the two design teams may create solutions that contain identical design elements Any flaws or vulnerabilities in the common elements would then be shared between the two designs making the system vulnerable to common-cause failures thus defeating the benefit of utilizing diverse design teams A method is proposed herein to address this limitation This method entails the classification of the individual requirements of the source specification according to a detailed hierarchical taxonomy and the subsequent altering of the classified requirements The taxonomy is structured so that the leaf-level classifiers are mutually exclusive or uncorrelated and the classified requirements are altered to be more stringent The original and constrained requirements are allocated to two specifications documents in such a way that for certain requirements, the original version appears in the specification for one design team and the constrained version appears in the specification for the other By using this process, sufficient requirements diversity results increasing the likelihood the two separate development teams will achieve a greater degree of design and implementation diversity than two teams using the same set of requirements This increased product diversity should ultimately result in fewer latent common-cause faults residing in the two diverse systems Furthermore, the degree of diversity achieved is expected to be greater when requirements diversity is employed, as compared to a traditional approach in which diversity is achieved by chance © 2014 The Authors. Published by Elsevier B. V.},
	pages = {465--472},
	booktitle = {Procedia Computer Science},
	publisher = {Elsevier B.V.},
	author = {Maurio, J. and {McClure}, C.},
	date = {2014},
	note = {{ISSN}: 18770509},
	keywords = {Hardware and software, Hierarchical systems, Hardware, Systems analysis, Product design, Safety engineering, Design and implementations, Carry logic, Diversity, Hierarchical taxonomy, Information retrieval systems, Requirements, Safety critical systems, Security systems, Separation, Specifications, Specifications document, Taxonomies, Traditional approaches, xno}
}

@article{rodriguez_study_2013,
	title = {A study of subgroup discovery approaches for defect prediction},
	volume = {55},
	issn = {09505849},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880777880&doi=10.1016%2fj.infsof.2013.05.002&partnerID=40&md5=4bd143673df3bbb32127deb28c13f397},
	doi = {10.1016/j.infsof.2013.05.002},
	abstract = {Context Although many papers have been published on software defect prediction techniques, machine learning approaches have yet to be fully explored. Objective In this paper we suggest using a descriptive approach for defect prediction rather than the precise classification techniques that are usually adopted. This allows us to characterise defective modules with simple rules that can easily be applied by practitioners and deliver a practical (or engineering) approach rather than a highly accurate result. Method We describe two well-known subgroup discovery algorithms, the {SD} algorithm and the {CN}2-{SD} algorithm to obtain rules that identify defect prone modules. The empirical work is performed with publicly available datasets from the Promise repository and object-oriented metrics from an Eclipse repository related to defect prediction. Subgroup discovery algorithms mitigate against characteristics of datasets that hinder the applicability of classification algorithms and so remove the need for preprocessing techniques. Results The results show that the generated rules can be used to guide testing effort in order to improve the quality of software development projects. Such rules can indicate metrics, their threshold values and relationships between metrics of defective modules. Conclusions The induced rules are simple to use and easy to understand as they provide a description rather than a complete classification of the whole dataset. Thus this paper represents an engineering approach to defect prediction, i.e., an approach which is useful in practice, easily understandable and can be applied by practitioners. © 2013 Elsevier B.V. All rights reserved.},
	pages = {1810--1822},
	number = {10},
	journaltitle = {Information and Software Technology},
	author = {Rodriguez, D. and Ruiz, R. and Riquelme, J.C. and Harrison, R.},
	date = {2013},
	keywords = {Defects, Software defect prediction, Forecasting, Classification (of information), Defect prediction, Machine learning approaches, Algorithms, Imbalanced Data-sets, Classification technique, Rules, Preprocessing techniques, Subgroup discovery, xyes},
	file = {Rodriguez et al. - 2013 - A study of subgroup discovery approaches for defec.pdf:C\:\\Users\\michalm\\Zotero\\storage\\H3YLINFQ\\Rodriguez et al. - 2013 - A study of subgroup discovery approaches for defec.pdf:application/pdf}
}

@article{rafael_lenz_linking_2013,
	title = {Linking software testing results with a machine learning approach},
	volume = {26},
	issn = {09521976},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876919615&doi=10.1016%2fj.engappai.2013.01.008&partnerID=40&md5=d736723192e999d5f815a975d64fb628},
	doi = {10.1016/j.engappai.2013.01.008},
	abstract = {Software testing techniques and criteria are considered complementary since they can reveal different kinds of faults and test distinct aspects of the program. The functional criteria, such as Category Partition, are difficult to be automated and are usually manually applied. Structural and fault-based criteria generally provide measures to evaluate test sets. The existing supporting tools produce a lot of information including: input and produced output, structural coverage, mutation score, faults revealed, etc. However, such information is not linked to functional aspects of the software. In this work, we present an approach based on machine learning techniques to link test results from the application of different testing techniques. The approach groups test data into similar functional clusters. After this, according to the tester's goals, it generates classifiers (rules) that have different uses, including selection and prioritization of test cases. The paper also presents results from experimental evaluations and illustrates such uses. © 2013 Elsevier Ltd.},
	pages = {1631--1640},
	number = {5},
	journaltitle = {Engineering Applications of Artificial Intelligence},
	author = {Rafael Lenz, A. and Pozo, A. and Regina Vergilio, S.},
	date = {2013},
	keywords = {Testing, Learning systems, Software testing, Machine learning approaches, Category partition, Experimental evaluation, Functional aspects, Software testing techniques, Supporting tool, Test coverage criteria, Testing technique, xno}
}

@article{chang_integrating_2013,
	title = {Integrating action-based defect prediction to provide recommendations for defect action correction},
	volume = {23},
	issn = {02181940},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879385903&doi=10.1142%2fS0218194013500022&partnerID=40&md5=e888fc5bc9722ab6fbb1a4fb3591f3c1},
	doi = {10.1142/S0218194013500022},
	abstract = {Reducing software defects is an essential activity for Software Process Improvement. The Action-Based Defect Prediction ({ABDP}) approach fragments the software process into actions, and builds software defect prediction models using data collected from the execution of actions and reported defects. Though the {ABDP} approach can be applied to predict possible defects in subsequent actions, the efficiency of corrections is dependent on the skill and knowledge of the stakeholders. To address this problem, this study proposes the Action Correction Recommendation ({ACR}) model to provide recommendations for action correction, using the Negative Association Rule mining technique. In addition to applying the association rule mining technique to build a High Defect Prediction Model ({HDPM}) to identify high defect action, the {ACR} builds a Low Defect Prediction Model ({LDPM}). For a submitted action, each {HDPM} rule used to predict the action as a high defect action and the {LDPM} rules are analyzed using negative association rule mining to spot the rule items with different characteristics in {HDPM} and {LDPM} rules. This information not only identifies the attributes required for corrections, but also provides a range (or a value) to facilitate the high defect action corrections. This study applies the {ACR} approach to a business software project to validate the efficiency of the proposed approach. The results show that the recommendations obtained can be applied to decrease software defect removal efforts. © 2013 World Scientific Publishing Company.},
	pages = {147--172},
	number = {2},
	journaltitle = {International Journal of Software Engineering and Knowledge Engineering},
	author = {Chang, C.-P.},
	date = {2013},
	keywords = {Defects, Software defect prediction, Defect prediction models, Forecasting, Defect prediction, Mathematical models, action correction recommendation, clustering, Negative association rules, Software process, Software Process Improvement, xno}
}

@article{glukhikh_using_2012,
	title = {Using dependencies to improve precision of code analysis},
	volume = {46},
	issn = {01464116},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894610046&doi=10.3103%2fS0146411612070097&partnerID=40&md5=daf98a2673f5ad907940e7b8860e5b22},
	doi = {10.3103/S0146411612070097},
	abstract = {Development of dependency analysis methods in order to improve static code analysis precision is considered in this paper. Reasons for precision loss when detecting defects in program source code using abstract interpretation methods are explained. Need for program object dependency extraction and interpretation is justified by numerous real-world examples. Dependency classification is presented. Necessity for aggregate analysis of values and dependencies is considered. Dependency extraction from assignment statements is described. Dependency interpretation based on logic inference using logic and arithmetic rules is proposed. The methods proposed are implemented in defect detection tool Digitek Aegis, significant increase of precision is shown. © Allerton Press, Inc., 2012.},
	pages = {338--344},
	number = {7},
	journaltitle = {Automatic Control and Computer Sciences},
	author = {Glukhikh, M.I. and Itsykson, V.M. and Tsesko, V.A.},
	date = {2012},
	keywords = {xno}
}

@article{liparas_applying_2012,
	title = {Applying the Mahalanobis-Taguchi strategy for software defect diagnosis},
	volume = {19},
	issn = {09288910},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856626430&doi=10.1007%2fs10515-011-0091-2&partnerID=40&md5=e280f048a589f5dcabe472e790c6dcf2},
	doi = {10.1007/s10515-011-0091-2},
	abstract = {The Mahalanobis-Taguchi ({MT}) strategy combines mathematical and statistical concepts like Mahalanobis distance, Gram-Schmidt orthogonalization and experimental designs to support diagnosis and decision-making based on multivariate data. The primary purpose is to develop a scale to measure the degree of abnormality of cases, compared to "normal" or "healthy" cases, i.e. a continuous scale from a set of binary classified cases. An optimal subset of variables for measuring abnormality is then selected and rules for future diagnosis are defined based on them and the measurement scale. This maps well to problems in software defect prediction based on a multivariate set of software metrics and attributes. In this paper, the {MT} strategy combined with a cluster analysis technique for determining the most appropriate training set, is described and applied to well-known datasets in order to evaluate the fault-proneness of software modules. The measurement scale resulting from the {MT} strategy is evaluated using {ROC} curves and shows that it is a promising technique for software defect diagnosis. It compares favorably to previously evaluated methods on a number of publically available data sets. The special characteristic of the {MT} strategy that it quantifies the level of abnormality can also stimulate and inform discussions with engineers and managers in different defect prediction situations. © 2011 Springer Science+Business Media, {LLC}.},
	pages = {141--165},
	number = {2},
	journaltitle = {Automated Software Engineering},
	author = {Liparas, D. and Angelis, L. and Feldt, R.},
	date = {2012},
	keywords = {Software metrics, Defects, Software defect prediction, Software defects, Forecasting, Software testing, Software modules, Defect prediction, Cluster analysis, Mahalanobis distances, Cluster analysis technique, Continuous scale, Data processing, Data sets, Fault-proneness, Gram-Schmidt orthogonalizations, Mahalanobis-Taguchi strategy, Measurement scale, Multivariate data, Optimal subsets, Rating, {ROC} curves, Statistical concepts, Training sets, xno}
}

@inproceedings{munir_automated_2012,
	title = {Automated heuristic defect classification ({AHDC}) for haze induced defect growth management and mask requalification},
	volume = {8324},
	isbn = {978-0-8194-8980-7},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861034561&doi=10.1117%2f12.924329&partnerID=40&md5=1c03aa196099e34a6728f956d3d9bebe},
	doi = {10.1117/12.924329},
	abstract = {This article presents results from a heuristic automated defect classification algorithm for reticle inspection that mimics the classification rules. {AHDC} does not require {CAD} data, thus it can be rapidly deployed in a high volume production environment without the need for extensive design data management. To ensure classification consistency a software framework tracks every defect in repeated inspections. Through its various image based derived metrics it is shown that such a system manages and tracks repeated defects in applications such as haze induced defect growth. © 2012 {SPIE}.},
	booktitle = {Proceedings of {SPIE} - The International Society for Optical Engineering},
	author = {Munir, S. and Qidwai, G.},
	date = {2012},
	note = {{ISSN}: 0277786X},
	keywords = {Defects, Classification (of information), Computer programming, Defect management, Information management, Inspection, {ADC}, Computer aided design, Defect tracking, Haze, Mask inspection, Mask repair, Photomasks, Process control, Units of measurement, xno}
}

@article{najadat_enhance_2012,
	title = {Enhance rule based detection for software fault prone modules},
	volume = {6},
	issn = {17389984},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859858705&partnerID=40&md5=0c2be465f65ed4ded20fcb863ceac2ec},
	abstract = {Software quality assurance is necessary to increase the level of confidence in the developed software and reduce the overall cost for developing software projects. The problem addressed in this research is the prediction of fault prone modules using data mining techniques. Predicting fault prone modules allows the software managers to allocate more testing and resources to such modules. This can also imply a good investment in better design in future systems to avoid building error prone modules. Software quality models that are based upon data mining from previous projects can identify fault-prone modules in the current similar development project, once similarity between projects is established. In this paper, we applied different data mining rule-based classification techniques on several publicly available datasets of the {NASA} software repository (e.g. {PC}1, {PC}2, etc). The goal was to classify the software modules into either fault prone or not fault prone modules. The paper proposed a modification on the {RIDOR} algorithm on which the results show that the enhanced {RIDOR} algorithm is better than other classification techniques in terms of the number of extracted rules and accuracy. The implemented algorithm learns defect prediction using mining static code attributes. Those attributes are then used to present a new defect predictor with high accuracy and low error rate.},
	pages = {75--86},
	number = {1},
	journaltitle = {International Journal of Software Engineering and its Applications},
	author = {Najadat, H. and Alsmadi, I.},
	date = {2012},
	keywords = {xyes}
}

@article{mahouachi_new_2012,
	title = {A new design defects classification: Marrying detection and correction},
	volume = {7212 {LNCS}},
	issn = {03029743},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859139094&doi=10.1007%2f978-3-642-28872-2_31&partnerID=40&md5=b53b0ec3051e981a6695a2be0372308b},
	doi = {10.1007/978-3-642-28872-2_31},
	abstract = {Previous work classify design defects based on symptoms (long methods, large classes, long parameter lists, etc.), and treat separately detection and correction steps. This paper introduces a new classification of defects using correction possibilities. Thus, correcting different code fragments appending to specific defect category need, approximately, the same refactoring operations to apply. To this end, we use genetic programming to generate new form of classification rules combining detection and correction steps. We report the results of our validation using different open-source systems. Our proposal achieved high precision and recall correction scores. © 2012 Springer-Verlag Berlin Heidelberg.},
	pages = {455--470},
	journaltitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Mahouachi, R. and Kessentini, M. and Ghedira, K.},
	date = {2012},
	note = {{ISBN}: 9783642288715},
	keywords = {Defects, Open source software, Genetic algorithms, Open systems, Software engineering, Computer software maintenance, Computer programming, Search-based software engineering, Classification rules, Genetic programming, Design, Classification of defects, Code fragments, Design defects, High precision, High-precision, New design, New forms, Open source system, Refactorings, Specific defects, xno}
}

@inproceedings{niculita_use_2012,
	title = {Use of {COTS} functional analysis software as an {IVHM} design tool for detection and isolation of {UAV} fuel system faults},
	isbn = {978-1-936263-05-9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920517871&partnerID=40&md5=4b1b1beda665787a056f8bd1462237e2},
	abstract = {This paper presents a new approach to the development of health management solutions which can be applied to both new and legacy platforms during the conceptual design phase. The approach involves the qualitative functional modelling of a system in order to perform an Integrated Vehicle Health Management ({IVHM}) design-the placement of sensors and the diagnostic rules to be used in interrogating their output. The qualitative functional analysis was chosen as a route for early assessment of failures in complex systems. Functional models of system components are required for capturing the available system knowledge used during various stages of system and {IVHM} design. {MADe}™ (Maintenance Aware Design environment), a {COTS} software tool developed by {PHM} Technology, was used for the health management design. A model has been built incorporating the failure diagrams of five failure modes for five different components of a {UAV} fuel system. Thus an inherent health management solution for the system and the optimised sensor set solution have been defined. The automatically generated sensor set solution also contains a diagnostic rule set, which was validated on the fuel rig for different operation modes taking into account the predicted fault detection/isolation and ambiguity group coefficients. It was concluded that when using functional modelling, the {IVHM} design and the actual system design cannot be done in isolation. The functional approach requires permanent input from the system designer and reliability engineers in order to construct a functional model that will qualitatively represent the real system. In other words, the physical insight should not be isolated from the failure phenomena and the diagnostic analysis tools should be able to adequately capture the experience bases. This approach has been verified on a laboratory bench top test rig which can simulate a range of possible fuel system faults. The rig is fully instrumented in order to allow benchmarking of various sensing solution for fault detection/isolation that were identified using functional analysis.},
	pages = {28--45},
	booktitle = {Proceedings of the Annual Conference of the Prognostics and Health Management Society 2012, {PHM} 2012},
	publisher = {Prognostics and Health Management Society},
	author = {Niculita, O. and Irving, P. and Jennions, I.K.},
	date = {2012},
	keywords = {Automatically generated, Fault detection, Health, Diagnostic analysis, Systems analysis, Design, Conceptual design, Conceptual design phase, Fuel systems, Fuels, Functional analysis, Functional approach, Functional modelling, Integrated vehicle health managements, Placement of sensors, Reliability engineers, xno}
}

@article{chaturvedi_empirical_2012,
	title = {An empirical comparison of machine learning techniques in predicting the bug severity of open and closed source projects},
	volume = {4},
	issn = {19423926},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887444112&doi=10.4018%2fjossp.2012040103&partnerID=40&md5=56422d7f930f344977a1a4cf842570d6},
	doi = {10.4018/jossp.2012040103},
	abstract = {Bug severity is the degree of impact that a defect has on the development or operation of a component or system, and can be classified into different levels based on their impact on the system. Identification of severity level can be useful for bug triager in allocating the bug to the concerned bug fixer. Various researchers have attempted text mining techniques in predicting the severity of bugs, detection of duplicate bug reports and assignment of bugs to suitable fixer for its fix. In this paper, an attempt has been made to compare the performance of different machine learning techniques namely Support vector machine ({SVM}), probability based Naïve Bayes ({NB}), Decision Tree based J48 (A Java implementation of C4.5), rule based Repeated Incremental Pruning to Produce Error Reduction ({RIPPER}) and Random Forests ({RF}) learners in predicting the severity level (1 to 5) of a reported bug by analyzing the summary or short description of the bug reports. The bug report data has been taken from {NASA}'s {PITS} (Projects and Issue Tracking System) datasets as closed source and components of Eclipse, Mozilla \& {GNOME} datasets as open source projects. The analysis has been carried out in {RapidMiner} and {STATISTICA} data mining tools. The authors measured the performance of different machine learning techniques by considering (i) the value of accuracy and F-Measure for all severity level and (ii) number of best cases at different threshold level of accuracy and F-Measure. Copyright © 2012, {IGI} Global.},
	pages = {32--59},
	number = {2},
	journaltitle = {International Journal of Open Source Software and Processes},
	author = {Chaturvedi, K.K. and Singh, V.B.},
	date = {2012},
	note = {Publisher: {IGI} Global},
	keywords = {Data mining, {NASA}, Open source software, Forecasting, Learning systems, Supervised classification, Decision trees, Open systems, Support vector machines, Text mining, Artificial intelligence, Learning algorithms, Supervised learning, Text processing, 10-fold cross-validation, Bug Repositories, Bug Severity, Multi-class classification, xno}
}

@inproceedings{mishra_support_2011,
	title = {Support vector machine based fuzzy classification model for software fault prediction},
	isbn = {978-0-9727412-8-6},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872196229&partnerID=40&md5=89b748f95b05d7826d68addbf7331afa},
	abstract = {Defect proneness prediction of software modules always attracts the developers because it can reduce the testing cost as well as software development time. In the current context, with constantly increasing constraints like requirement ambiguity and complex development process, developing a fault free reliable software is a daunting task. To deliver reliable software, software engineers are required to execute exhaustive test cases which become tedious and costly for software enterprises. To ameliorate the testing process one can use a defect prediction model so that testers can focus their efforts on defect prone modules. Software defect prediction models use historical defect database to forecast error-prone modules. Defect prediction models require empirical validation to ensure their relevance to a software company. In this paper, a new Support Vector Machine based Fuzzy classification based prediction model has been proposed and evaluated on bug data base of an open source software project. In the proposed model a rule base is constructed using support vectors and the membership grade is calculated using Gaussian membership functions. Rule set optimization is done using Genetic algorithm. It is found that the proposed model gives very promising results on the criteria of probability of bug detection, probability of false alarm and accuracy.},
	pages = {693--708},
	booktitle = {Proceedings of the 5th Indian International Conference on Artificial Intelligence, {IICAI} 2011},
	author = {Mishra, B. and Shukla, K.K.},
	date = {2011},
	keywords = {Defects, Software defect prediction, Defect prediction models, Genetic algorithms, Open systems, Software testing, Development process, Fault prediction, Support vector machines, Artificial intelligence, Software fault prediction, Software modules, Fuzzy systems, Fuzzy rule base, Open source software projects, Genetic programming, Rule base, Fuzzy classification, Mathematical models, Bug detection, Empirical validation, Error prones, Exhaustive tests, Fault, Gaussian membership function, Industry, Membership functions, Membership grade, Prediction model, Probability of false alarm, {ROC}, Rule set, Software company, Software engineers, Software enterprise, Support vector, Testing process, xyes}
}

@inproceedings{burla_assistant_2011,
	title = {Assistant systems for efficient multiscale measurement and inspection},
	volume = {8082},
	isbn = {978-0-8194-8678-3},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858417912&doi=10.1117%2f12.889344&partnerID=40&md5=753cf7341434d996e8c234cc6c5a3df6},
	doi = {10.1117/12.889344},
	abstract = {Optical inspection systems constitute hardware components (e.g. measurement sensors, lighting systems, positioning systems etc.) and software components (system calibration techniques, image processing algorithms for defect detection and classification, data fusion, etc.). Given an inspection task choosing the most suitable components is not a trivial process and requires expert knowledge. For multiscale measurement systems, the optimization of the measurement system is an unsolved problem even for human experts. In this contribution we propose two assistant systems (hardware assistant and software assistant), which help in choosing the most suitable components depending on the task considering the properties of the object (e.g. material, surface roughness, etc.) and the defects (e.g. defect types, dimensions, etc.). The hardware assistant system uses general rules of thumb, sensor models/simulations and stored expert knowledge to specify the sensors along with their parameters and the hierarchy (if necessary) in a multiscale measurement system. The software assistant system then simulates many measurements with all possible defect types for the chosen sensors. Artificial neural networks ({ANN}) are used for pre-selection and genetic algorithms are used for finer selection of the defect detection algorithms along with their optimized parameters. In this contribution we will show the general architecture of the assistant system and results obtained for the detection of typical defects on technical surfaces in the micro-scale using a multiscale measurement system. © 2011 {SPIE}.},
	booktitle = {Proceedings of {SPIE} - The International Society for Optical Engineering},
	author = {Burla, A. and Haist, T. and Lyda, W. and Aissa, M.H. and Osten, W.},
	date = {2011},
	note = {{ISSN}: 0277786X},
	keywords = {Defects, Genetic algorithms, Software component, Optimization, Neural networks, Hardware, Sensors, Image processing, Defect detection, Parameter optimization, Inspection, Data fusion, Defect detection algorithm, Defect type, Expert knowledge, Hardware components, Human expert, Image processing algorithm, Inspection tasks, Lighting systems, Measurement sensor, Measurement system, Micro-scales, Multiscales, Optical data processing, Optical inspection, Optical inspection systems, Optical testing, Optical variables measurement, Optimized parameter, Positioning system, Pre-selection, Sensor data fusion, Sensor model, Surface roughness, System calibration, System use, Technical surfaces, Unsolved problems, xno}
}

@article{macdonell_impact_2011,
	title = {The impact of sampling and rule set size on generated fuzzy inference system predictive accuracy: Analysis of a software engineering data set},
	volume = {364 {AICT}},
	issn = {18684238},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-80055061905&doi=10.1007%2f978-3-642-23960-1_43&partnerID=40&md5=c8a7de97df99b17d991cb0f94bac94e9},
	doi = {10.1007/978-3-642-23960-1_43},
	abstract = {Software project management makes extensive use of predictive modeling to estimate product size, defect proneness and development effort. Although uncertainty is acknowledged in these tasks, fuzzy inference systems, designed to cope well with uncertainty, have received only limited attention in the software engineering domain. In this study we empirically investigate the impact of two choices on the predictive accuracy of generated fuzzy inference systems when applied to a software engineering data set: sampling of observations for training and testing; and the size of the rule set generated using fuzzy c-means clustering. Over ten samples we found no consistent pattern of predictive performance given certain rule set size. We did find, however, that a rule set compiled from multiple samples generally resulted in more accurate predictions than single sample rule sets. More generally, the results provide further evidence of the sensitivity of empirical analysis outcomes to specific model-building decisions. © 2011 {IFIP} International Federation for Information Processing.},
	pages = {360--369},
	issue = {{PART} 2},
	journaltitle = {{IFIP} Advances in Information and Communication Technology},
	author = {{MacDonell}, S.G.},
	date = {2011},
	note = {{ISBN}: 9783642239595},
	keywords = {Forecasting, Software engineering, Software testing, Project management, Training and testing, Fuzzy C means clustering, Artificial intelligence, Accurate prediction, Fuzzy inference, Fuzzy systems, Predictive accuracy, Fuzzy inference systems, Source codes, C (programming language), Information management, Predictive performance, Rule set, Certain rule, Empirical analysis, Innovation, Multiple samples, Predictive modeling, Product sizes, Sampling, Sensitivity analysis, Single sample, Software engineering data, Software engineering domain, Software project management, software size, Software size, Statistical tests, xyes}
}

@inproceedings{paikari_customization_2011,
	title = {Customization support for {CBR}-based defect prediction},
	isbn = {978-1-4503-0709-3},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-80054074641&doi=10.1145%2f2020390.2020406&partnerID=40&md5=ffb5fd618dcab0c0d3cdfd836148e4ec},
	doi = {10.1145/2020390.2020406},
	abstract = {Background: The prediction performance of a case-based reasoning ({CBR}) model is influenced by the combination of the following parameters: (i) similarity function, (ii) number of nearest neighbor cases, (iii) weighting technique used for attributes, and (iv) solution algorithm. Each combination of the above parameters is considered as an instantiation of the general {CBR}-based prediction method. The selection of an instantiation for a new data set with specific characteristics (such as size, defect density and language) is called customization of the general {CBR} method. Aims: For the purpose of defect prediction, we approach the question which combinations of parameters works best at which situation. Three more specific questions were studied: ({RQ}1) Does one size fit all? Is one instantiation always the best? ({RQ}2) If not, which individual and combined parameter settings occur most frequently in generating the best prediction results? ({RQ}3) Are there context-specific rules to support the customization? Method: In total, 120 different {CBR} instantiations were created and applied to 11 data sets from the {PROMISE} repository. Predictions were evaluated in terms of their mean magnitude of relative error ({MMRE}) and percentage Pred(a) of objects fulfilling a prediction quality level a. For the third research question, dependency network analysis was performed. Results: Most frequent parameter options for {CBR} instantiations were neural network based sensitivity analysis (as the weighting technique), un-weighted average (as the solution algorithm), and maximum number of nearest neighbors (as the number of nearest neighbors). Using dependency network analysis, a set of recommendations for customization was provided. Conclusion: An approach to support customization is provided. It was confirmed that application of context-specific rules across groups of similar data sets is risky and produces poor results. Copyright © 2011 {ACM}.},
	booktitle = {{ACM} International Conference Proceeding Series},
	author = {Paikari, E. and Sun, B. and Ruhe, G. and Livani, E.},
	date = {2011},
	keywords = {Defects, Forecasting, Software engineering, Models, Defect prediction, Prediction performance, Neural networks, Algorithms, Data sets, Sensitivity analysis, Case based reasoning, {CBr}, Combined parameter, Customization, Defect density, Electric network analysis, Instantiation, Nearest neighbors, Parameter estimation, Prediction methods, Prediction quality, Predictive control systems, Relative errors, Research questions, Similarity functions, Solution algorithms, Weighting techniques, xno}
}

@inproceedings{woodley_multisource_2011,
	title = {Multisource information fusion for logistics},
	volume = {8064},
	isbn = {978-0-8194-8638-7},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960107118&doi=10.1117%2f12.883498&partnerID=40&md5=c5a4cca5b2260cf032c26a5b3a190517},
	doi = {10.1117/12.883498},
	abstract = {Current Army logistical systems and databases contain massive amounts of data that need an effective method to extract actionable information. The databases do not contain root cause and case-based analysis needed to diagnose or predict breakdowns. A system is needed to find data from as many sources as possible, process it in an integrated fashion, and disseminate information products on the readiness of the fleet vehicles. 21st Century Systems, Inc. introduces the Agent- Enabled Logistics Enterprise Intelligence System ({AELEIS}) tool, designed to assist logistics analysts with assessing the availability and prognostics of assets in the logistics pipeline. {AELEIS} extracts data from multiple, heterogeneous data sets. This data is then aggregated and mined for data trends. Finally, data reasoning tools and prognostics tools evaluate the data for relevance and potential issues. Multiple types of data mining tools may be employed to extract the data and an information reasoning capability determines what tools are needed to apply them to extract information. This can be visualized as a push-pull system where data trends fire a reasoning engine to search for corroborating evidence and then integrate the data into actionable information. The architecture decides on what reasoning engine to use (i.e., it may start with a rule-based method, but, if needed, go to condition based reasoning, and even a model-based reasoning engine for certain types of equipment). Initial results show that {AELEIS} is able to indicate to the user of potential fault conditions and root-cause information mined from a database. © 2011 {SPIE}.},
	booktitle = {Proceedings of {SPIE} - The International Society for Optical Engineering},
	author = {Woodley, R. and Petrov, P. and Noll, W.},
	date = {2011},
	note = {{ISSN}: 0277786X},
	keywords = {Data mining, Software agents, Expert systems, Database systems, Search engines, Algorithms, Potential faults, Data-mining tools, Logistics, Root cause, Data reasoning, Data trend, Equipment, Fleet operations, Heterogeneous data, Information dissemination, Information fusion, Information products, Integrated fashion, Logistical systems, Logistics enterprise, Model-based Reasoning, Multi-source information fusion, Reasoning capabilities, Reasoning engine, Rule-based method, Vehicle health monitoring, xno}
}

@inproceedings{eshkevari_exploratory_2011,
	title = {An exploratory study of identifier renamings},
	isbn = {978-1-4503-0574-7},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959236933&doi=10.1145%2f1985441.1985449&partnerID=40&md5=11ad8236748b5b292e44475803c43757},
	doi = {10.1145/1985441.1985449},
	abstract = {Identifiers play an important role in source code understandability, maintainability, and fault-proneness. This paper reports a study of identifier renamings in software systems, studying how terms (identifier atomic components) change in source code identifiers. Specifically, the paper (i) proposes a term renaming taxonomy, (ii) presents an approximate lightweight code analysis approach to detect and classify term renamings automatically into the taxonomy dimensions, and (iii) reports an exploratory study of term renamings in two open-source systems, Eclipse-{JDT} and Tomcat. We thus report evidence that not only synonyms are involved in renamings but also (in a small fraction) more unexpected changes occur: surprisingly, we detected hypernym (a more abstract term, e.g., size vs. length) and hyponym (a more concrete term, e.g., restriction vs. rule) renamings, and antonym renamings (a term replaced with one having the opposite meaning, e.g., closing vs. opening). Despite being only a fraction of all renamings, synonym, hyponym, hypernym, and antonym renamings may hint at some program understanding issues and, thus, could be used in a renamingrecommendation system to improve code quality. © 2011 {ACM}.},
	pages = {33--42},
	booktitle = {Proceedings - International Conference on Software Engineering},
	author = {Eshkevari, L.M. and Arnaoudova, V. and Di Penta, M. and Oliveto, R. and Guéhéneuc, Y.-G. and Antoniol, G.},
	date = {2011},
	note = {{ISSN}: 02705257},
	keywords = {Semantics, Software systems, Source codes, Maintainability, Taxonomies, Fault-proneness, Open source system, Atomic components, Code analysis, Code quality, Exploratory studies, identifier renaming, Program understanding, software evolution, Understandability, xno}
}

@article{stark_comparison_2011,
	title = {A comparison of parametric software estimation models using real project data},
	volume = {24},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79951524273&partnerID=40&md5=72dc666c142aa9f3af427e6c1b5bc490},
	abstract = {Defense managers and system engineers require estimates of project cost/effort, duration, and quality in order to secure funding and set xpectations with customers, end users, and management teams. Researchers and practitioners of software metrics have developed models to help project anagers and system engineers produce estimates of project effort, duration, and quality. These models generally quantify the project scope using estimated source lines of code or function points, and then require the application of generalized rules-of-thumb to arrive at the needed project estimates of staffing, duration, and quality. Experts agree that for these models to perform at their best, the parameters should be calibrated based on project data from the using organization. Our question was, "How do parametric models perform out-of-the-box (that is, without calibration)?" This is analogous to a project team without access to historical data using the models as published. What level of accuracy can they expect? We examined several published models by comparing the predicted values against the actual results from 54 software projects completed by a {SEI} {CMMI} Level 3 organization with a mature (and commended) measurement program. This paper evaluated a subset of these approaches - nine simple models (four effort estimation models, three duration estimation models, and two software quality (i.e., defect) models)-using 54 non-trivial commercial projects completed recently by a {CMMI} Level 3 organization. This certification means that the data was collected in a standard manner and makes sense to use in this study. It does not imply that a defined process level is needed to use the results. For the effort estimation models, we found that the upper bound of the best case model contained 81\% of our projects, that is, four out of five of our projects would use less effort than predicted by the best case model, whereas the average effort estimate across all models contained only 54\% of our projects, or a little better than a coin flip if we estimate using the average. Duration estimates performed significantly better. In the best case model, the upper bound estimate contained 93\% of our projects with the overall model average at 91\% and the lower bound estimate exceeded the actual duration more than 70\% of the time. This means we can out-perform the project duration seven out of 10 times using the shortest duration estimated using the models out-of-the box. For quality modeling, one of the defect prediction approaches worked quite well, with the upper bound containing 94\% of the projects (or 9.4 times out of 10 we will deliver fewer defects than forecast by the model). This information is useful to executives and managers performing early project estimates without detailed analysis of the requirements or architecture as the bounds allow them to quickly respond to customer requests with some level of confidence. So, if you are asked for a project estimate and do not have access to historical data or well-calibrated local estimation models, there is hope. Based on your available sizing information, you can use these models out-of-the-box with some success as long as you keep these things in mind: • Caper's Jones approach was the only one that (relatively) accurately addressed all three project management estimation needs for effort, duration, and quality. • None of the four effort estimation models were particularly effective with our project data, but using the upper bound of the Rone model gives the project team an 80\% chance of meeting the effort estimate. • A project should never commit to the lower bound effort estimates from any of the models we evaluated. • The duration estimation models are particularly effective with our project data. Using the upper bound of the Boehm model gives a project team a better than 90\% chance of completing the project within the estimated calendar time. • Capers Jones' quality model was the most accurate predictor of quantity of defects in our software development projects. From our analysis, it appears as though duration and quality models are quite useful, but effort estimation is still problematic. We suggest researchers investigate other approaches to effort estimation that are not based on {SLOC} or Function Points. For example, models that rely on use cases or story points and can estimate all three key parameters (i.e., effort, duration, and quality) may prove valuable in the future. The translation from mission or business need to requirements and architecture is a huge challenge that impacts estimates on each iteration, by developing models to address these early solution descriptions, managers and system engineers can benefit with earlier estimates.},
	pages = {22--27},
	number = {1},
	journaltitle = {{CrossTalk}},
	author = {Stark, G.},
	date = {2011},
	keywords = {Software metrics, Computer software selection and evaluation, Defects, Software design, Project management, Software Quality, Defect prediction, Software project, Software development projects, Lower bounds, Effort Estimation, Estimation, Business needs, Commercial projects, Developed model, Effort estimation model, End users, Engineers, Estimation models, Function point, Historical data, Key parameters, Local estimation, Management team, Managers, Measurement programs, Non-trivial, Overall-model, Parametric models, Parametric software estimation models, Process levels, Project data, Project duration, Project estimates, Project scope, Project team, Quality modeling, Quality models, Real projects, Source lines of codes, System engineers, Upper Bound, xno}
}

@article{peng_ensemble_2011,
	title = {Ensemble of software defect predictors: An {AHP}-based evaluation method},
	volume = {10},
	issn = {02196220},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-78751522102&doi=10.1142%2fS0219622011004282&partnerID=40&md5=d535f5e25a57ca903f2c57a8efface15},
	doi = {10.1142/S0219622011004282},
	abstract = {Classification algorithms that help to identify software defects or faults play a crucial role in software risk management. Experimental results have shown that ensemble of classifiers are often more accurate and robust to the effects of noisy data, and achieve lower average error rate than any of the constituent classifiers. However, inconsistencies exist in different studies and the performances of learning algorithms may vary using different performance measures and under different circumstances. Therefore, more research is needed to evaluate the performance of ensemble algorithms in software defect prediction. The goal of this paper is to assess the quality of ensemble methods in software defect prediction with the analytic hierarchy process ({AHP}), which is a multicriteria decision-making approach that prioritizes decision alternatives based on pairwise comparisons. Through the application of the {AHP}, this study compares experimentally the performance of several popular ensemble methods using 13 different performance metrics over 10 public-domain software defect datasets from the {NASA} Metrics Data Program ({MDP}) repository. The results indicate that ensemble methods can improve the classification results of software defect prediction in general and {AdaBoost} gives the best results. In addition, tree and rule based classifiers perform better in software defect prediction than other types of classifiers included in the experiment. In terms of single classifier, K-nearest-neighbor, C4.5, and Naïve Bayes tree ranked higher than other classifiers. © 2011 World Scientific Publishing Company.},
	pages = {187--206},
	number = {1},
	journaltitle = {International Journal of Information Technology and Decision Making},
	author = {Peng, Y. and Kou, G. and Wang, G. and Wu, W. and Shi, Y.},
	date = {2011},
	keywords = {xno},
	file = {Submitted Version:C\:\\Users\\michalm\\Zotero\\storage\\T8PAZY9S\\Peng et al. - 2011 - ENSEMBLE OF SOFTWARE DEFECT PREDICTORS AN AHP-BAS.pdf:application/pdf}
}

@inproceedings{carzaniga_automatic_2010,
	title = {Automatic workarounds for web applications},
	isbn = {978-1-60558-791-2},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-78751558915&doi=10.1145%2f1882291.1882327&partnerID=40&md5=f927b8893ddcb109732b0ad28ac0bfd7},
	doi = {10.1145/1882291.1882327},
	abstract = {We present a technique that finds and executes workarounds for faulty Web applications automatically and at runtime. Automatic workarounds exploit the inherent redundancy of Web applications, whereby a functionality of the application can be obtained through different sequences of invocations of Web {APIs}. In general, runtime workarounds are applied in response to a failure, and require that the application remain in a consistent state before and after the execution of a workaround. Therefore, they are ideally suited for interactive Web applications, since those allow the user to act as a failure detector with minimal effort, and also either use read-only state or manage their state through a transactional data store. In this paper we focus on faults found in the access libraries of widely used Web applications such as Google Maps. We start by classifying a number of reported faults of the Google Maps and {YouTube} {APIs} that have known workarounds. From those we derive a number of general and {API}-specific program-rewriting rules, which we then apply to other faults for which no workaround is known. Our experiments show that workarounds can be readily deployed within Web applications, through a simple client-side plug-in, and that program-rewriting rules derived from elementary properties of a common library can be effective in finding valid and previously unknown workarounds. © 2010 {ACM}.},
	pages = {237--246},
	booktitle = {Proceedings of the {ACM} {SIGSOFT} Symposium on the Foundations of Software Engineering},
	author = {Carzaniga, A. and Gorla, A. and Perino, N. and Pezzè, M.},
	date = {2010},
	keywords = {Software engineering, automatic workarounds, Before and after, Consistent state, Failure Detectors, Google maps, Inherent redundancy, Interactive web applications, Plug-ins, Rewriting rules, Runtimes, Transactional data, web api, {WEB} application, World Wide Web, {YouTube}, xno},
	file = {Full Text:C\:\\Users\\michalm\\Zotero\\storage\\GIB3UP8H\\Carzaniga et al. - 2010 - Automatic workarounds for web applications.pdf:application/pdf}
}

@article{de_carvalho_non-ordered_2010,
	title = {A Non-ordered rule induction algorithm through multi-objective particle Swarm Optimization: Issues and applications},
	volume = {261},
	issn = {1860949X},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-78049237861&doi=10.1007%2f978-3-642-05165-4_2&partnerID=40&md5=e274d4ab47045da5492dae6cc716bfa5},
	doi = {10.1007/978-3-642-05165-4_2},
	abstract = {Multi-Objective Metaheuristics permit to conceive a complete novel approach to induce classifiers, where the properties of the rules can be expressed in different objectives, and then the algorithm finds these rules in an unique run by exploring Pareto dominance concepts. Furthermore, these rules can be used as an unordered classifier, in this way, the rules are more intuitive and easier to understand because they can be interpreted independently one of the other. The quality of the learned rules is not affected during the learning process because the dataset is not modified, as in traditional rule induction approaches. With this philosophy, this chapter describes a Multi-Objective Particle Swarm Optimization ({MOPSO}) algorithm. One reason to choose the Particle Swarm Optimization Meta heuristic is its recognized ability to work in numerical domains. This propriety allows the described algorithm deals with both numerical and discrete attributes. The algorithm is evaluated by using the area under {ROC} curve and, by comparing the performance of the induced classifiers with other ones obtained with well known rule induction algorithms. The produced Pareto Front coverage of the algorithm is also analyzed following a Multi-Objective methodology. In addition to this, some application results in the Software Engineering domain are described, more specifically in the context of software testing. Software testing is a fundamental Software Engineering activity for quality assurance that is traditionally very expensive. The algorithm is used to induce rules for fault-prediction that can help to reduce testing efforts. The empirical evaluation and the comparison show the effectiveness and scalability of this new approach. © 2010 Springer-Verlag Berlin Heidelberg.},
	pages = {17--44},
	journaltitle = {Studies in Computational Intelligence},
	author = {De Carvalho, A.B. and Pozo, A. and Vergilio, S.},
	date = {2010},
	note = {{ISBN}: 9783642051647},
	keywords = {xno}
}

@inproceedings{sami_design-level_2010,
	title = {Design-level metrics estimation based on code metrics},
	isbn = {978-1-60558-638-0},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954733232&doi=10.1145%2f1774088.1774612&partnerID=40&md5=d78121d5c680fa18faad511b0b68faa2},
	doi = {10.1145/1774088.1774612},
	abstract = {Fault detection based on mining code and design metrics has been an active research area for many years. Basically "module"-based metrics for source code and design level are calculated or obtained and data mining is used to build predictor models. However, in many projects due to organizational or software process models, design level metrics are not available and/or accurate. It has been shown that performance of these classifiers or predictors decline if only source code features are used for training them. Based on best of our know knowledge no set of rule to estimate design level metrics based on code level metrics has been presented since it is believed that design level metrics have additional information and cannot be estimated without access to design artifacts. In this study we present a fuzzy modeling system to find and present these relationships for projects presented in {NASA} Metrics Data Repository ({MDP}) datasets. Interestingly, we could find a set of empirical rules that govern all the projects regardless of size, programming language and software development methodology. Comparison of fault detectors built based on estimated design metrics with actual design metrics on various projects showed a very small difference in accuracy of classifiers and validated our hypothesis that estimation of design metrics based on source code attributes can become a practical exercise. © 2010 {ACM}.},
	pages = {2531--2535},
	booktitle = {Proceedings of the {ACM} Symposium on Applied Computing},
	author = {Sami, A. and Fakhrahmad, S.M.},
	date = {2010},
	keywords = {Software metrics, Defects, Software design, {NASA}, Software defect prediction, Fault detection, Computer software, Fuzzy systems, Classifiers, Source codes, Set of rules, Design, Fuzzy classification, Data sets, Parameter estimation, Code metrics, Data repositories, Design artifacts, Design levels, Design metrics, Fault detector, Fuzzy modeling systems, Mining codes, Programming language, Research areas, Software development methodologies, Software process models, xyes, xnacc}
}

@article{zheng_cost-sensitive_2010,
	title = {Cost-sensitive boosting neural networks for software defect prediction},
	volume = {37},
	issn = {09574174},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-77249108251&doi=10.1016%2fj.eswa.2009.12.056&partnerID=40&md5=c5ace00567884beaa8404f785eb1702b},
	doi = {10.1016/j.eswa.2009.12.056},
	abstract = {Software defect predictors which classify the software modules into defect-prone and not-defect-prone classes are effective tools to maintain the high quality of software products. The early prediction of defect-proneness of the modules can allow software developers to allocate the limited resources on those defect-prone modules such that high quality software can be produced on time and within budget. In the process of software defect prediction, the misclassification of defect-prone modules generally incurs much higher cost than the misclassification of not-defect-prone ones. Most of the previously developed predication models do not consider this cost issue. In this paper, three cost-sensitive boosting algorithms are studied to boost neural networks for software defect prediction. The first algorithm based on threshold-moving tries to move the classification threshold towards the not-fault-prone modules such that more fault-prone modules can be classified correctly. The other two weight-updating based algorithms incorporate the misclassification costs into the weight-update rule of boosting procedure such that the algorithms boost more weights on the samples associated with misclassified defect-prone modules. The performances of the three algorithms are evaluated by using four datasets from {NASA} projects in terms of a singular measure, the Normalized Expected Cost of Misclassification ({NECM}). The experimental results suggest that threshold-moving is the best choice to build cost-sensitive software defect prediction models with boosted neural networks among the three algorithms studied, especially for the datasets from projects developed by object-oriented language. © 2009 Elsevier Ltd. All rights reserved.},
	pages = {4537--4543},
	number = {6},
	journaltitle = {Expert Systems with Applications},
	author = {Zheng, J.},
	date = {2010},
	keywords = {Computer software selection and evaluation, Defects, {NASA}, Software defect prediction, Software defects, Object oriented programming, High-quality software, Software developer, Software modules, Early prediction, Adaptive boosting, Neural networks, Algorithms, Costs, Fault-prone modules, Mathematical models, Data sets, {AdaBoost}, Best choice, Boosting algorithm, Cost-sensitive, Effective tool, Expected cost of misclassification, High quality, Misclassification costs, Misclassifications, {NASA} projects, Object-oriented languages, {ON} time, Predication model, Singular measures, xno}
}

@article{severin_ehive_2010,
	title = {{eHive}: An Artificial Intelligence workflow system for genomic analysis},
	volume = {11},
	issn = {14712105},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953983801&doi=10.1186%2f1471-2105-11-240&partnerID=40&md5=bba433ae680a556b39304e390748ecc9},
	doi = {10.1186/1471-2105-11-240},
	abstract = {Background: The Ensembl project produces updates to its comparative genomics resources with each of its several releases per year. During each release cycle approximately two weeks are allocated to generate all the genomic alignments and the protein homology predictions. The number of calculations required for this task grows approximately quadratically with the number of species. We currently support 50 species in Ensembl and we expect the number to continue to grow in the future.Results: We present {eHive}, a new fault tolerant distributed processing system initially designed to support comparative genomic analysis, based on blackboard systems, network distributed autonomous agents, dataflow graphs and block-branch diagrams. In the {eHive} system a {MySQL} database serves as the central blackboard and the autonomous agent, a Perl script, queries the system and runs jobs as required. The system allows us to define dataflow and branching rules to suit all our production pipelines. We describe the implementation of three pipelines: (1) pairwise whole genome alignments, (2) multiple whole genome alignments and (3) gene trees with protein homology inference. Finally, we show the efficiency of the system in real case scenarios.Conclusions: {eHive} allows us to produce computationally demanding results in a reliable and efficient way with minimal supervision and high throughput. Further documentation is available at: http://www.ensembl.org/info/docs/{eHive}/. © 2010 Severin et al; licensee {BioMed} Central Ltd.},
	journaltitle = {{BMC} Bioinformatics},
	author = {Severin, J. and Beal, K. and Vilella, A.J. and Fitzgerald, S. and Schuster, M. and Gordon, L. and Ureta-Vidal, A. and Flicek, P. and Herrero, J.},
	date = {2010},
	keywords = {Software, Artificial intelligence, Alignment, article, Autonomous agents, Blackboard systems, Comparative genomic, Comparative genomics, computer program, Data flow analysis, Databases, Distributed processing systems, Genes, Genetic, genetic database, genome, Genome, genomics, Genomics, methodology, Multiple whole genome alignment, Production pipelines, Protein homology prediction, Proteins, Query languages, Whole genome alignment, xno},
	file = {Full Text:C\:\\Users\\michalm\\Zotero\\storage\\2HDUVGYQ\\Severin et al. - 2010 - eHive An Artificial Intelligence workflow system .pdf:application/pdf}
}

@article{rodriguez-echeverria_suggesting_2021,
	title = {Suggesting model transformation repairs for rule-based languages using a contract-based testing approach},
	issn = {1619-1366},
	doi = {10.1007/s10270-021-00891-0},
	abstract = {Model transformations play an essential role in most model-driven software projects. As the size and complexity of model transformations increase, their reuse, evolution and maintenance become a challenge. This work further details the Model Transformation {TEst} Specification ({MoTES}) approach, which leverages contract-based model testing techniques to assist engineers in model transformation evolution and repairing. The main novelty of our approach is to use contract-based model transformation testing as a foundation to derive suggestions of concrete adaptation actions. {MoTES} uses contracts to specify the expected behaviour of the model transformation under test. These contracts are transformed into model transformations which act as oracles on input-output model pairs, previously generated by executing the transformation under test on provided input models. By further processing, the oracles' output model, precision and recall metrics are calculated for every output pattern (testing results). These metrics are then categorised to increase the user's ability to interpret and act on them. The {MoTES} approach defines 8 cases for precision and recall values classification (test result cases). As traceability information is retained from transformation rules to each output pattern, it is possible to classify each transformation rule involved according to its impact on the metrics, e.g. the number of true positives generated. The {MoTES} approach defines 37 cases for these classifications, with each one linked to a particular (abstract) action suggested on a rule, such as relaxation of the rules. A comprehensive evaluation of this approach is also presented, consisting of three case studies. Two previous case studies performed over two model transformations ({UML}2ER and E2M) are replicated to contrast {MoTES} with an existing model transformation fault localisation approach. An additional case study presents how {MoTES} helps with the evolution of an existing model transformation in the context of a reverse engineering project. Main evaluation results show that our approach can not only detect the errors introduced in the transformations but also localise the faulty rule and suggest the proper repair actions, which significantly reduce testers' effort. From a quantitative perspective, in the third case study, {MoTES} was able to indicate one faulty rule from 19 possibilities for each result case and suggest one or two repair actions from 6 possibilities for each faulty rule.},
	journaltitle = {{SOFTWARE} {AND} {SYSTEMS} {MODELING}},
	author = {Rodriguez-Echeverria, Roberto and Macias, Fernando and Rutle, Adrian and Conejero, Jose M.},
	date = {2021},
	keywords = {Testing, Precision and recall, Classification (of information), Comprehensive evaluation, Evaluation results, Model transformation, Repair, Reverse engineering, Rule-based language, Test specifications, Traceability information, Transformation rules, xno}
}

@article{ceron-figueroa_stochastic_2020,
	title = {Stochastic gradient boosting for predicting the maintenance effort of software-intensive systems},
	volume = {14},
	issn = {1751-8806},
	doi = {10.1049/iet-sen.2018.5332},
	abstract = {The maintenance of software-intensive systems ({SISs}) must be undertaken to correct faults, improve the design, implement enhancements, adapt programmes such that different hardware, software, system features, and telecommunications facilities can be used, as well as to migrate legacy software. A lack of planning has been identified as one explanation for late and over budget software projects. An activity of planning is effort prediction. The goal of this study is to propose the application of a stochastic gradient boosting ({SGB}) model for predicting the {SIS} maintenance effort. We compare the {SGB} prediction accuracy with those obtained with statistical regression, neural network, support vector regression, decision trees, and association rules. We trained and tested the models with five {SIS} data sets selected from the International Software Benchmarking Standards Group Release 11. The {SGB} prediction accuracy was statistically better than the mentioned five models in the two larger data sets. We can conclude that a {SGB} can be applied to predict the maintenance effort of {SISs} coded in languages of the third generation and developed on either mainframes or multi-platform. The predicted effort corresponds to the aggregate of efforts obtained from the project team, project management, and project administration.},
	pages = {82--87},
	number = {2},
	journaltitle = {{IET} {SOFTWARE}},
	author = {Ceron-Figueroa, Sergio and Lopez-Martin, Cuauhtemoc and Yanez-Marquez, Cornelio},
	date = {2020-04},
	note = {Publisher: {IEEE}; Centro Investigacion Matematicas A C},
	keywords = {Forecasting, Stochastic systems, Decision trees, Software testing, Budget control, Computer software maintenance, Effort prediction, Human resource management, International Software Benchmarking Standards Group, Legacy systems, Maintenance efforts, Prediction accuracy, Project management, Software intensive systems, Statistical regression, Stochastic gradient boosting, Stochastic models, Support vector regression, Third generation, xno},
	file = {Full Text:C\:\\Users\\michalm\\Zotero\\storage\\UMH7U5YA\\Cerón-Figueroa et al. - 2020 - Stochastic gradient boosting for predicting the ma.pdf:application/pdf}
}

@article{niedermayr_too_2019,
	title = {Too trivial to test? An inverse view on defect prediction to identify methods with low fault risk},
	issn = {2376-5992},
	doi = {10.7717/peerj-cs.187},
	abstract = {Background: Test resources are usually limited and therefore it is often not possible to completely test an application before a release. To cope with the problem of scarce resources, development teams can apply defect prediction to identify fault-prone code regions. However, defect prediction tends to low precision in cross-project prediction scenarios. Aims: We take an inverse view on defect prediction and aim to identify methods that can be deferred when testing because they contain hardly any faults due to their code being “trivial”. We expect that characteristics of such methods might be project-independent, so that our approach could improve cross-project predictions. Method: We compute code metrics and apply association rule mining to create rules for identifying methods with low fault risk ({LFR}). We conduct an empirical study to assess our approach with six Java open-source projects containing precise fault data at the method level. Results: Our results show that inverse defect prediction can identify approx. 32-44\% of the methods of a project to have a {LFR}; on average, they are about six times less likely to contain a fault than other methods. In cross-project predictions with larger, more diversified training sets, identified methods are even 11 times less likely to contain a fault. Conclusions: Inverse defect prediction supports the efficient allocation of test resources by identifying methods that can be treated with less priority in testing activities and is well applicable in cross-project prediction scenarios.},
	journaltitle = {{PEERJ} {COMPUTER} {SCIENCE}},
	author = {Niedermayr, Rainer and Roehm, Tobias and Wagner, Stefan},
	date = {2019-04-15},
	keywords = {Testing, Defects, Empirical studies, Open source software, Forecasting, Open source projects, Codes (symbols), Defect prediction, Development teams, Efficient allocations, Fault-prone codes, Identifying methods, Inverse problems, Low-fault-risk methods, Well testing, xno},
	file = {Full Text:C\:\\Users\\michalm\\Zotero\\storage\\9G2E4NKN\\Niedermayr et al. - 2019 - Too trivial to test An inverse view on defect pre.pdf:application/pdf}
}

@inproceedings{petric_jinx_2016,
	title = {The Jinx on the {NASA} Software Defect Data Sets},
	isbn = {978-1-4503-3691-8},
	doi = {10.1145/2915970.2916007},
	abstract = {Background: The {NASA} datasets have previously been used extensively in studies of software defects. In 2013 Shepperd et al. presented an essential set of rules for removing erroneous data from the {NASA} datasets making this data more reliable to use. Objective: We have now found additional rules necessary for removing problematic data which were not identified by Shepperd et al. Results: In this paper, we demonstrate the level of erroneous data still present even after cleaning using Shepperd et al.'s rules and apply our new rules to remove this erroneous data. Conclusion: Even after systematic data cleaning of the {NASA} {MDP} datasets, we found new erroneous data. Data quality should always be explicitly considered by researchers before use.},
	booktitle = {{PROCEEDINGS} {OF} {THE} 20TH {INTERNATIONAL} {CONFERENCE} {ON} {EVALUATION} {AND} {ASSESSMENT} {IN} {SOFTWARE} {ENGINEERING} 2016 ({EASE} `16)},
	author = {Petric, Jean and Bowes, David and Hall, Tracy and Christianson, Bruce and Baddoo, Nathan},
	date = {2016},
	keywords = {Defects, {NASA}, Software defect prediction, Software defects, Learning systems, Software engineering, Artificial intelligence, Aluminum, Data cleaning, Data quality, Erroneous datum, Set of rules, xno},
	file = {Accepted Version:C\:\\Users\\michalm\\Zotero\\storage\\WI7QTY7P\\Petrić et al. - 2016 - The jinx on the NASA software defect data sets.pdf:application/pdf}
}

@article{huang_links_2014,
	title = {The links between human error diversity and software diversity: Implications for fault diversity seeking},
	volume = {89},
	issn = {0167-6423},
	doi = {10.1016/j.scico.2014.03.004},
	abstract = {Software diversity is known to improve fault tolerance in N-version software systems by independent development. As the leading cause of software faults, human error is considered an important factor in diversity seeking. However, there is little scientific research focusing on how to seek software fault diversity based on human error mechanisms. A literature review was conducted to extract factors that may differentiate people with respect to human error-proneness. In addition, we constructed a conceptual model of the links between human error diversity and software diversity. An experiment was designed to validate the hypotheses, in the form of a programming contest, accompanied by a survey of cognitive styles and personality traits. One hundred ninety-two programs were submitted for the identical problem, and 70 surveys were collected. Code inspection revealed 23 faults, of which 10 were coincident faults. The results show that personality traits seems not effective predictors for fault diversity as a whole model, whereas cognitive styles and program measurements moderately account for the variation of fault density. The results also show causal relations between performance levels and coincident faults; coincident faults are unlikely to occur at skill-based performance level; the coincident faults introduced in rule-based performances show a high probability of occurrence, and the coincident faults introduced in knowledge-based performances are shaped by the content and formats of the task itself. Based on these results, we have proposed a model to seek software diversity and prevent coincident faults. (C) 2014 Elsevier B.V. All rights reserved.},
	pages = {350--373},
	issue = {C},
	journaltitle = {{SCIENCE} {OF} {COMPUTER} {PROGRAMMING}},
	author = {Huang, Fuqun and Liu, Bin and Song, You and Keyal, Shreya},
	date = {2014-09-01},
	keywords = {Surveys, Computer software, Knowledge based systems, Cognitive styles, Errors, Human errors, N version programming, Personality traits, Software diversity, N-version programming, Cognitive style, Human error, Personality trait, xno}
}

@article{czibula_software_2014,
	title = {Software defect prediction using relational association rule mining},
	volume = {264},
	issn = {0020-0255},
	doi = {10.1016/j.ins.2013.12.031},
	abstract = {This paper focuses on the problem of defect prediction, a problem of major importance during software maintenance and evolution. It is essential for software developers to identify defective software modules in order to continuously improve the quality of a software system. As the conditions for a software module to have defects are hard to identify, machine learning based classification models are still developed to approach the problem of defect prediction. We propose a novel classification model based on relational association rules mining. Relational association rules are an extension of ordinal association rules, which are a particular type of association rules that describe numerical orderings between attributes that commonly occur over a dataset. Our classifier is based on the discovery of relational association rules for predicting whether a software module is or it is not defective. An experimental evaluation of the proposed model on the open source {NASA} datasets, as well as a comparison to similar existing approaches is provided. The obtained results show that our classifier overperforms, for most of the considered evaluation measures, the existing machine learning based techniques for defect prediction. This confirms the potential of our proposal. (C) 2014 Elsevier Inc. All rights reserved.},
	pages = {260--278},
	journaltitle = {{INFORMATION} {SCIENCES}},
	author = {Czibula, Gabriela and Marian, Zsuzsanna and Czibula, Istvan Gergely},
	date = {2014-04-20},
	keywords = {xyes}
}

@article{khoshgoftaar_software_2014,
	title = {Software quality assessment using a multi-strategy classifier},
	volume = {259},
	issn = {0020-0255},
	doi = {10.1016/j.ins.2010.11.028},
	abstract = {Classifying program Modules as fault-prone or not fault-prone is a valuable technique for guiding the software development process, so that resources can be allocated to components most likely to have faults. The rule-based classification and the case-based learning techniques are commonly used in software quality classification problems. However, studies show that these two techniques share some complementary strengths and weaknesses. Therefore, in this paper we propose a new multi-strategy classification model, {RB}2CBL, which integrates a rule-based ({RB}) model with two case-based learning ({CBL}) models. {RB}2CBL possesses the merits of both the {RB} model and {CBL} model and restrains their drawbacks. In the {RB}2CBL model, the parameter optimization of the {CBL} models is critical and an embedded genetic algorithm optimizer is used. Two case studies were carried out to validate the proposed method. The results show that, by suitably choosing the accuracy of the {RB} model, the {RB}2CBL model outperforms the {RB} model alone without overfitting. (C) 2010 Elsevier Inc. All rights reserved.},
	pages = {555--570},
	journaltitle = {{INFORMATION} {SCIENCES}},
	author = {Khoshgoftaar, Taghi M. and Xiao, Yudong and Gao, Kehan},
	date = {2014-02-20},
	keywords = {Computer software selection and evaluation, Genetic algorithms, Software engineering, Software Quality, Software development process, Classification models, Rule-based classification, Case based learning, Parameter optimization, Rule-based models, Software quality assessment, xyes}
}

@inproceedings{xiao_titan_2014,
	title = {Titan: A Toolset That Connects Software Architecture with Quality Analysis},
	isbn = {978-1-4503-3056-5},
	doi = {10.1145/2635868.2661677},
	abstract = {In this tool demo, we will illustrate our tool-Titan-that supports a new architecture model: design rule spaces ({DR}-Spaces). We will show how Titan can capture both architecture and evolutionary structure and help to bridge the gap between architecture and defect prediction. We will demo how to use our toolset to capture hundreds of buggy files into just a few architecturally related groups, and to reveal architecture issues that contribute to the error-proneness and change-proneness of these groups. Our tool has been used to analyze dozens of large-scale industrial projects, and has demonstrated its ability to provide valuable direction on which parts of the architecture are problematic, and on why, when, and how to refactor. The video demo of Titan can be found at https://art.cs.drexel.edu/(similar to)lx52/titan.mp4},
	pages = {763--766},
	booktitle = {22ND {ACM} {SIGSOFT} {INTERNATIONAL} {SYMPOSIUM} {ON} {THE} {FOUNDATIONS} {OF} {SOFTWARE} {ENGINEERING} ({FSE} 2014)},
	publisher = {Assoc Comp Machinery Special Interest Grp Software Engn; {CVIC} {SE}; {NSF}; Microsoft Res; Huawei; Neusoft; Siemens; Yonyou; Hong Kong Univ Sci \& Technol; Google; Radica; Samsung Res Amer; {IBM} Res; {TCL}; {CCC}},
	author = {Xiao, Lu and Cai, Yuanfang and Kazman, Rick},
	date = {2014},
	keywords = {Computer software selection and evaluation, Industrial projects, Software engineering, Computer software maintenance, Software Quality, Defect prediction, Software architecture, Design rules, Architecture modeling, Change proneness, Error proneness, Toolsets, xno}
}

@article{park_design_2013,
	title = {The design of polynomial function-based neural network predictors for detection of software defects},
	volume = {229},
	issn = {0020-0255},
	doi = {10.1016/j.ins.2011.01.026},
	abstract = {In this study, we introduce a design methodology of polynomial function-based Neural Network (pf-{NN}) classifiers (predictors). The essential design components include Fuzzy C-Means ({FCM}) regarded as a generic clustering algorithm and polynomials providing all required nonlinear capabilities of the model. The learning method uses a weighted cost function (objective function) while to analyze the performance of the system we engage a standard receiver operating characteristics ({ROC}) analysis. The proposed networks are used to detect software defects. From the conceptual standpoint, the classifier of this form can be expressed as a collection of “if-then” rules. Fuzzy clustering (Fuzzy C-Means, {FCM}) is aimed at the development of premise layer of the rules while the corresponding consequences of the rules are formed by some local polynomials. A detailed learning algorithm for the pf-{NNs} is presented with particular provisions made for dealing with imbalanced classes encountered quite commonly in software quality problems. The use of simple measures such as accuracy of classification becomes questionable. In the assessment of quality of classifiers, we confine ourselves to the use of the area under curve ({AUC}) in the receiver operating characteristics ({ROCs}) analysis. {AUC} comes as a sound classifier metric capturing a tradeoff between the high true positive rate ({TP}) and the low false positive rate ({FP}). The performance of the proposed classifier is contrasted with the results produced by some “standard” Radial Basis Function ({RBF}) neural networks. (C) 2011 Elsevier Inc. All rights reserved.},
	pages = {40--57},
	journaltitle = {{INFORMATION} {SCIENCES}},
	author = {Park, Byoung-Jun and Oh, Sung-Kwun and Pedrycz, Witold},
	date = {2013-04-20},
	keywords = {Clustering algorithms, Computer software selection and evaluation, Defects, Software defects, Receiver operating characteristics, Pattern recognition, Software Quality, False positive rates, Learning algorithms, Imbalanced data, Fuzzy systems, Neural networks, Quality control, Design, Design Component, Design Methodology, Functions, Fuzzy C mean, Fuzzy C-means, Fuzzy clustering, Imbalanced class, Learning methods, Neural network predictor, Objective functions, Radial basis function networks, Radial basis function neural networks, Receiver operating characteristics analysis, True positive rates, Two-class discrimination, Weighted cost functions, xno}
}

@article{rodriguez_searching_2012,
	title = {Searching for rules to detect defective modules: A subgroup discovery approach},
	volume = {191},
	issn = {0020-0255},
	doi = {10.1016/j.ins.2011.01.039},
	abstract = {Data mining methods in software engineering are becoming increasingly important as they can support several aspects of the software development life-cycle such as quality. In this work, we present a data mining approach to induce rules extracted from static software metrics characterising fault-prone modules. Due to the special characteristics of the defect prediction data (imbalanced, inconsistency, redundancy) not all classification algorithms are capable of dealing with this task conveniently. To deal with these problems, Subgroup Discovery ({SD}) algorithms can be used to find groups of statistically different data given a property of interest. We propose {EDER}-{SD} (Evolutionary Decision Rules for Subgroup Discovery), a {SD} algorithm based on evolutionary computation that induces rules describing only fault-prone modules. The rules are a well-known model representation that can be easily understood and applied by project managers and quality engineers. Thus, rules can help them to develop software systems that can be justifiably trusted. Contrary to other approaches in {SD}, our algorithm has the advantage of working with continuous variables as the conditions of the rules are defined using intervals. We describe the rules obtained by applying our algorithm to seven publicly available datasets from the {PROMISE} repository showing that they are capable of characterising subgroups of fault-prone modules. We also compare our results with three other well known {SD} algorithms and the {EDER}-{SD} algorithm performs well in most cases. (C) 2011 Elsevier Inc. All rights reserved.},
	pages = {14--30},
	journaltitle = {{INFORMATION} {SCIENCES}},
	author = {Rodriguez, D. and Ruiz, R. and Riquelme, J. C. and Aguilar-Ruiz, J. S.},
	date = {2012-05-15},
	keywords = {Data mining, Software metrics, Defects, Software engineering, Project management, Defect prediction, Classification algorithm, Software systems, Evolutionary algorithms, Imbalanced Data-sets, Rules, Fault-prone modules, Subgroup discovery, Data sets, Continuous variables, Data mining methods, Decision rules, Model representation, Project managers, Quality engineers, Software development life cycle, xyes, xbest},
	file = {Submitted Version:C\:\\Users\\michalm\\Zotero\\storage\\BQZNCWEN\\Rodríguez et al. - 2012 - Searching for rules to detect defective modules A.pdf:application/pdf}
}

@inproceedings{tua_software_2019,
	title = {Software Defect Prediction Using Software Metrics with Naïve Bayes and Rule Mining Association Methods},
	volume = {1},
	doi = {10.1109/ICST47872.2019.9166448},
	abstract = {Software defect prediction ({SDP}) can help testers decide allocation of resources rationally to find defects effectively, so as to improve software quality. Naive Bayes ({NB}) is one of the most used classification algorithms because of the simplicity of the algorithm and easy to implement. The purpose of this study is to add the process of selecting features using {ARM} in the software prediction process using the {NB} method in the hope that it can improve the performance of the method using software metrics. Software metrics have an association with one another in completing software, so this cannot be ignored. Results of the empirical evaluation of scenario 1 (one) showed an increase with the values of parameter precision, recall, f-measure and accuracy of 0.101, 0.190, 0.154 and 0.180, and scenario 2 (second) also increased by 0.106, 0.182, 0.159 and 0.163, also as in scenario 3 (third) the proposed method shows good performance compared to using {SVM}, {NN} and {DTREE} with an average performance of 0.960 while the others are 0.855, 0.859 and 0.861. From the empirical results of the three scenarios made, the proposed performance method is better than the other methods.},
	pages = {1--5},
	booktitle = {2019 5th International Conference on Science and Technology ({ICST})},
	author = {Tua, Fernando Maruli and Danar Sunindyo, Wikan},
	date = {2019-07},
	keywords = {association rule mining, Barium compounds, Classification algorithm, Clustering algorithms, Computer software selection and evaluation, Data mining, Defects, Empirical evaluations, F measure, Feature extraction, feature selection, Forecasting, k-means, Naive bayes, Naïve Bayes, Prediction process, Predictive models, Rule mining, Software, software defect prediction, Software defect prediction, software engineering, Software metrics, Software quality, Support vector machines, Testing, xno}
}

@inproceedings{karthik_defect_2010,
	title = {Defect association and complexity prediction by mining association and clustering rules},
	volume = {7},
	doi = {10.1109/ICCET.2010.5485608},
	abstract = {Number of defects remaining in a system provides an insight into the quality of the system. Software defect prediction focuses on classifying the modules of a system into fault prone and non-fault prone modules. This paper focuses on predicting the fault prone modules as well as identifying the types of defects that occur in the fault prone modules. Software defect prediction is combined with association rule mining to determine the associations that occur among the detected defects and the effort required for isolating and correcting these defects. Clustering rules are used to classify the defects into groups indicating their complexity: {SIMPLE}, {MODERATE} and {COMPLEX}. Moreover the defects are used to predict the effect on the project schedules and the nature of risk concerning the completion of such projects.},
	pages = {V7--569--V7--573},
	booktitle = {2010 2nd International Conference on Computer Engineering and Technology},
	author = {Karthik, R. and Manikandan, N.},
	date = {2010-04},
	keywords = {Association rule mining, Association rules, Associative processing, Clustering, Clustering rules, Complexity predictions, Data mining, Defect Associations, Defect classification, Defect Classification, Defect correction effort, Defect Correction Effort, Defects, Fault diagnosis, Fault-prone, Fault-prone modules, Forecasting, Information technology, Mining associations, Prediction methods, Project management, Project schedules, Resource management, Risk analysis, Software defect prediction, Software systems, Testing, xyes},
	file = {Karthik and Manikandan - 2010 - Defect association and complexity prediction by mi.pdf:C\:\\Users\\michalm\\Zotero\\storage\\TCX4Z455\\Karthik and Manikandan - 2010 - Defect association and complexity prediction by mi.pdf:application/pdf}
}

@inproceedings{thapa_software_2020,
	title = {Software Defect Prediction Using Atomic Rule Mining and Random Forest},
	doi = {10.1109/CITISIA50690.2020.9371797},
	abstract = {This research aims to improve software defect prediction in terms of accuracy and processing time. The new proposed algorithm is based on the Random Forest Algorithm that classifies and distributes the data based on tree module. It has value either 1 for defective module or 0 for the non-defective module. Random Forest Algorithm selects a feature from a subset of features which has been already classified. Random Forest Algorithm uses a number of trees for the prediction. For this research, datasets were tested with 10 and 15 sets of trees. Results showed an improvement in accuracy and processing time when the proposed system was used compared to the current solution for the software defect model generation and prediction. The proposed solution achieved an accuracy of 90.09\% whereas processing time dropped by 54.14\%. Processing time decreased from 19.78s to 9.07s during the prediction for over 100 records. Accuracy was improved from 89.97\% to 90.09\%. The proposed solution uses Atomic Rule Mining with Random Forest Algorithm for software defect prediction. It consists of classification and prediction process by using the Random Forest Algorithm during storing data that is carried out using Atomic Rule Mining.},
	pages = {1--8},
	booktitle = {2020 5th International Conference on Innovative Technologies in Intelligent Systems and Industrial Applications ({CITISIA})},
	author = {Thapa, Suroj and Alsadoon, Abeer and Prasad, P.W.C. and Al-Dala’in, Thair and Rashid, Tarik A.},
	date = {2020-11},
	keywords = {Apriori, Atomic Rule Mining, Atoms, Classification algorithms, Data mining, Decision trees, Deep Learning, Defects, Forecasting, Forestry, Intelligent systems, Number of trees, Prediction algorithms, Prediction process, Processing time, Random Forest, Random forest algorithm, Random forests, Rule mining, Software, Software algorithms, Software defect prediction, Software Defect Prediction, Software defects, Tree modules, Vegetation, xno}
}

@article{he_ensemble_2019,
	title = {Ensemble {MultiBoost} Based on {RIPPER} Classifier for Prediction of Imbalanced Software Defect Data},
	volume = {7},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2019.2934128},
	abstract = {Identifying defective software entities is essential to ensure software quality during software development. However, the high dimensionality and class distribution imbalance of software defect data seriously affect software defect prediction performance. In order to solve this problem, this paper proposes an Ensemble {MultiBoost} based on {RIPPER} classifier for prediction of imbalanced Software Defect data, called {EMR}\_SD. Firstly, the algorithm uses principal component analysis ({PCA}) method to find out the most effective features from the original features of the data set, so as to achieve the purpose of dimensionality reduction and redundancy removal. Furthermore, the combined sampling method of adaptive synthetic sampling ({ADASYN}) and random sampling without replacement is performed to solve the problem of data class imbalance. This classifier establishes association rules based on attributes and classes, using {MultiBoost} to reduce deviation and variance, so as to achieve the purpose of reducing classification error. The proposed prediction model is evaluated experimentally on the {NASA} {MDP} public datasets and compared with existing similar algorithms. The results show that {EMR}\_SD algorithm is superior to {DNC}, {CEL} and other defect prediction techniques in most evaluation indicators, which proves the effectiveness of the algorithm.},
	pages = {110333--110343},
	journaltitle = {{IEEE} Access},
	author = {He, Haitao and Zhang, Xu and Wang, Qian and Ren, Jiadong and Liu, Jiaxin and Zhao, Xiaolin and Cheng, Yongqiang},
	date = {2019},
	keywords = {Class distributions, class imbalance, Classification algorithms, Classification errors, combined sampling, Computer software selection and evaluation, Defect prediction, Defects, Dimensionality reduction, Evaluation indicators, Feature extraction, Forecasting, High dimensionality, {MultiBoost}, {NASA}, Prediction algorithms, Predictive analytics, Predictive models, Principal component analysis, Redundancy removal, rule learning, Software, Software algorithms, Software defect prediction, Software design, Software entities, Software quality, xyes},
	file = {Full Text:C\:\\Users\\michalm\\Zotero\\storage\\SKE8UCK9\\He et al. - 2019 - Ensemble MultiBoost Based on RIPPER Classifier for.pdf:application/pdf;Full Text:C\:\\Users\\michalm\\Zotero\\storage\\XKMMDBBD\\He et al. - 2019 - Ensemble MultiBoost Based on RIPPER Classifier for.pdf:application/pdf}
}

@inproceedings{gao_empirical_2019,
	title = {Empirical Study: Are Complex Network Features Suitable for Cross-Version Software Defect Prediction?},
	doi = {10.1109/ICSESS47205.2019.9040793},
	abstract = {Software defect prediction can identify possible defective software modules and improve testing efficiency. Traditional software defect prediction mainly focuses on using code features and process-based features for research. The rules of complex network are suitable for software. Using complex network features to represent defect information provides a new idea for software defect prediction. In this paper, we first select 18 versions of 9 open source projects through certain rules and then build a logistic regression model based on three kinds of features (complex network features, traditional code features, merged features) to evaluate the predictive defect ability of complex network features. The results show that: (1) Compared with traditional code features, complex network features have better ability to predict defects for cross-versions software defect prediction; (2) Merged features are not as good as complex network features in defect prediction for cross-version software defect prediction, but still better than traditional code features.},
	pages = {1--5},
	booktitle = {2019 {IEEE} 10th International Conference on Software Engineering and Service Science ({ICSESS})},
	author = {Gao, Houleng and Lu, Minyan and Pan, Cong and Xu, Biao},
	date = {2019-10},
	note = {{ISSN}: 2327-0594},
	keywords = {Feature extraction, Predictive models, Defects, Software defect prediction, Empirical studies, Open source software, Forecasting, Open source projects, Open systems, Software testing, Complex networks, Software modules, Defect prediction, Logistic regression, Logistic Regression modeling, Network features, Testing efficiency, Software systems, Logistics, Software algorithms, complex network features, cross-version software defect prediction, logistic regression, xno}
}

@article{singh_fuzzy_2017,
	title = {Fuzzy Rule-Based Approach for Software Fault Prediction},
	volume = {47},
	issn = {2168-2232},
	doi = {10.1109/TSMC.2016.2521840},
	abstract = {Knowing faulty modules prior to testing makes testing more effective and helps to obtain reliable software. Here, we develop a framework for automatic extraction of human understandable fuzzy rules for software fault detection/classification. This is an integrated framework to simultaneously identify useful determinants (attributes) of faults and fuzzy rules using those attributes. At the beginning of the training, the system assumes every attribute (feature) as a useless feature and then uses a concept of feature attenuating gate to select useful features. The learning process opens the gates or closes them more tightly based on utility of the features. Our system can discard derogatory and indifferent attributes and select the useful ones. It can also exploit subtle nonlinear interaction between attributes. In order to demonstrate the effectiveness of the framework, we have used several publicly available software fault data sets and compared the performance of our method with that of some existing methods. The results using tenfold cross-validation setup show that our system can find useful fuzzy rules for fault prediction.},
	pages = {826--837},
	number = {5},
	journaltitle = {{IEEE} Transactions on Systems, Man, and Cybernetics: Systems},
	author = {Singh, Pradeep and Pal, Nikhil R. and Verma, Shrish and Vyas, Om Prakash},
	date = {2017-05},
	keywords = {Feature extraction, Software, Software metrics, Forecasting, Learning systems, Software testing, Fault detection, Software reliability, Software fault prediction, Computer software, Fuzzy inference, Fuzzy rules, Rule generation, Software metrices, Automatic extraction, Feature modulating gates, Integrated frameworks, Nonlinear interactions, Software fault detection, machine learning, fuzzy rule generation, Logic gates, software fault prediction, software metric selection, xyes, xfuzzy, xcross-project}
}

@inproceedings{anezakis_verification_2018,
	title = {Verification of the effectiveness of fuzzy rule-based fault prediction: A replication study},
	doi = {10.1109/INISTA.2018.8466331},
	abstract = {The prediction success of faulty modules in a software helps practitioners to plan the budget of software maintenance that leads developers to improve the reliability of software systems. Despite various learning algorithms and statistical methods, fault prediction needs novel methods for enhancing the success of the prediction. Fault prediction can be performed using fuzzy rules that are new for this field. In this work, fuzzy rule-based fault prediction approach, which was developed by Singh et al. [11], is replicated to validate the success of fuzzy rule-based fault prediction in open-source data sets. The steps of the experiment and the steps of Singh et al's work, which are applied for replication, both are same. Classification is performed after generating clusters that are constituted using fuzzy rules in normalized data sets. According to the prediction results obtained by applying 10*10 cross-validation, fuzzy rule-based fault prediction produces less errors in open-source data sets when it is compared with industrial data sets. In addition to this, the results validate the findings of Singh et al.'s work in terms of some performance parameters of the fault prediction.},
	pages = {1--8},
	booktitle = {2018 Innovations in Intelligent Systems and Applications ({INISTA})},
	author = {Anezakis, Vardis-Dimitris and Öztürk, Muhammed Maruf},
	date = {2018-07},
	keywords = {Feature extraction, Software metrics, {NASA}, Open source software, Forecasting, Classification (of information), Intelligent systems, Budget control, Software reliability, Fault prediction, Measurement, Learning algorithms, Fuzzy inference, Fuzzy rules, Performance parameters, Fault data, Fuzzy rule based, Industrial datum, Open source datum, Replication study, Prediction algorithms, fault data sets, fault prediction, Fuzzy rule, Modulation, modulator learning, software metrics, xyes}
}

@inproceedings{naufal_software_2015,
	title = {Software complexity metric-based defect classification using {FARM} with preprocessing step {CFS} and {SMOTE} a preliminary study},
	doi = {10.1109/ICITSI.2015.7437685},
	abstract = {One criteria for assessing the software quality is ensuring that there is no defect in the software which is being developed. Software defect classification can be used to prevent software defects. More earlier software defects are detected in the software life cycle, it will minimize the software development costs. This study proposes a software defect classification using Fuzzy Association Rule Mining ({FARM}) based on complexity metrics. However, not all complexity metrics affect on software defect, therefore it requires metrics selection process using Correlation-based Feature Selection ({CFS}) so it can increase the classification performance. This study will conduct experiments on the {NASA} {MDP} open source dataset that is publicly accessible on the {PROMISE} repository. This datasets contain history log of software defects based on software complexity metric. In {NASA} {MDP} dataset the data distribution between defective and not defective modules are not balanced. It is called class imbalanced problem. Class imbalance problem can affect on classification performance. It needs a technique to solve this problem using oversampling method. Synthetic Minority Oversampling Technique ({SMOTE}) is used in this study as oversampling method. With the advantages possessed by {FARM} in learning on dataset which has quantitative data attribute and combined with the software complexity metrics selection process using {CFS} and oversampling using {SMOTE}, this method is expected has a better performance than the previous methods.},
	pages = {1--6},
	booktitle = {2015 International Conference on Information Technology Systems and Innovation ({ICITSI})},
	author = {Naufal, Mohammad Farid and Rochimah, Siti},
	date = {2015-11},
	keywords = {Data mining, Feature extraction, Software, Software metrics, Computer software selection and evaluation, Defects, Software design, {NASA}, Software defects, Open source software, Learning systems, Program debugging, Association rules, Artificial intelligence, Fuzzy rules, Life cycle, Classification performance, Faulting, Bugs, Class imbalance problems, Computational complexity, Correlation based feature selections ({CFS}), Fuzzy association rule, Problem solving, Software development costs, Synthetic minority over-sampling techniques, Fault, Training, Complexity theory, Fuzzy Association Rule Mining, Correlation-based Feature Selection, Defect, Machine Learning, Software Defect Classification, Synthetic Minority Oversampling Technique, xno}
}

@inproceedings{singh_comprehensive_2017,
	title = {Comprehensive model for software fault prediction},
	doi = {10.1109/ICICI.2017.8365311},
	abstract = {Software Fault prediction ({SFP}) is an important task in the fields of software engineering to develop a cost effective software. Most of the software fault prediction is performed on same project date i.e., training and testing with same projects fault data. In case of unavailability of fault training data which is possible for the new project, data from the similar types/category of other projects can be used to train the model for the prediction. The software projects has been categorized into three categories by Boehm. The project within a certain group will be having good similarities with other projects within the group. So it is more suitable to train using the projects from same group. In this work we proposed to develop a model with similar category of data to predict the fault of another project belongs to same category. On basis of {KLOC} we have taken five organic software projects and performed various cross project and within project experiments. To generate a comprehensive generalized model for organic software's fault prediction, we have modeled various rule based to learner. Various rule-based learners used for comparison are {JRip}, {CART}, Conjunctive Rule, C4.5, {NNge}, {OneR}, Ridor, {PART}, and decision table-Naive Bayes hybrid classifier ({DTNB}).},
	pages = {1103--1108},
	booktitle = {2017 International Conference on Inventive Computing and Informatics ({ICICI})},
	author = {Singh, Pradeep},
	date = {2017-11},
	keywords = {Predictive models, Software, Testing, Cost engineering, Forecasting, Software testing, Training and testing, Training data, Fault prediction, Comprehensive model, Software fault prediction, Rule based, Cost effectiveness, Decision tables, Generalized models, Hybrid classifier, Three categories, Training, Computational modeling, Data models, Rule based Learner, xyes, xcross-project},
	file = {Singh - 2017 - Comprehensive model for software fault prediction.pdf:C\:\\Users\\michalm\\Zotero\\storage\\5EF5YECT\\Singh - 2017 - Comprehensive model for software fault prediction.pdf:application/pdf}
}

@inproceedings{mutlu_automatic_2018,
	title = {Automatic Rule Generation of Fuzzy Systems: A Comparative Assessment on Software Defect Prediction},
	doi = {10.1109/UBMK.2018.8566479},
	abstract = {Fuzzy rule base systems are expert systems rely on fuzzy set theory. Here the knowledge of human expert is transfered to the artificial model via fuzzy rules. Therefore, preciseness, completeness and coverage of fuzzy rules in a fuzzy system is vital for the accuracy and plausibility of fuzzy reasoning. However, in such cases where the human expert is unable to supply the rules sufficiently, data-based automatic rule generation methods attract attention. In this study, 2 linear and 2 evolutionary approaches of automatic fuzzy rule generation methods are investigated. The investigated linear solutions contain Wang-Mendel Method and E2E-{HFS}, while {MOGUL} and {IVTURS}-{FARC} are the selected evolutionary approaches. Wang-Mendel and {MOGUL} is commonly considered as basic methods of the group they belong to. {IVTURS}-{FARC} is distinguished with its ability to handle interval valued fuzzy sets. Among the rest of the algorithms, E2E-{HFS} is unique with its weak dependency to data. Because it only use some simple properties of corresponding input variable. In order to compare the completeness and the accuracy of automatically generated fuzzy rules, several experiments are performed on different software defect prediction datasets, and the classification performance of resulting fuzzy systems is evaluated. Provided results show that even if training of evolutionary approaches seem to be more precise, similar accuracy can be achieved by linear approaches, and they perform better regarding the experiments on unseen data.},
	pages = {209--214},
	booktitle = {2018 3rd International Conference on Computer Science and Engineering ({UBMK})},
	author = {Mutlu, Begum and Sezer, Ebru A. and Akcayol, M. Ali},
	date = {2018-09},
	keywords = {Software, Automatically generated, Defects, Software defect prediction, Forecasting, Classification (of information), Expert systems, Computer software, Fuzzy inference, Fuzzy rules, Fuzzy systems, Rule generation, Fuzzy logic, Fuzzy sets, Classification performance, Comparative assessment, Evolutionary rules, Fuzzy inference systems, Fuzzy set theory, Interval-valued fuzzy sets, Linguistics, Software Defect Prediction, Evolutionary Rule Learning, Fuzzy Inference Systems, Fuzzy Rule Generation, Genetics, xyes, xfuzzy},
	file = {Mutlu et al. - 2018 - Automatic Rule Generation of Fuzzy Systems A Comp.pdf:C\:\\Users\\michalm\\Zotero\\storage\\BCXGVN26\\Mutlu et al. - 2018 - Automatic Rule Generation of Fuzzy Systems A Comp.pdf:application/pdf}
}

@inproceedings{rosli_design_2011,
	title = {The design of a software fault prone application using evolutionary algorithm},
	doi = {10.1109/ICOS.2011.6079246},
	abstract = {Most of the current project management software's are utilizing resources on developing areas in software projects. This is considerably essential in view of the meaningful impact towards time and cost-effective development. One of the major areas is the fault proneness prediction, which is used to find out the impact areas by using several approaches, techniques and applications. Software fault proneness application is an application based on computer aided approach to predict the probability that the software contains faults. The application will uses object oriented metrics and count metrics values from open source software as input values to the genetic algorithm for generation of the rules to classify the software modules in the categories of Faulty and Non Faulty modules. At the end of the process, the result will be visualized using genetic algorithm applet, bar and pie chart. This paper will discussed the detail design of software fault proneness application by using genetic algorithm based on the object oriented approach and will be presented using the Unified Modeling Language ({UML}). The aim of the proposed design is to develop an automated tool for software development group to discover the most likely software modules to be high problematic in the future.},
	pages = {338--343},
	booktitle = {2011 {IEEE} Conference on Open Systems},
	author = {Rosli, Marshima Mohd and Teo, Noor Hasimah Ibrahim and Yusop, Nor Shahida Mohamad and Mohammad, Noor Shahriman},
	date = {2011-09},
	keywords = {Software, Software design, Open source software, Forecasting, Genetic algorithms, Object oriented programming, Open systems, Software testing, Project management, Software fault, Fault prediction, Fault proneness, Measurement, Computer bugs, Application programs, Cost effectiveness, Genetic programming, Fault-proneness prediction, Computer aided-approach, Object oriented approach, Object oriented metrics, Software fault proneness, Unified Modeling Language, fault prediction, Object oriented modeling, fault proneness, software fault prone, software testing, Unified modeling language, xno}
}

@inproceedings{kaur_evaluation_2017,
	title = {Evaluation of imbalanced learning with entropy of source code metrics as defect predictors},
	doi = {10.1109/ICTUS.2017.8286041},
	abstract = {This paper evaluates imbalanced learning algorithms with entropy of source code metrics as predictor variables. Four open source software systems are studied. These systems are {ECLIPSE} {JDT}, {EQUINOX}, {MYLYN} and {ECLIPSE} {PDE} {UI}. The results of this paper indicate that imbalanced learning algorithms perform better than classical learning methods in terms of Recall, G-mean 1, G-mean 2 and F-measures. For recall measure Condensed Nearest Neighbor rule +Tomek links ({CNNTL}) perform best, for G-mean 1, G-mean 2 and F-measure Random undersampling ({RUS}) perform best.},
	pages = {403--409},
	booktitle = {2017 International Conference on Infocom Technologies and Unmanned Systems (Trends and Future Directions) ({ICTUS})},
	author = {Kaur, Kamaldeep and Name, Jasmeet Kaur and Malhotra, Jyotsana},
	date = {2017-12},
	keywords = {Defects, Open source software, Learning systems, Open systems, Class imbalance learning, Codes (symbols), Measurement, Defect prediction, Imbalanced Learning, Learning algorithms, Software systems, Computer programming languages, Condensed nearest neighbor rule, Entropy, Open source software systems, Predictor variables, Random under samplings, Source code metrics, Prediction algorithms, Software algorithms, defect prediction, class imbalance learning, entropy of source code metrics, xno}
}

@inproceedings{alhazzaa_trade-offs_2019,
	title = {Trade-Offs between Early Software Defect Prediction versus Prediction Accuracy},
	doi = {10.1109/CSCI49370.2019.00216},
	abstract = {In any software development organization, reliability is crucial. Defect prediction is key in providing management with the tools for release planning. To predict defects we ask the question of how much data is required to make usable predictions? When testing, a rule of thumb is to start defect prediction after 60\% of system test has been accomplished. In an operational phase, managers cannot usually determine what constitutes 60\% of a release and might not want to wait that long to start defect prediction. Here we discuss the trade-offs between the need of early predictions versus making more accurate predictions.},
	pages = {1144--1150},
	booktitle = {2019 International Conference on Computational Science and Computational Intelligence ({CSCI})},
	author = {Alhazzaa, Lamees and Amschler Andrews, Anneliese},
	date = {2019-12},
	keywords = {Predictive models, Software, Software design, Software defect prediction, Forecasting, Economic and social effects, Prediction accuracy, Software reliability, Artificial intelligence, Accurate prediction, Commerce, Defect prediction, Early prediction, Operational phase, Release planning, Software development organizations, Estimation, Data models, change-point, defects, estimation, Mathematical model, prediction, software reliability, xno}
}

@article{riaz_rough_2018,
	title = {Rough Noise-Filtered Easy Ensemble for Software Fault Prediction},
	volume = {6},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2018.2865383},
	abstract = {Software fault prediction is the very important research topic for software quality assurance. Data-driven approaches provide robust mechanisms to deal with software fault prediction. However, the prediction performance of the model highly depends on the quality of the data set. Many software data sets suffer from the problem of class imbalance. In this regard, undersampling is a popular data pre-processing method in dealing with the class imbalance problem; easy ensemble presents a robust approach to achieve a high classification rate and address the biases toward majority class samples. However, imbalance class is not the only issue that harms the performance of classifiers. Some noisy data and irrelevant and redundant features may also reduce the performance of predictive accuracy of the classifier. In this paper, we propose two-stage data pre-processing, which incorporates feature selection and rough set-based K nearest neighbour rule ({KNN}) noise filter afore executing easy ensemble rough-{KNN} noise-filtered easy ensemble ({RKEE}). In the first stage, we eliminate the irrelevant and redundant features by the feature ranking algorithm, and in the second stage, we handle the imbalance class problem by using rough-{KNN} noise filter to eliminate noisy samples from both the minority and the majority class and also handle the uncertainty and the overlapping problem from both the minority and the majority class. Experimental evaluation on real-world software projects, such as {NASA} and Eclipse data set, is performed in order to demonstrate the effectiveness of our proposed approach. Furthermore, this paper comprehensively investigates the influencing factor in our approach, such as the impact of the rough set theory on noise-filter, the relationship between model performance and imbalance ratio, and so on. Comprehensive experiments indicate that the proposed approach shows outstanding performance with significance in terms of area-under-the-curve.},
	pages = {46886--46899},
	journaltitle = {{IEEE} Access},
	author = {Riaz, Saman and Arshad, Ali and Jiao, Licheng},
	date = {2018},
	keywords = {Data mining, Feature extraction, feature selection, Software, Computer software selection and evaluation, {NASA}, Forecasting, Classification (of information), Quality assurance, Class imbalance, Data handling, Software fault prediction, Computer software, Classification algorithm, Data preprocessing, Easy Ensemble, Filtering theory, Noise filters, Noise measurements, Personnel training, Processing, Rough set theory, Classification algorithms, Training, class imbalance, data preprocessing, easy ensemble, noise filter, Noise measurement, rough set theory, Rough sets, xno}
}

@inproceedings{li_mining_2010,
	title = {Mining Frequent Patterns from Software Defect Repositories for Black-Box Testing},
	doi = {10.1109/IWISA.2010.5473578},
	abstract = {Software defects are usually detected by inspection, black-box testing or white-box testing. Current software defect mining work focuses on mining frequent patterns without distinguishing these different kinds of defects, and mining with respect to defect type can only give limited guidance on software development due to overly broad classification of defect type. In this paper, we present four kinds of frequent patterns from defects detected by black-box testing (called black-box defect) based on a kind of detailed classification named {ODC}-{BD} (Orthogonal Defect Classification for Blackbox Defect). The frequent patterns include the top 10 conditions (data or operation) which most easily result in defects or severe defects, the top 10 defect phenomena which most frequently occur and have a great impact on users, association rules between function modules and defect types. We aim to help project managers, black-box testers and developers improve the efficiency of software defect detection and analysis using these frequent patterns. Our study is based on 5023 defect reports from 56 large industrial projects and 2 open source projects.},
	pages = {1--4},
	booktitle = {2010 2nd International Workshop on Intelligent Systems and Applications},
	author = {Li, Ning and Li, Zhanhuai and Zhang, Lijun},
	date = {2010-05},
	keywords = {Data mining, Defects, Software design, Software quality, Software defects, Open source software, Intelligent systems, Association rules, Software testing, Project management, Computer software, Black-box testing, Defect classification, Inspection, Associative processing, Frequent Itemsets, Software defect, Itemsets, Programming, Pattern analysis, Software development management, xyes, xassociation-rules}
}

@inproceedings{jin_software_2010,
	title = {Software Fault Prediction Model Based on Adaptive Dynamical and Median Particle Swarm Optimization},
	volume = {1},
	doi = {10.1109/MMIT.2010.11},
	abstract = {Software quality prediction can play a role of importance in software management, and thus in improve the quality of software systems. By mining software with data mining technique, predictive models can be induced that software managers the insights they need to tackle these quality problems in an efficient way. This paper deals with the adaptive dynamic and median particle swarm optimization ({ADMPSO}) based on the {PSO} classification technique. {ADMPSO} can act as a valid data mining technique to predict erroneous software modules. The predictive model in this paper extracts the relationship rules of software quality and metrics. Information entropy approach is applied to simplify the extraction rule set. The empirical result shows that this method set of rules can be streamlined and the forecast accuracy can be improved.},
	pages = {44--47},
	booktitle = {2010 Second International Conference on Multimedia and Information Technology},
	author = {Jin, Cong and Dong, En-Mei and Qin, Li-Na},
	date = {2010-04},
	keywords = {Data mining, Predictive models, Computer software selection and evaluation, Software quality, Forecasting, Classification (of information), Software Quality, Fault prediction, Particle swarm optimization ({PSO}), Software fault prediction, Software modules, Software systems, Set of rules, Classification technique, Quality of softwares, Mathematical models, Predictive control systems, Empirical results, Adaptive dynamics, Classification, Data mining techniques, Extraction rule, Forecast accuracy, Information entropy, Information technology, Mining software, Quality problems, Relationship rules, Software management, Software managers, Software quality prediction, Computer science, Conference management, Multimedia systems, Particle swarm optimization, Quality management, xyes}
}

@inproceedings{diamantopoulos_towards_2015,
	title = {Towards Interpretable Defect-Prone Component Analysis Using Genetic Fuzzy Systems},
	doi = {10.1109/RAISE.2015.13},
	abstract = {The problem of Software Reliability Prediction is attracting the attention of several researchers during the last few years. Various classification techniques are proposed in current literature which involve the use of metrics drawn from version control systems in order to classify software components as defect-prone or defect-free. In this paper, we create a novel genetic fuzzy rule-based system to efficiently model the defect-proneness of each component. The system uses a Mamdani-Assilian inference engine and models the problem as a one-class classification task. System rules are constructed using a genetic algorithm, where each chromosome represents a rule base (Pittsburgh approach). The parameters of our fuzzy system and the operators of the genetic algorithm are designed with regard to producing interpretable output. Thus, the output offers not only effective classification, but also a comprehensive set of rules that can be easily visualized to extract useful conclusions about the metrics of the software.},
	pages = {32--38},
	booktitle = {2015 {IEEE}/{ACM} 4th International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering},
	author = {Diamantopoulos, Themistoklis and Symeonidis, Andreas},
	date = {2015-05},
	keywords = {Software, Defects, Forecasting, Genetic algorithms, Software engineering, Software component, Software reliability, Artificial intelligence, Software fault prediction, Measurement, Fuzzy inference, Fuzzy systems, Fuzzy logic, Classification technique, Component analysis, Genetic fuzzy systems, One-class Classification, Pittsburgh approach, Version control system, software fault prediction, Genetics, defect-prone components, genetic fuzzy systems, Sociology, Software Reliability Prediction, xyes}
}

@article{cai_design_2019,
	title = {Design Rule Spaces: A New Model for Representing and Analyzing Software Architecture},
	volume = {45},
	issn = {1939-3520},
	doi = {10.1109/TSE.2018.2797899},
	abstract = {In this paper, we propose an architecture model called Design Rule Space ({DRSpace}). We model the architecture of a software system as multiple overlapping {DRSpaces}, reflecting the fact that any complex software system must contain multiple aspects, features, patterns, etc. We show that this model provides new ways to analyze software quality. In particular, we introduce an Architecture Root detection algorithm that captures {DRSpaces} containing large numbers of a project's bug-prone files, which are called Architecture Roots ({ArchRoots}). After investigating {ArchRoots} calculated from 15 open source projects, the following observations become clear: from 35 to 91 percent of a project's most bug-prone files can be captured by just 5 {ArchRoots}, meaning that bug-prone files are likely to be architecturally connected. Furthermore, these {ArchRoots} tend to live in the system for significant periods of time, serving as the major source of bug-proneness and high maintainability costs. Moreover, each {ArchRoot} reveals multiple architectural flaws that propagate bugs among files and this will incur high maintenance costs over time. The implication of our study is that the quality, in terms of bug-proneness, of a large, complex software project cannot be fundamentally improved without first fixing its architectural flaws.},
	pages = {657--682},
	number = {7},
	journaltitle = {{IEEE} Transactions on Software Engineering},
	author = {Cai, Yuanfang and Xiao, Lu and Kazman, Rick and Mo, Ran and Feng, Qiong},
	date = {2019-07},
	keywords = {Computer software selection and evaluation, Open source software, Program debugging, Reverse engineering, Code smell, Computer architecture, Technical debts, Defect prediction, Analytical models, Bug localizations, Computer bugs, Production facility, Software architecture, Software systems, defect prediction, bug localization, code smells, Production facilities, reverse-engineering, technical debt, xyes}
}

@inproceedings{watanabe_identifying_2016,
	title = {Identifying recurring association rules in software defect prediction},
	doi = {10.1109/ICIS.2016.7550867},
	abstract = {Association rule mining discovers patterns of co-occurrences of attributes as association rules in a data set. The derived association rules are expected to be recurrent, that is, the patterns recur in future in other data sets. This paper defines the recurrence of a rule, and aims to find a criteria to distinguish between high recurrent rules and low recurrent ones using a data set for software defect prediction. An experiment with the Eclipse Mylyn defect data set showed that rules of lower than 30 transactions showed low recurrence. We also found that the lower bound of transactions to select high recurrence rules is dependent on the required precision of defect prediction.},
	pages = {1--6},
	booktitle = {2016 {IEEE}/{ACIS} 15th International Conference on Computer and Information Science ({ICIS})},
	author = {Watanabe, Takashi and Monden, Akito and Kamei, Yasutaka and Morisaki, Shuji},
	date = {2016-06},
	keywords = {association rule mining, Data mining, Computer software selection and evaluation, Defects, Software quality, Software defect prediction, Empirical studies, Forecasting, Association rules, Software Quality, Measurement, Defect prediction, Computer software, Data set, Co-occurrence, Information science, Lower bounds, Required precision, data mining, defect prediction, empirical study, software quality, Decision support systems, xno}
}

@inproceedings{fan_high-frequency_2018,
	title = {High-Frequency Keywords to Predict Defects for Android Applications},
	volume = {02},
	doi = {10.1109/COMPSAC.2018.10273},
	abstract = {Android defect prediction has proved to be useful to reduce the manual testing effort for finding bugs. In recent years, researchers design metrics related to defects and analyze historical information to predict whether files contain defects using machine learning. However, those models learn to predict defects based on the characteristics of programs while ignoring the internal information, e.g., the functional and semantic information within the source code. This paper proposes a model, {HIRER}, to learn the functional and semantic information to predict whether files contain defects automatically for Android applications. Specifically, {HIRER} learns internal information within the source code based on the high-frequency keywords extracted from programs' Abstract Syntax Trees ({ASTs}). It gets rule-based programming patterns from high-frequency keywords and uses Deep Belief Network ({DBN}), a deep neutral network, to learn functional and semantic features from the programming patterns. We implement a defect testing system with five machine learning techniques based on {HIRER} to predict defective files in source code automatically. Then, we apply it on four open source Android applications. The results show that learned functional and semantic features can predict more defects than traditional metrics. In different versions of {MMS}, Gallery2, Bluetooth, Calendar open source applications, {HIRER} improves the {AUC} of the predicted results respectively in average.},
	pages = {442--447},
	booktitle = {2018 {IEEE} 42nd Annual Computer Software and Applications Conference ({COMPSAC})},
	author = {Fan, Yaqing and Cao, Xinya and Xu, Jing and Xu, Sihan and Yang, Hongji},
	date = {2018-07},
	note = {{ISSN}: 0730-3157},
	keywords = {Feature extraction, Predictive models, Defects, Open source software, Forecasting, Machine learning techniques, Semantics, Trees (mathematics), Codes (symbols), Artificial intelligence, Measurement, Defect prediction, Application programs, Abstract Syntax Trees, Android, Android (operating system), Computer programming languages, Deep belief network ({DBN}), Deep learning, Functional programming, High frequency {HF}, Historical information, Open source application, defect prediction, Programming, Androids, high-frequency, Humanoid robots, xno}
}

@inproceedings{karre_defect_2015,
	title = {A defect dependency based approach to improve software quality in integrated software products},
	abstract = {Integrated software products are complex in design. They are prone to defects caused by integrated and non-integrated modules of the entire integrated software suite. In such software products, a small proportion of defects are fixed as soon as they are reported. Rest of the defects are targeted for fixes in future product release cycles. Among such targeted defects, most of them seem to be insignificant and innocuous in the current version but have the potential to become acute in future versions. In this paper, we propose an approach to study defect dependency of the reported defect using a dependency metric. Identifying the dependency of a defect in an integrated product suite can help the product stake-owners to prioritize them and help improve software quality.},
	pages = {110--117},
	booktitle = {2015 International Conference on Evaluation of Novel Approaches to Software Engineering ({ENASE})},
	author = {Karre, Sai Anirudh and Reddy, Y. Raghu},
	date = {2015-04},
	keywords = {Data mining, Software, Testing, Computer software selection and evaluation, Defects, Software products, Classification (of information), Software Quality, Measurement, Product design, Dependency metric, Integrated module, Integrated products, Integrated software, Integrated software suite, Rule-based classification, Training, Defect Dataset, Defect Dependency, Dependency Metric, Influenza, Integrated Software Products, Rule-based Classification, xno}
}

@inproceedings{ibarguren_consolidated_2017,
	title = {The Consolidated Tree Construction algorithm in imbalanced defect prediction datasets},
	doi = {10.1109/CEC.2017.7969629},
	abstract = {In this short paper, we compare well-known rule/tree classifiers in software defect prediction with the {CTC} decision tree classifier designed to deal with class imbalanced. It is well-known that most software defect prediction datasets are highly imbalance (non-defective instances outnumber defective ones). In this work, we focused only on tree/rule classifiers as these are capable of explaining the decision, i.e., describing the metrics and thresholds that make a module error prone. Furthermore, rules/decision trees provide the advantage that they are easily understood and applied by project managers and quality assurance personnel. The {CTC} algorithm was designed to cope with class imbalance and noisy datasets instead of using preprocessing techniques (oversampling or undersampling), ensembles or cost weights of misclassification. The experimental work was carried out using the {NASA} datasets and results showed that induced {CTC} decision trees performed better or similar to the rest of the rule/tree classifiers.},
	pages = {2656--2660},
	booktitle = {2017 {IEEE} Congress on Evolutionary Computation ({CEC})},
	author = {Ibarguren, Igor and Pérez, Jesús M. and Mugerza, Javier and Rodriguez, Daniel and Harrison, Rachel},
	date = {2017-06},
	keywords = {Software, {NASA}, Decision trees, Measurement, Algorithm design and analysis, Prediction algorithms, Software algorithms, xyes},
	file = {Submitted Version:C\:\\Users\\michalm\\Zotero\\storage\\KMXS79CF\\Ibarguren et al. - 2017 - The Consolidated Tree Construction algorithm in im.pdf:application/pdf}
}

@inproceedings{sun_improving_2010,
	title = {Improving the Precision of Dependence-Based Defect Mining by Supervised Learning of Rule and Violation Graphs},
	doi = {10.1109/ISSRE.2010.37},
	abstract = {Previous work has shown that application of graph mining techniques to system dependence graphs improves the precision of automatic defect discovery by revealing subgraphs corresponding to implicit programming rules and to rule violations. However, developers must still confirm, edit, or discard reported rules and violations, which is both costly and error-prone. In order to reduce developer effort and further improve precision, we investigate the use of supervised learning models for classifying and ranking rule and violation subgraphs. In particular, we present and evaluate logistic regression models for rules and violations, respectively, which are based on general dependence-graph features. Our empirical results indicate that (i) use of these models can significantly improve the precision and recall of defect discovery, and (ii) our approach is superior to existing heuristic approaches to rule and violation ranking and to an existing static-warning classifier, and (iii) accurate models can be learned using only a few labeled examples.},
	pages = {1--10},
	booktitle = {2010 {IEEE} 21st International Symposium on Software Reliability Engineering},
	author = {Sun, Boya and Podgurski, Andy and Ray, Soumya},
	date = {2010-11},
	note = {{ISSN}: 2332-6549},
	keywords = {Data mining, Computer software selection and evaluation, Defects, Precision and recall, Quality assurance, Heuristic methods, Software reliability, Measurement, Logistic regression, Computer bugs, Regression analysis, Supervised learning, Defect classification, Rule violation, Logistic regression models, Logistics, Error prones, Computer systems programming, Defect discovery, Dependence graphs, Empirical results, Graph features, Graph mining, Heuristic approach, Ranking rules, Subgraphs, System dependence graph, Classification algorithms, Programming, logistic regression, Computational modeling, defect classification, defect mining, dependence graph, xno}
}

@inproceedings{anwar_using_2012,
	title = {Using Association Rules to Identify Similarities between Software Datasets},
	doi = {10.1109/QUATIC.2012.66},
	abstract = {A number of V\&V datasets are publicly available. These datasets have software measurements and defectiveness information regarding the software modules. To facilitate V\&V, numerous defect prediction studies have used these datasets and have detected defective modules effectively. Software developers and managers can benefit from the existing studies to avoid analogous defects and mistakes if they are able to find similarity between their software and the software represented by the public datasets. This paper identifies the similar datasets by comparing association patterns in the datasets. The proposed approach finds association rules from each dataset and identifies the overlapping rules from the 100 strongest rules from each of the two datasets being compared. Afterwards, average support and average confidence of the overlap is calculated to determine the strength of the similarity between the datasets. This study compares eight public datasets and results show that {KC}2 and {PC}2 have the highest similarity 83\% with 97\% support and 100\% confidence. Datasets with similar attributes and almost same number of attributes have shown higher similarity than the other datasets.},
	pages = {114--119},
	booktitle = {2012 Eighth International Conference on the Quality of Information and Communications Technology},
	author = {Anwar, Saba and Rana, Zeeshan Ali and Shamail, Shafay and Awais, Mian M.},
	date = {2012-09},
	keywords = {Defects, Software engineering, Association rules, Software developer, Software modules, Defect prediction, Association patterns, dataset similarity, Software Measurement, Software measures, defect prediction, association rules, software measures, xno}
}

@inproceedings{lopes_margarido_classification_2011,
	title = {Classification of defect types in requirements specifications: Literature review, proposal and assessment},
	abstract = {Requirements defects have a major impact throughout the whole software lifecycle. Having a specific defects classification for requirements is important to analyse the root causes of problems, build checklists that support requirements reviews and to reduce risks associated with requirements problems. In our research we analyse several defects classifiers; select the ones applicable to requirements specifications, following rules to build defects taxonomies; and assess the classification validity in an experiment of requirements defects classification performed by graduate and undergraduate students. Not all subjects used the same type of defect to classify the same defect, which suggests that defects classification is not consensual. Considering our results we give recommendations to industry and other researchers on the design of classification schemes and treatment of classification results.},
	pages = {1--6},
	booktitle = {6th Iberian Conference on Information Systems and Technologies ({CISTI} 2011)},
	author = {Lopes Margarido, Isabel and Faria, João Pascoal and Vidal, Raul Moreira and Vieira, Marco},
	date = {2011-06},
	note = {{ISSN}: 2166-0735},
	keywords = {Software, Testing, Defects, Software engineering, Software life cycles, software, Specifications, Taxonomies, Inspection, Classification of defects, Classification results, Classification scheme, Defects classification, Information systems, Literature reviews, requirements, Requirements specifications, Root cause, Students, Support requirements, Undergraduate students, Programming, defects, classification, Documentation, taxonomy, Taxonomy, xno}
}

@article{xiaolong_rfc_2021,
	title = {{RFC}: A feature selection algorithm for software defect prediction},
	volume = {32},
	issn = {1004-4132},
	doi = {10.23919/JSEE.2021.000032},
	abstract = {Software defect prediction ({SDP}) is used to perform the statistical analysis of historical defect data to find out the distribution rule of historical defects, so as to effectively predictdefects in the new software. However, there are redundant and irrelevant features in the software defect datasets affecting the performance of defect predictors. In order to identify and remove the redundant and irrelevant features in software defectdatasets, we propose Relief F-based clustering ({RFC}), a cluster-based feature selection algorithm. Then, the correlation between features is calculated based on the symmetric uncertainty. According to the correlation degree, {RFC} partitions features into kclusters based on the k-medoids algorithm, and finally selects the representative features from each cluster to form the final feature subset. In the experiments, we compare the proposed {RFC} with classical feature selection algorithms on nine National Aeronautics and Space Administration ({NASA}) software defectprediction datasets in terms of area under curve ({AUC}) and F-value. The experimental results show that {RFC} can effectively improve the performance of {SDP}.},
	pages = {389--398},
	number = {2},
	journaltitle = {Journal of Systems Engineering and Electronics},
	author = {Xiaolong, Xu and Wen, Chen and Xinheng, Wang},
	date = {2021-04},
	keywords = {Clustering algorithms, Feature extraction, feature selection, Software, Defects, Based clustering, Correlation between features, Correlation degree, Distribution rule, Feature selection algorithm, K-medoids algorithms, {NASA}, Software defect prediction, Software defects, Systems engineering, Prediction algorithms, Software algorithms, cluster, Correlation, Partitioning algorithms, software defect prediction ({SDP}), xno}
}

@inproceedings{gainaru_taming_2012,
	title = {Taming of the Shrew: Modeling the Normal and Faulty Behaviour of Large-scale {HPC} Systems},
	doi = {10.1109/IPDPS.2012.107},
	abstract = {{HPC} systems are complex machines that generate a huge volume of system state data called "events". Events are generated without following a general consistent rule and different hardware and software components of such systems have different failure rates. Distinguishing between normal system behaviour and faulty situation relies on event analysis. Being able to detect quickly deviations from normality is essential for system administration and is the foundation of fault prediction. As {HPC} systems continue to grow in size and complexity, mining event flows become more challenging and with the upcoming 10 Pet flop systems, there is a lot of interest in this topic. Current event mining approaches do not take into consideration the specific behaviour of each type of events and as a consequence, fail to analyze them according to their characteristics. In this paper we propose a novel way of characterizing the normal and faulty behaviour of the system by using signal analysis concepts. All analysis modules create {ELSA} (Event Log Signal Analyzer), a toolkit that has the purpose of modelling the normal flow of each state event during a {HPC} system lifetime, and how it is affected when a failure hits the system. We show that these extracted models provide an accurate view of the system output, which improves the effectiveness of proactive fault tolerance algorithms. Specifically, we implemented a filtering algorithm and short-term fault prediction methodology based on the extracted model and test it against real failure traces from a large-scale system. We show that by analyzing each event according to its specific behaviour, we get a more realistic overview of the entire system.},
	pages = {1168--1179},
	booktitle = {2012 {IEEE} 26th International Parallel and Distributed Processing Symposium},
	author = {Gainaru, Ana and Cappello, Franck and Kramer, William},
	date = {2012-05},
	note = {{ISSN}: 1530-2075},
	keywords = {Data mining, Predictive models, Fault detection, Fault prediction, Fault tolerance, Analytical models, Complex machines, Distributed parameter networks, Entire system, Event analysis, Event mining, Failure rate, Filtering algorithm, Hardware and software components, Normal flow, Proactive fault, Signal analysis, Signal analyzers, System administration, System output, System state, Prediction algorithms, Correlation, fault detection, fault tolerance, large-scale {HPC} systems, Large-scale systems, signal analysis, xno}
}

@inproceedings{chen_inspection_2013,
	title = {Inspection flow of yield impacting systematic defects},
	doi = {10.1109/eMDC.2013.6756065},
	abstract = {Yield impacting systematic defects finding is no longer just relied on Design Rule Checking ({DRC}) provided by designer or Lithography Rule Checking ({LRC}) provided by post-optical proximity correction ({OPC}) results. An inspection flow is proposed in this paper, which is combining the inspection {KLA} tool and Hotspot Pattern Analyzer ({HPA}) database software to do the systematic defects filtering, sorting, grouping, and classification on the data base after hot scan inspection. 2nd time high sensitive inspection is done with new care area, which is reduced into one ten-thousandth of original inspection area. Following this inspection flow, we can identify the process window more accuracy.},
	pages = {1--3},
	booktitle = {2013 e-Manufacturing Design Collaboration Symposium ({eMDC})},
	author = {Chen, Chimin and Yang, {ChengHua} and Liao, Hsiang-Chou and Luoh, Tuung and Yang, Ling-Wu and Yang, Tahone and Chen, Kuang-Chao and Lu, Chih-Yuan and Liu, Donghua and Fan, Jeff and Lv, Rong},
	date = {2013-09},
	keywords = {Defects, Integrated circuit layout, Manufacture, Database software, Design rule checking, Finite element method, Hot Scan, Inspection, Inspection flow, Pattern Grouping, Photolithography, Proximity correction, {PWQ}, Systematic defects, {FEM}, Finite element analysis, Joints, Lead, Systematic Defect, Systematics, xno}
}

@inproceedings{gowda_false_2018,
	title = {False Positive Analysis of Software Vulnerabilities Using Machine Learning},
	doi = {10.1109/CCEM.2018.00010},
	abstract = {Dynamic Application Security Testing is conducted with the help of automated tools that have built-in scanners which automatically crawl all the webpages of the application and report security vulnerabilities based on certain set of pre-defined scan rules. Such pre-defined rules cannot fully determine the accuracy of a vulnerability and very often one needs to manually validate these results to remove the false positives. Eliminating false positives from such results can be a quite painful and laborious task. This article proposes an approach of eliminating false positives by using machine learning . Based on the historic data available on false positives, suitable machine learning models are deployed to predict if the reported defect is a real vulnerability or a false positive},
	pages = {3--6},
	booktitle = {2018 {IEEE} International Conference on Cloud Computing in Emerging Markets ({CCEM})},
	author = {Gowda, Sumanth and Prajapati, Divyesh and Singh, Ranjit and Gadre, Swanand S.},
	date = {2018-11},
	keywords = {Predictive models, Software, Testing, Learning systems, Decision trees, Machine learning, Machine learning models, Software vulnerabilities, Commerce, Automated tools, Cloud computing, Dynamic applications, False positive, Security of data, Security vulnerabilities, Software security, vulnerabilities, Prediction algorithms, Machine Learning, Data models, False Positive Analysis, Software Security, xno}
}

@inproceedings{elberzhager_optimizing_2014,
	title = {Optimizing Quality Assurance Strategies through an Integrated Quality Assurance Approach – Guiding Quality Assurance with Assumptions and Selection Rules},
	doi = {10.1109/SEAA.2014.12},
	abstract = {Quality assurance activities are often still expensive or do not offer the expected quality. A recent trend aimed at overcoming this problem is tighter integration of several quality assurance techniques such as analysis and testing in order to exploit synergy effects and thus reduce costs or improve the coverage of quality assurance activities. However, one main challenge in exploiting such benefits is that knowledge about the relationships between many different factors is needed, such as the quality assurance techniques considered, the number of defects, the remaining defect-proneness, or product and budget data. Such knowledge is often not available. Based on a combined analysis and testing methodology called In {QA}, we developed an iterative rule-based procedure that considers several factors in order to gather knowledge and allows deriving different strategies to guide the quality assurance activities. We derived several specific and reasonable strategies to demonstrate the approach.},
	pages = {402--405},
	booktitle = {2014 40th {EUROMICRO} Conference on Software Engineering and Advanced Applications},
	author = {Elberzhager, Frank and Bauer, Thomas},
	date = {2014-08},
	note = {{ISSN}: 2376-9505},
	keywords = {Testing, Defects, Forecasting, Quality assurance, Iterative methods, Software engineering, Budget control, Application programs, Knowledge based systems, Defect proneness, Integration testing, Analysis and testing, Quality control, Integration, analysis, assumptions, Combined analysis, Electronic guidance systems, Integrated quality, Quality assurance strategies, rules, Inspection, prediction, Context, Calibration, Concrete, guidance, integration, testing, tool prototype, xyes}
}

@inproceedings{herzig_empirically_2015,
	title = {Empirically Detecting False Test Alarms Using Association Rules},
	volume = {2},
	doi = {10.1109/ICSE.2015.133},
	abstract = {Applying code changes to software systems and testing these code changes can be a complex task that involves many different types of software testing strategies, e.g. system and integration tests. However, not all test failures reported during code integration are hinting towards code defects. Testing large systems such as the Microsoft Windows operating system requires complex test infrastructures, which may lead to test failures caused by faulty tests and test infrastructure issues. Such false test alarms are particular annoying as they raise engineer attention and require manual inspection without providing any benefit. The goal of this work is to use empirical data to minimize the number of false test alarms reported during system and integration testing. To achieve this goal, we use association rule learning to identify patterns among failing test steps that are typically for false test alarms and can be used to automatically classify them. A successful classification of false test alarms is particularly valuable for product teams as manual test failure inspection is an expensive and time-consuming process that not only costs engineering time and money but also slows down product development. We evaluating our approach on system and integration tests executed during Windows 8.1 and Microsoft Dynamics {AX} development. Performing more than 10,000 classifications for each product, our model shows a mean precision between 0.85 and 0.90 predicting between 34\% and 48\% of all false test alarms.},
	pages = {39--48},
	booktitle = {2015 {IEEE}/{ACM} 37th {IEEE} International Conference on Software Engineering},
	author = {Herzig, Kim and Nagappan, Nachiappan},
	date = {2015-05},
	note = {{ISSN}: 1558-1225},
	keywords = {Testing, Cost engineering, Software engineering, Association rules, Software testing, Alarm systems, Codes (symbols), Software systems, Product development, Integration testing, Empirical data, Engineering time, Integration, Manual inspection, Mean precision, Microsoft windows, Safety engineering, Software testing strategies, Test infrastructures, Windows operating system, Inspection, Manuals, xno}
}

@inproceedings{wang_understanding_2021,
	title = {Understanding and Facilitating the Co-Evolution of Production and Test Code},
	doi = {10.1109/SANER50967.2021.00033},
	abstract = {Software products frequently evolve. When the production code undergoes major changes such as feature addition or removal, the corresponding test code typically should co-evolve. Otherwise, the outdated test may be ineffective in revealing faults or cause spurious test failures, which could confuse developers and waste {QA} resources. Despite its importance, maintaining such co-evolution can be time- and resource-consuming. Existing work has disclosed that, in practice, test code often fails to co-evolve with the production code. To facilitate the co-evolution of production and test code, this work explores how to automatically identify outdated tests. To gain insights into the problem, we conducted an empirical study on 975 open-source Java projects. By manually analyzing and comparing the positive cases, where the test code co-evolves with the production code, and the negative cases, where the co-evolution is not observed, we found that various factors (e.g., the different language constructs modified in the production code) can determine whether the test code should be updated. Guided by the empirical findings, we proposed a machine-learning based approach, {SITAR}, that holistically considers different factors to predict test changes. We evaluated {SITAR} on 20 popular Java projects. These results show that {SITAR}, under the within-project setting, can reach an average precision and recall of 81.4\% and 76.1\%, respectively, for identifying test code that requires update, which significantly outperforms rule-based baseline methods. {SITAR} can also achieve promising results under the cross-project setting and multiclass prediction, which predicts the exact change types of test code.},
	pages = {272--283},
	booktitle = {2021 {IEEE} International Conference on Software Analysis, Evolution and Reengineering ({SANER})},
	author = {Wang, Sinan and Wen, Ming and Liu, Yepang and Wang, Ying and Wu, Rongxin},
	date = {2021-03},
	note = {{ISSN}: 1534-5351},
	keywords = {Feature extraction, Testing, Software products, Baseline methods, Empirical findings, Empirical studies, Gain insight, Java programming language, Language constructs, Multiclass prediction, Open source software, Precision and recall, Reengineering, Turing machines, Machine learning, Semantics, Java, Production, Syntactics, Conferences, mining software repositories, Software evolution, test maintenance, xno}
}

@inproceedings{chen_effects_2016,
	title = {Effects of online fault detection mechanisms on Probabilistic Timing Analysis},
	doi = {10.1109/DFT.2016.7684067},
	abstract = {In real time systems, random caches have been proposed as a way to simplify software timing analysis, by avoiding corner cases usually found in deterministic systems. Using this random approach, one can obtain an application's probabilistic Worst Case Execution Time ({pWCET}) to be used for timing analysis. As with deterministic systems, technology scaling in cache memories is making transient and permanent faults more likely, which in turn affects the system's timing behavior. To mitigate these effects, one can introduce a detection mechanism that classifies a fault as transient or permanent, with the goal of disabling permanently faulty cache blocks to avoid future accesses. In this paper, we compare the effects of two online detection mechanisms for permanent faults, namely rule-based detection and Dynamic Hidden Markov Model (D-{HMM}) based detection, for the generation of safe {pWCET} estimates. Experimental results show that different mechanisms can greatly affect safe {pWCET} margins, and that by using D-{HMM} the {pWCET} of the system can be improved compared to rule-based detection.},
	pages = {41--46},
	booktitle = {2016 {IEEE} International Symposium on Defect and Fault Tolerance in {VLSI} and Nanotechnology Systems ({DFT})},
	author = {Chen, Chao and Panerati, Jacopo and Beltrame, Giovanni},
	date = {2016-09},
	note = {{ISSN}: 2377-7966},
	keywords = {Defects, Fault detection, Fault tolerance, Cache memory, Different mechanisms, Hidden Markov models, Interactive computer systems, Real time systems, Rule based detection, Software timing analysis, Timing circuits, Transient and permanent fault, Worst-case execution time, Reliability, Detection mechanism, Deterministic systems, Markov processes, Nanotechnology, On-line fault detection, {VLSI} circuits, Benchmark testing, Biological system modeling, Silicon, xno}
}

@inproceedings{xu_classification_2020,
	title = {The Classification and Propagation of Program Comments},
	abstract = {Natural language comments are like bridges between human logic and software semantics. Developers use comments to describe the function, implementation, and property of code snippets. This kind of connections contains rich information, like the potential types of a variable and the pre-condition of a method, among other things. In this paper, we categorize comments and use natural language processing techniques to extract information from them. Based on the semantics of programming languages, different rules are built for each comment category to systematically propagate comments among code entities. Then we use the propagated comments to check the code usage and comments consistency. Our demo system finds 37 bugs in real-world projects, 30 of which have been confirmed by the developers. Except for bugs in the code, we also find 304 pieces of defected comments. The 12 of them are misleading and 292 of them are not correct. Moreover, among the 41573 pieces of comments we propagate, 87 comments are for private native methods which had neither code nor comments. We also conduct a user study where we find that propagated comments are as good as human-written comments in three dimensions of consistency, naturalness, and meaningfulness.},
	pages = {1394--1396},
	booktitle = {2020 35th {IEEE}/{ACM} International Conference on Automated Software Engineering ({ASE})},
	author = {Xu, Xiangzhe},
	date = {2020-09},
	note = {{ISSN}: 2643-1572},
	keywords = {Data mining, Software, Program debugging, Extract informations, {NAtural} language processing, Natural language processing systems, Natural languages, Real world projects, Semantics, Semantics of programming languages, Software engineering, Three dimensions, User study, Computer bugs, comment, Computer languages, Natural language processing, programm analysis, xno}
}

@article{rajapaksha_sqaplanner_2021,
	title = {{SQAPlanner}: Generating Data-Informed Software Quality Improvement Plans},
	issn = {1939-3520},
	doi = {10.1109/TSE.2021.3070559},
	abstract = {Software Quality Assurance ({SQA}) planning aims to define proactive plans, such as defining maximum file size, to prevent the occurrence of software defects in future releases. To aid this, defect prediction models have been proposed to generate insights as the most important factors that are associated with software quality. Such insights that are derived from traditional defect models are far from actionable—i.e., practitioners still do not know what they should do or avoid to decrease the risk of having defects, and what is the risk threshold for each metric. A lack of actionable guidance and risk threshold can lead to inefficient and ineffective {SQA} planning processes. In this paper, we investigate the practitioners' perceptions of current {SQA} planning activities, current challenges of such {SQA} planning activities, and propose four types of guidance to support {SQA} planning. We then propose and evaluate our {AI}-Driven {SQAPlanner} approach, a novel approach for generating four types of guidance and their associated risk thresholds in the form of rule-based explanations for the predictions of defect prediction models. Finally, we develop and evaluate a visualization for our {SQAPlanner} approach. Through the use of qualitative survey and empirical evaluation, our results lead us to conclude that {SQAPlanner} is needed, effective, stable, and practically applicable. We also find that 80\% of our survey respondents perceived that our visualization is more actionable. Thus, our {SQAPlanner} paves a way for novel research in actionable software analytics—i.e., generating actionable guidance on what should practitioners do and not do to decrease the risk of having defects to support {SQA} planning.},
	pages = {1--1},
	journaltitle = {{IEEE} Transactions on Software Engineering},
	author = {Rajapaksha, Dilini and Tantithamthavorn, Chakkrit and Bergmeir, Christoph and Buntine, Wray and Jiarpakdee, Jirayus and Grundy, John},
	date = {2021},
	keywords = {Predictive models, Software, Computer software selection and evaluation, Defects, Software quality, Software defects, Defect prediction models, Forecasting, Predictive analytics, Air navigation, Defect model, Empirical evaluations, Planning process, Qualitative surveys, Quality assurance, Risk threshold, Risks, Software quality improvements, Surveys, Visualization, Artificial intelligence, Tools, Actionable Software Analytics, Explainable {AI}, Planning, Software Quality Assurance, {SQA} Planning, xyes},
	file = {Submitted Version:C\:\\Users\\michalm\\Zotero\\storage\\L4PA8RXG\\Rajapaksha et al. - 2021 - SQAPlanner Generating Data-Informed Software Qual.pdf:application/pdf}
}

@inproceedings{de_castro_ribeiro_detection_2018,
	title = {Detection and Classification of Faults in Aeronautical Gas Turbine Engine: a Comparison Between two Fuzzy Logic Systems},
	doi = {10.1109/FUZZ-IEEE.2018.8491444},
	abstract = {Gas turbines are the most common engine used in the majority of commercial aircraft. Due to its criticality, to detect and classify faults in a gas turbine is extremely important. In this work, a type-1 and singleton fuzzy logic system trained by steepest descent method is used for detecting and classifying gas turbine faults. The data set was obtained through simulations on the software Propulsion Diagnostic Method Evaluation Strategy created by the National Aeronautics and Space Administration. Results are compared to those obtained with a type-1 fuzzy classifier with rule extraction by Wang and Mendel method. Analysis of results shows the effectiveness of the proposed model. When compared to the Wang and Mendel fuzzy classifier, it requires fewer rules to achieve a better performance.},
	pages = {1--7},
	booktitle = {2018 {IEEE} International Conference on Fuzzy Systems ({FUZZ}-{IEEE})},
	author = {de Castro Ribeiro, Mateus Gheorghe and Calderano, Pedro Henrique Souza and Amaral, Renan Piazzaroli Finotti and de Menezes, Ivan Fabio Mota and Tanscheit, Ricardo and Vellasco, Marley Maria Bernardes Rebuzzi and de Aguiar, Eduardo Pestana},
	date = {2018-07},
	keywords = {Computer circuits, {NASA}, Classification (of information), Fault detection, Aircraft, Fuzzy systems, Fuzzy logic, Fuzzy sets, Aeronautical gas turbines, Commercial aircraft, Data set, Diagnostic methods, Error detection, Fuzzy classifiers, Fuzzy logic system, Gas turbines, Gases, Rule extraction, Steepest descent method, Sensors, Classification, Engines, Indexes, Aeronautical Gas Turbine, Atmospheric modeling, Detection., Fuzzy Logic System, Turbines, xno}
}

@inproceedings{hashim_automated_2010,
	title = {Automated visual inspection for metal parts based on morphology and fuzzy rules},
	doi = {10.1109/ICCAIE.2010.5735137},
	abstract = {Automated visual inspection system ({AVIS}) is a method of analyzing, classifying, detection defects for products at the production line. Usually, this inspection is either conducted by human, machine or both. In this paper, we explain an algorithm that capable to classify mechanical products in real time. The system is consists of two parts: hardware and software. The algorithm used the web-camera attaching to an adjustable arm to capture various image. Our main objective is to develop an image processing algorithm and fuzzy reasoning that can compute both the area and circularity of mechanical shapes and hence classify them according to their categories. The result shows the accuracy of classification is 80.5 \% for group classification and 98\% for individual classification of mechanical parts.},
	pages = {527--531},
	booktitle = {2010 International Conference on Computer Applications and Industrial Electronics},
	author = {Hashim, Haider Sh. and Abdullah, Siti Norul Huda Sheikh and Prabuwono, Anton Satria},
	date = {2010-12},
	keywords = {Feature extraction, Visualization, Hardware and software, Fuzzy rules, Algorithms, Inspection, Image processing algorithm, Automated visual inspection, Automated visual inspection systems, Computer applications, Computer vision, Fuzzy reasoning, Group classification, Industrial electronics, Mathematical morphology, Mechanical parts, Mechanical product, Metal part, Metal parts, Morphology, Production line, Real time, Visual inspection, fuzzy rules, Image edge detection, mathematical morphology, metal part, Pragmatics, Shape, visual inspection, xno}
}

@inproceedings{menzies_local_2011,
	title = {Local vs. global models for effort estimation and defect prediction},
	doi = {10.1109/ASE.2011.6100072},
	abstract = {Data miners can infer rules showing how to improve either (a) the effort estimates of a project or (b) the defect predictions of a software module. Such studies often exhibit conclusion instability regarding what is the most effective action for different projects or modules. This instability can be explained by data heterogeneity. We show that effort and defect data contain many local regions with markedly different properties to the global space. In other words, what appears to be useful in a global context is often irrelevant for particular local contexts. This result raises questions about the generality of conclusions from empirical {SE}. At the very least, {SE} researchers should test if their supposedly general conclusions are valid within subsets of their data. At the very most, empirical {SE} should become a search for local regions with similar properties (and conclusions should be constrained to just those regions).},
	pages = {343--351},
	booktitle = {2011 26th {IEEE}/{ACM} International Conference on Automated Software Engineering ({ASE} 2011)},
	author = {Menzies, Tim and Butcher, Andrew and Marcus, Andrian and Zimmermann, Thomas and Cok, David},
	date = {2011-11},
	note = {{ISSN}: 1938-4300},
	keywords = {Data mining, Software, Defects, Software engineering, Software modules, Defect prediction, Principal component analysis, Data miners, Effort Estimation, Estimation, Global models, validation, Context, Couplings, defect/effort estimation, empirical {SE}, Runtime, {USA} Councils, xyes, xdataset}
}

@inproceedings{shiling_time_2019,
	title = {Time Series Prediction and Pattern Recognition of Fault Decomposition Gas for High Voltage Composite Electrical Appliances Based on Grey System {OBGM} (1,N) Model},
	doi = {10.1109/ICCC47050.2019.9064088},
	abstract = {The {GIL}/{GIS} is widely used in the power system. The application of grey system to forecast and analyze the time series of gas decomposition and fault pattern recognition has good engineering value for its operation condition monitoring. Based on the programming of {MATLAB} software environment, time series prediction model based on grey system {OBGM} (1,N) is realized. Based on model, grey system time series analysis algorithm is applied to predict the time-varying sequence of the volume fraction of {SF}6 gas decomposition products and the characteristics of the partial discharge Atlas of the high-voltage combined electrical equipment {GIL}/{GIS} in fault state. Parametric time series, grey system is further applied to data mining and analysis of association rules of time series state change, and the time series set is applied to cluster analysis of typical fault types of high-voltage {GIL}/{GIS} power equipment. The grey time series prediction and grey relational clustering analysis of small sample test data can be carried out by using the grey system to analyze large data of high voltage {GIL}/{GIS} power equipment condition assessment. The multi-dimensional and the grey system information fusion technology proposed in this paper is especially suitable for the application of the small sample and the poor data in the high-voltage {GIL}/{GIS} power equipment operation condition detection technology. The time series prediction and the pattern recognition technology of the fault decomposition gas based on the grey system {OBGM} (1, N) model proposed can provide some technical support for the operation and maintenance of high voltage combinations. The characteristic parameters of partial discharge pattern and grey relational clustering analysis have the good engineering and the theoretical value.},
	pages = {943--947},
	booktitle = {2019 {IEEE} 5th International Conference on Computer and Communications ({ICCC})},
	author = {Shiling, Zhang and Jianchao, Wang},
	date = {2019-12},
	keywords = {Data mining, Predictive models, Forecasting, {MATLAB}, Cluster analysis, Condition monitoring, Electrical appliances, Electrical equipment, Operation and maintenance, Operation condition monitoring, Partial discharge pattern, Partial discharges, Pattern recognition systems, Pattern recognition technologies, Relational clustering, System theory, Time series analysis, Time series prediction, Monitoring, Mathematical model, Discharges (electric), Gas insulation, grey relational clustering analysis, grey system {OBGM}(1, grey time series prediction, high voltage composite electrical appliances, N) model, xno}
}

@inproceedings{mao_variable_2011,
	title = {Variable Precision Rough Set-Based Fault Diagnosis for Web Services},
	doi = {10.1109/TrustCom.2011.215},
	abstract = {Web service is the emergent technology for constructing more complex and flexible software system for business applications. However, some new features of Web service-based software such as heterogeneity and loose coupling bring great trouble to the latter fault debugging and diagnosis. In the paper, variable precision rough set-based diagnosis framework is presented. In such debugging model, {SOAP} message monitoring and service invocation instrument are used to record service interface information. Meanwhile, factors of execution context are also viewed as conditional attributes of knowledge representation system. The final execution result is treated as the decision attribute, and failure ontology is utilized to classify system's failure behaviors. Based on this extended information system, variable precision rough set reasoning is performed to generate the probability association rules, which are the clues for locating the possible faulty services. In addition, the experiment on a real-world Web services system is performed to demonstrate the feasibility and effectiveness of our proposed method.},
	pages = {1550--1555},
	booktitle = {2011IEEE 10th International Conference on Trust, Security and Privacy in Computing and Communications},
	author = {Mao, Chengying},
	date = {2011-11},
	note = {{ISSN}: 2324-9013},
	keywords = {Ontology, Program debugging, Association rules, Computer system recovery, Failure analysis, Software systems, Embedded systems, Rough set theory, Rough set, Business applications, Decision attribute, Embedded software, Execution context, Failure behaviors, Knowledge representation, Program diagnostics, Service interfaces, Service invocation, Service-based, {SOAP} messages, Variable precision, Variable precision rough sets, Web services, Information systems, association rule, Cognition, Business, fault diagnosis, failure, Linux, rough set, service interface, Wireless communication, xno}
}

@article{wang_security_2020,
	title = {Security Assessment of Blockchain in Chinese Classified Protection of Cybersecurity},
	volume = {8},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2020.3036004},
	abstract = {Classified protection is one of primary security policies of information system in many countries. With the increasing popularity of blockchain in various fields of applications, it is extremely necessary to promote classified protection for blockchain's risk assessment in order to push forward the sustainable development of blockchain. Taking the Level 3 in Chinese classified protection 2.0 as an example, this paper proposes the common evaluation rules on blockchain to ensure that blockchain can meet the needs of countries to build it as critical infrastructure. Both assessment requirements and enforcement proposals are presented and analyzed from the standpoint of blockchain's core technologies, e.g., peer-to-peer network, distributed ledger, contract's scripting system, and consensus mechanism. Moreover, the assessment results on three main platforms, Bitcoin, Ethereum, and Hyperledger, are summarized and analyzed in compliance with the control points specified in the level 3. Our investigation indicates that the current blockchain is able to satisfy the requirements of evaluation items in many aspects, such as software fault tolerance, resource control, backup and recovery, but further improvements are still needed for some aspects, including security audit, access control, identification and authentication, data integrity, etc., in order to satisfy the requirements of important fields on national security, economic development and human life.},
	pages = {203440--203456},
	journaltitle = {{IEEE} Access},
	author = {Wang, Di and Zhu, Yan and Zhang, Yi and Liu, Guowei},
	date = {2020},
	keywords = {Software, Access control, Blockchain, Compliance control, Core technology, Cyber security, Evaluation items, Evaluation rules, Fault tolerance, Fault tolerant computer systems, National security, Peer to peer networks, Resource control, Risk assessment, Security assessment, Security policy, Software fault tolerances, {XML}, assessment and analysis, classified protection of cybersecurity, consensus mechanism, Distributed ledger, Peer-to-peer computing, peer-to-peer network, Proposals, Risk management, Sustainable development, xno}
}

@inproceedings{denlinger_managing_2018,
	title = {Managing Defect Reduction to Achieve Reliability Growth},
	doi = {10.1109/RAM.2018.8463032},
	abstract = {The importance of reliability as well as a clear definition for a reliability requirement are presented. The concept of functional performance demonstrated early in product development with a very few hours of testing is presented as a more effective early warning sign that a program is behind schedule with respect to reliability goals. Utilizing the six types of defects encountered in a product development program as predictors of reliability is introduced. Types of failure modes and potential systemic causes of failure are discussed. The prediction relationship between manufacturing and design quality flaws with field failure mode experience is demonstrated. Systems dynamics modeling and empirical tracking of fix effectiveness can be used to predict the learning cycles required to launch a successful product. The concepts of defects per unit ({DPU}) (Defects in functional performance, product design quality and manufacturing process control); the Laplace estimate of proportion defective in a lot based on small sample size to predict the number of learning cycles required for each system program launch; and the rule of 10:4, where every 10 fixes of failure will lead to the discovery of four new causes of failure in the next set of prototypes, are introduced.},
	pages = {1--7},
	booktitle = {2018 Annual Reliability and Maintainability Symposium ({RAMS})},
	author = {Denlinger, Daniel},
	date = {2018-01},
	note = {{ISSN}: 2577-0993},
	keywords = {Testing, Defects, Forecasting, Software testing, Software reliability, Measurement, Failure modes, Functional performance, Learning cycle, Maintainability, Manufacture, Per unit, Prediction-relationships, Product design, Product development, Product development programs, Program Maturity, Reliability requirements, Rule of 10:4, Logic gates, Manufacturing, Defect per Unit, Learning Cycle, Manufacturing Rolled Throughput Yield, xno}
}

@inproceedings{kazman_case_2015,
	title = {A Case Study in Locating the Architectural Roots of Technical Debt},
	volume = {2},
	doi = {10.1109/ICSE.2015.146},
	abstract = {Our recent research has shown that, in large-scale software systems, defective files seldom exist alone. They are usually architecturally connected, and their architectural structures exhibit significant design flaws which propagate bugginess among files. We call these flawed structures the architecture roots, a type of technical debt that incurs high maintenance penalties. Removing the architecture roots of bugginess requires refactoring, but the benefits of refactoring have historically been difficult for architects to quantify or justify. In this paper, we present a case study of identifying and quantifying such architecture debts in a large-scale industrial software project. Our approach is to model and analyze software architecture as a set of design rule spaces ({DRSpaces}). Using data extracted from the project's development artifacts, we were able to identify the files implicated in architecture flaws and suggest refactorings based on removing these flaws. Then we built economic models of the before and (predicted) after states, which gave the organization confidence that doing the refactorings made business sense, in terms of a handsome return on investment.},
	pages = {179--188},
	booktitle = {2015 {IEEE}/{ACM} 37th {IEEE} International Conference on Software Engineering},
	author = {Kazman, Rick and Cai, Yuanfang and Mo, Ran and Feng, Qiong and Xiao, Lu and Haziyev, Serge and Fedak, Volodymyr and Shapochka, Andriy},
	date = {2015-05},
	note = {{ISSN}: 1558-1225},
	keywords = {Software engineering, Computer architecture, Technical debts, Economics, Architectural structure, Design rules, Economic models, Flawed structure, Industrial software, Large-scale software systems, Recent researches, Business, History, Microprocessors, Sonar detection, xno, xeee}
}

@inproceedings{xuedong_strategy_2017,
	title = {Strategy of diagnosis and test in {ICCS} of large-scale laser facility},
	doi = {10.1109/CAC.2017.8243207},
	abstract = {Integrated Computer Control System (abbr. {ICCS}) of large-scale laser facility uses a scalable software architecture to manage more than 10,000 control points to operate 48 powerful laser beamlines, provides for the integration of all elements of laser and target area distributed subsystems to form an overall operational control system. Fault detection and diagnosis test are the important technique to ensure the system's safety, reliability and healthy manage. This paper is an overview of the diagnostic and test strategy used in {ICCS} of large-scale laser facility, including design rules, task decomposition strategy, classification strategy, the scheme of on-line diagnosis and offline diagnosis, diagnosis process and data preconditioning about data standardization and quality control.},
	pages = {2563--2566},
	booktitle = {2017 Chinese Automation Congress ({CAC})},
	author = {Xuedong, Zhang and Xiaoli, Wang and Tianyou, Yun},
	date = {2017-10},
	keywords = {Software, Testing, Computer aided diagnosis, Fault detection, Hardware, Computer control systems, Data preconditioning, Data standardization, Distributed subsystems, Fault detection and diagnosis, {ICCS}, Integrated computer control systems, On-line diagnosis, Operational control, Quality control, Standardization, Fault diagnosis, Control systems, diagnosis and test, Laser theory, off-line diagnosis, on-line diagnosis, standardization, xno}
}

@inproceedings{sini_towards_2018,
	title = {Towards an automatic approach for hardware verification according to {ISO} 26262 functional safety standard},
	doi = {10.1109/IOLTS.2018.8474083},
	abstract = {The Failure Mode, Effect and Diagnostic Analysis ({FMEDA}) is a technique widely adopted by automotive industry to assess the level of reliability of hardware designs. Although very useful, it has the problem of taking a long time to complete and requires experts with extensive knowledge of the circuit under consideration. In this paper, it is presented a comparison between the analysis results obtained from an automatic tool developed by the authors with respect to the ones obtained by hand from a team of experts, followed by a critical review of the strengths and weaknesses, about the rules for automatic classification of the faults effects.},
	pages = {287--290},
	booktitle = {2018 {IEEE} 24th International Symposium on On-Line Testing And Robust System Design ({IOLTS})},
	author = {Sini, J. and Sonza Reorda, M. and Violante, M. and Sarson, P.},
	date = {2018-07},
	note = {{ISSN}: 1942-9401},
	keywords = {Software, Automatic classification, Reliability analysis, Software reliability, Measurement, Failure analysis, Computer software, Accident prevention, Automatic approaches, Automobile electronic equipment, Automotive industry, Circuit faults, Computer hardware, Critical review, Diagnostic analysis, Embedded systems, Functional Safety, Hardware, Hardware verification, {ISO} 26262, {ISO} Standards, Microcontrollers, Reliability, Systems analysis, Tools, Integrated circuit modeling, Safety, Automotive electronics, failure analysis, {ISO} 26262 standard, xno}
}

@article{wang_practical_2017,
	title = {Practical Network-Wide Packet Behavior Identification by {AP} Classifier},
	volume = {25},
	issn = {1558-2566},
	doi = {10.1109/TNET.2017.2720637},
	abstract = {Identifying the network-wide forwarding behaviors of a packet is essential for many network management applications, including rule verification, policy enforcement, attack detection, traffic engineering, and fault localization. Current tools that can perform packet behavior identification either incur large time and memory costs or do not support real-time updates. In this paper, we present {AP} Classifier, a control plane tool for packet behavior identification. {AP} Classifier is developed based on the concept of atomic predicates, which can be used to characterize the forwarding behaviors of packets. Experiments using the data plane network state of two real networks show that the processing speed of {AP} Classifier is faster than existing tools by at least an order of magnitude. Furthermore, {AP} Classifier uses very small memory and is able to support real-time updates.},
	pages = {2886--2899},
	number = {5},
	journaltitle = {{IEEE}/{ACM} Transactions on Networking},
	author = {Wang, Huazhe and Qian, Chen and Yu, Ye and Yang, Hongkun and Lam, Simon S.},
	date = {2017-10},
	keywords = {Computer aided diagnosis, Fault detection, Software defined networking, Failure analysis, Interactive computer systems, Real time systems, Computer networks, Data structures, Fault localization, {IEEE} transactions, Management applications, Packet classification, Packet networks, Policy enforcement, Ports (Computers), Practical networks, Throughput, Tools, Traffic Engineering, Fault diagnosis, Network-wide behavior, packet classification, Real-time systems, software-defined networking, xno}
}

@inproceedings{li_fractal_2010,
	title = {Fractal study on fault system of Carboniferous in Junggar Basin based on {GIS}},
	doi = {10.1109/GEOINFORMATICS.2010.5567793},
	abstract = {Fault system is a significant evidence of tectonic movement during crust tectonic evolution and may play an more important role in oil-gas accumulation process than other tectonic types in sedimentary basin. Carboniferous surface faults in Junggar Basin developed well and varied in size and distribution. There are about 200 faults in Carboniferous, and 187 of them are thrust faults. Chaos-fractals theories have been widely investigated and great progress has been made in the past three decades. One of the important conception-fractal dimension had become a powerful tool for describing non-linearity dynamical system characteristic. The clustered objects in nature are often fractal and fault system distribution in space is inhomogeneous, always occurs in groups, so we can describe spatial distribution of faults from the point of fractal dimension. Fractal dimension of fault system is a comprehensive factor associated with fault number, size, combination modes and dynamics mechanism, so it can evaluate the complexity of fault system quantitatively. The relationship between fault system and oil-gas accumulation is a focus and difficulty problem in petroleum geology, and fractal dimension is a new tool for describing fault distribution and predicting potential areas of hydrocarbon resources. Geographic Information System ({GIS}) is a kind of technological system collecting, storing, managing, computing, analyzing, displaying and describing the geospatial information supported by computer software and hardware. In the last 15-20 years, {GIS} have been increasingly used to address a wide variety of geoscience problems. Weights-of-evidence models use the theory of conditional probability to quantify spatial association between fractal dimension and oil-gas accumulation. The weights of evidence are combined with the prior probability of occurrence of oil-gas accumulation using Bayes'rule in a loglinear form under an assumption of conditional independence of the dimension maps to derive posterior probability of occurrence of oil-gas accumulation. In this paper, we first vectorize the fault system in Carboniferous of Junggar Basin in {GIS} software and store it as polyline layer in Geodatabase of {GIS} to manage and analyze, then calculate the fractal dimension of three types which are box dimension, information dimension and cumulative length dimension using spatial functions of {GIS}, in the last use weights-of-evidence model to calculate the correlation coefficients in {GIS} environment between oil-gas accumulation and three types of fractal dimension in order to quantity the importance of fault system.},
	pages = {1--5},
	booktitle = {2010 18th International Conference on Geoinformatics},
	author = {Li, Bo and Zhang, Tingshan and Ding, Guangming and Wang, Weiyuan and Xiang, Yu},
	date = {2010-06},
	note = {{ISSN}: 2161-0258},
	keywords = {Partial discharges, Analytical models, Weights of evidences, Gases, Computer hardware, Information systems, Bayes' rule, Box dimension, Combination modes, Conditional independences, Conditional probabilities, Correlation coefficient, Dynamical systems, Evidence model, Fault distribution, Fault system, Fractal dimension, Fractal studies, Geodatabase, Geographic information, Geographic information systems, Geosciences, Geospatial information, {GIS} software, Hydrocarbon resources, Hydrocarbons, Information dimensions, Junggar Basin, Non-Linearity, Oil-gas accumulation, Petroleum geology, Posterior probability, Prior probability, Probability, Sedimentary basin, Size distribution, Spatial distribution, Spatial functions, Surface faults, System characteristics, Technological system, Tectonic evolution, Tectonic movements, Tectonics, Thrust faults, Computational modeling, Correlation, fault system, fractal dimension, Fractals, Geographic Information System, Geographic Information Systems, Geology, oil-gas accumulation, Petroleum, weights-of-evidence model, xno}
}

@inproceedings{wang_state_2019,
	title = {State Assessment of Relay Protection Device Based on Defect Information},
	doi = {10.1109/EI247390.2019.9062256},
	abstract = {Considering the state assessment of relay protection device gray and fuzzy, an analytical method based on the combination of grey correlation analysis and apriori correlation analysis is proposed in this paper. Defect information data from the operation and maintenance of actual substations are used to assess and predict the status of relay protection devices which are affected by different manufacturers, different equipment types, operation time and maintenance records. Seven types of correlation rules are obtained in the research, and the analytical results can provide a strong scientific basis for targeted maintenance and operation plans based on the specific results in the correlation rules. The results provide a software package for the evaluation of relay protection devices in a regional substation of Shanghai power supply company of the State Grid.},
	pages = {1499--1503},
	booktitle = {2019 {IEEE} 3rd Conference on Energy Internet and Energy System Integration ({EI}2)},
	author = {Wang, Fa and Xiao, Jinxing and Feng, Jie and Xiong, Fenfang and Huang, Liangliang and Xu, Hongchao},
	date = {2019-11},
	keywords = {Defects, Electric power system protection, Operation and maintenance, Correlation analysis, Correlation methods, Different equipment, Grey correlation analysis, Maintenance, Maintenance and operation, Maintenance records, Power supply company, Relay protection, Relay protection devices, Maintenance engineering, Indexes, Correlation, Apriori correlation analysis, Companies, Condition assessment, Defect information, Power supplies, Protective relaying, Relay protection device, Substations, xno}
}

@inproceedings{yang_future_2019,
	title = {The Future of Broadband Access Network Architecture and Intelligent Operations},
	doi = {10.1109/CyberC.2019.00060},
	abstract = {This paper presents an overview of the evolution towards a broadband optical access network. Network operations transformation with emphasis on automated broadband service monitoring, fault detection, fault recovery, and fault prediction is also discussed. However, in order to support broadband access network evolution and to accelerate telco operations transformation, traditional network operations, administration and maintenance ({OAM}) methods will not be sufficient. Therefore, we describe our key research contributions regarding new approaches to support closed-loop broadband network {OAM} that will eliminate human touches and automate non-physical fault recovery for daily routine tasks. We have designed and implemented a network knowledge engine ({NKE}) centered software framework to enable data collection, correlation, and analysis for troubleshooting complex issues in the broadband access network. Our solutions will address the following network management domains, including (a) knowledge management, (b) surveillance management, (c) incident management, and (d) problem management. In this paper, we focus on auto proactive and predictive trouble management which can verify and confirm a passive device problem location by {NKE}. We will describe a step-by-step methodology to control data process flow, optical fiber path discovery management, and auto-detection of failed passive devices by using the following techniques: (a) big data analytics, (b) knowledge graph, (c) machine learning ({ML}), and (d) artificial intelligence ({AI}) prediction rule and learning policy involving the spatial-temporal algorithm. With combinations of these technique, our preliminary study indicates that the accuracy of auto fault detection is 93\%, while the false positive rate is 0.01\%. As results, we demonstrated the promise of this novel technology framework in supporting transformation from expert's domain knowledge into machine knowledge.},
	pages = {308--316},
	booktitle = {2019 International Conference on Cyber-Enabled Distributed Computing and Knowledge Discovery ({CyberC})},
	author = {Yang, Charlie Chen-Yui and Li, Guangzhi and Liu, Xiang and Wu, Zonghuan and Zhang, Kaiyu},
	date = {2019-10},
	keywords = {Machine learning, Fault detection, Advanced Analytics, Big data, Broad-band access networks, Broadband networks, Computer programming, Computer system recovery, Data Analytics, False positive rates, Flow graphs, Incident Management, Intelligent operations, Knowledge graphs, Knowledge management, Network architecture, Network operations, Optical access networks, Optical fibers, Software frameworks, Wireless communication, Bandwidth, Big Data Analytics, Broadband Access Network, Broadband communication, Cable {TV}, Coaxial cables, Knowledge engineering, Knowledge Graph, {ML} and {AI}, Telephone sets, xno}
}

@article{mahela_recognition_2020,
	title = {Recognition of Complex Power Quality Disturbances Using S-Transform Based Ruled Decision Tree},
	volume = {8},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2020.3025190},
	abstract = {Deteriorated quality of power leads to problems, such as equipment failure, automatic device resets, data errors, failure of circuit boards, loss of memory, power supply issues, uninterrupted power supply ({UPS}) systems generate alarm, corruption of software, and heating of wires in distribution network. These problems become more severe when complex (multiple) power quality ({PQ}) disturbances appear. Hence, this manuscript introduces an algorithm for identification of the complex nature {PQ} events in which it is supported by Stockwell's transform ({ST}) and decision tree ({DT}) using rules. {PQ} events with complex nature are generated in view of {IEEE}-1159 standard. Eighteen different types of complex {PQ} issues are considered and studied which include second, third, and fourth order disturbances. These are obtained by combining the single stage {PQ} events such as sag \& swell in voltage, momentary interruption ({MI}), spike, flicker, harmonics, notch, impulsive transient ({IT}), and oscillatory transient ({OT}). The {ST} supported frequency contour and proposed plots such as amplitude, summing absolute values, phase and frequency-amplitude obtained by multi-resolution analysis ({MRA}) of signals are used to identify the complex {PQ} events. The statistical features such as sum factor, Skewness, amplitude factor, and Kurtosis extracted from these plots are utilized to classify the complex {PQ} events using rule-based {DT}. This is established that proposed approach effectively identifies a number of complex nature {PQ} events with accuracy above 98\%. Performance of the proposed method is tested successfully even with noise level of 20 {dB} signal to noise ratio ({SNR}). Effectiveness of the proposed algorithm is established by comparing it with the methods reported in literature such as fuzzy c-means clustering ({FCM}) \& adaptive particle swarm optimization ({APSO}), Wavelet transform ({WT}) \& neural network ({NN}), spline {WT} \& {ST}, {ST} \& {NN}, and {ST} \& fuzzy expert system ({FES}). Results of simulations are validated by comparing them with real time results computed by Real Time Digital Simulator ({RTDS}). Different stages for design of complex {PQ} monitoring device using the proposed approach are also described. It is verified that the proposed approach can effectively be employed for design of the online complex {PQ} monitoring devices.},
	pages = {173530--173547},
	journaltitle = {{IEEE} Access},
	author = {Mahela, Om Prakash and Shaik, Abdul Gafoor and Khan, Baseem and Mahla, Rajendra and Alhelou, Hassan Haes},
	date = {2020},
	keywords = {Feature extraction, Decision trees, Adaptive particle swarm optimizations, Alarm systems, Complex networks, Electric fault currents, Electric power supplies to apparatus, Expert systems, Fuzzy C means clustering, Higher order statistics, Momentary Interruptions, Oscillatory transients, Outages, Particle swarm optimization ({PSO}), Power quality, Power quality disturbances, Real time digital simulator, Signal to noise ratio, Statistical features, Trees (mathematics), Uninterrupted power supply, Uninterruptible power systems, Wavelet transforms, Monitoring, Transforms, power quality, Complex nature {PQ} event, ruled decision tree, statistical feature, Stockwell's transform, Time-frequency analysis, xno}
}

@inproceedings{tiwari_study_2020,
	title = {Study of Combined Time Current Grading Protection Scheme for Distribution System},
	doi = {10.1109/PARC49193.2020.236651},
	abstract = {The paper paraphrases the power system protection principle using overcurrent relaying, distribution network configurations and relay coordination to enhance reliability of power supply. The distribution network topology and its protection schemes are described in detail. The most simple, cheapest \& high speed scheme of protections i.e. overcurrent and their limitations are reported. Rules for setting of relays operating time and pickup current setting under time graded, current graded and in combined time \& current graded protection schemes for various network configurations are examined. A {PSCAD}/{EMTDC} software environment is used to simulate the radial distribution network and {MATLAB} software for analysis. The variation in fault current and fault {MVA} with change in fault position are simulated. Result indicates the time and current setting for relays under combined time and current protection scheme. Instant of fault clearance and relay trip signal reflects coordination of primary and back protection. by proper selecting the faulty zone in case of combine setting of both time and current of relays. In addition the classification of overcurrent relay as per {ANSI}/{IEEE}, {IEC} standards and the protection zones for radial, ring main and parallel network are inferred \& the study engrossed on a small portion of distribution network consisting four subsection with load on each.},
	pages = {443--448},
	booktitle = {2020 International Conference on Power Electronics {IoT} Applications in Renewable Energy and its Control ({PARC})},
	author = {Tiwari, Ravi Shankar and Hari Gupta, Om},
	date = {2020-02},
	keywords = {{MATLAB}, Internet of things, Distribution network configuration, Distribution network topology, Electric power system protection, Grading, Network configuration, Overcurrent protection, Pickup current settings (Ip), Power electronics, Power system protection, Radial distribution networks, Reliability of power supply, Software environments, Current grading, Distribution system, {EMTDC}, {IDMT}, Ring main, Time graded, Time grading, xno}
}

@inproceedings{mutlu_end--end_2018,
	title = {End-to-End Hierarchical Fuzzy Inference Solution},
	doi = {10.1109/FUZZ-IEEE.2018.8491481},
	abstract = {Hierarchical Fuzzy System ({HFS}) is a popular approach for handling curse of dimensionality problem occurred in complex fuzzy rule-based systems with various and numerous inputs. However, the processes of modeling and reasoning of {HFS} have some critical issues to be considered. In this study, the effect of these issues on the accuracy and stability of the resulting system has been investigated, and an end-to-end {HFS} framework has been proposed. The proposed framework has three main steps such as single system modeling, rule partitioning and {HFS} reasoning. It is fully automated, generic, almost independent from data, and applicable for any kind of inference problem. In addition, the proposed framework preserves accuracy and stability during the {HFS} reasoning. These judgments have been ensured by a number of experimental studies on several datasets about software faulty prediction ({SFP}) problem with a large feature space. The main contributions of this paper are as follows: (i) it provides the entire {HFS} implementation from problem definition to calculation of final output, (ii) it increases the accuracy of recently proposed rule generation scheme in the literature, (iii) it presents the only possible fuzzy system solution for {SFP} problem containing a large feature space with reasonable accuracy.},
	pages = {1--9},
	booktitle = {2018 {IEEE} International Conference on Fuzzy Systems ({FUZZ}-{IEEE})},
	author = {Mutlu, Begum and Sezer, Ebru A. and Ali Akcayol, M.},
	date = {2018-07},
	keywords = {Fuzzy inference, Fuzzy systems, Fuzzy sets, Decision making, Critical issues, Curse of dimensionality, Fully automated, Hierarchical fuzzy, Hierarchical fuzzy systems, Hierarchical systems, Inference problem, Problem definition, Reasonable accuracy, Cognition, Production, Hafnium, Stability analysis, xyes}
}

@inproceedings{jafri_outlier_2018,
	title = {Outlier Detection in {WSN}},
	doi = {10.1109/ICICT43934.2018.9034286},
	abstract = {In Wireless Sensor Network ({WSN}), anomaly detection is a consequential challenge for tasks like fault detection, intrusion detection and supervising applications. The sensor nodes with constrained resources execute freely for collaborating and managing the network of wireless via which the amassment and transferring of raw data is considered towards the decision makers or the terminus users. This network could be utilized in challenging applications like home automation, health monitoring system, fire detection system and enemy target monitoring and so on in which there is a dependency on {WSN}. These types of applications have precise and reliable data. {WSN} could be incognizant to the anomalies that occur due to less costly hardware and software and non-operative Operating system ({OS}) that may affect the communication of network. The hybrid algorithms have been developed for anomaly detection that considers the intrinsic limits of wireless sensor networks in their development so that the energy consumption for the wireless sensor nodes is minimized and the throughput of the {WSN} is maximized during the simulation. Hamamoto et.al has also utilized Genetic Algorithm for signature generation and fuzzy logic to generate rule sets for the anomaly detection and the problems are identified in Hamamoto et al. research work. Generation of signature in each iteration may consume a lot of time. For large networks, the number of rules will be very high. The problem of this work is the removal of the problems occurred in Hamamoto using hybrid algorithm predicated on Genetic Algorithm and Artificial Neural Network. According to the fitness function, we will optimize the property of each node those are involved in the simulation of {WSN}. So, we design a novel fitness function for Genetic Algorithm ({GA}) and using Artificial Neural Network ({ANN}) as a classifier to detect the anomaly nodes. The simulation would be executed in {MATLAB} and for the authentication of the work; various performance parameters like Accuracy, Error Rate, True positive rate and False positive rate, Throughput and Energy Consumption will be calculated.},
	pages = {236--241},
	booktitle = {2018 3rd International Conference on Inventive Computation Technologies ({ICICT})},
	author = {Jafri, Rana and Kumar, Rakesh},
	date = {2018-11},
	keywords = {Genetic algorithms, Iterative methods, {MATLAB}, Hardware and software, Fault detection, Support vector machines, False positive rates, Health, Fuzzy logic, Neural networks, Anomaly detection, Constrained resources, Decision making, Energy utilization, Fire detection systems, Fire detectors, Gallium, Health monitoring system, Intrusion detection, Monitoring, Performance parameters, Sensor nodes, Signature generation, Wireless sensor node, Wireless sensor networks, {ANN}, Anomaly Detection, Artificial neural networks, {GA}, Neurons, Telecommunication traffic, {WSN}, xno}
}

@inproceedings{han_distributed_2019,
	title = {A distributed autonomic logistics system with parallel-computing diagnostic algorithm for aircrafts},
	doi = {10.1109/AUTEST.2019.8878477},
	abstract = {The autonomic logistic system ({ALS}), first used by the U.S. military {JSF}, is a new conceptional system which supports prognostic and health management system of aircrafts, including such as real-time failure monitoring, remaining useful life prediction and maintenance decisions-making. However, the development of {ALS} faces some challenges. Firstly, current {ALS} is mainly based on client/server architecture, which is very complex in a large-scale aircraft control center and software is required to be reconfigured for every accessed node, which will increase the cost and decrease the expandability of deployment for large scale aircraft control centers. Secondly, interpretation of telemetry parameters from the aircraft is a tough task considering various real-time flight conditions, including instructions from controllers, work statements of single machines or machine groups, and intrinsic physical meaning of telemetry parameters. It is troublesome to meet the expectation of full representing the relationship between faults and tests without a standard model. Finally, typical diagnostic algorithms based on dependency matrix are inefficient, especially the temporal waste when dealing with thousands of test points and fault modes, for the reason that the time complexity will increase exponentially as dependency matrix expansion. Under this situation, this paper proposed a distributed {ALS} under complex operating conditions, which has the following contributions 1) Introducing a distributed system based on browser/server architecture, which is divided overall system into primary control system and diagnostic and health assessment platform; 2) Designing a novel interface for modelling the interpretation rules of telemetry parameters and the relationship between faults and tests in consideration of multiple elements of aircraft conditions; 3) Proposing a promoted diagnostic algorithm under parallel computing in order to decrease the computing time complexity. what's more, this paper develops a construction with 3D viewer of aircraft for user to locate fault points and presents repairment instructions for maintenance personnels based on Interactive Electronic Technical Manual, which supports both online and offline. A practice in a certain aircraft demonstrated the efficiency of improved diagnostic algorithm and proposed {ALS}.},
	pages = {1--8},
	booktitle = {2019 {IEEE} {AUTOTESTCON}},
	author = {Han, Danyang and Yu, Jinsong and Song, Yue and Tang, Diyin and Dai, Jing},
	date = {2019-08},
	note = {{ISSN}: 1558-4550},
	keywords = {Aircraft, Aircraft control, Autonomic logistics, Browser/server architectures, Client server computer systems, Client/server architecture, Failure analysis, Flight control systems, Health, Maintenance decisions, Maintenance personnel, Matrix algebra, Prognostic and health management, Prognostics and health managements, Remaining useful life predictions, Telemetering equipment, fault diagnosis, autonomic logistic, prognostics and health management, xno},
	file = {Full Text:C\:\\Users\\michalm\\Zotero\\storage\\6N8QRTJB\\Han et al. - 2019 - A distributed autonomic logistics system with para.pdf:application/pdf}
}