
@inproceedings{szczypinski_qmazda_2017,
	title = {{QMaZda} - Software tools for image analysis and pattern recognition},
	volume = {2017-September},
	isbn = {978-83-62065-30-1},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041504873&doi=10.23919%2fSPA.2017.8166867&partnerID=40&md5=09992260f6b97c545049c6f6442bd115},
	doi = {10.23919/SPA.2017.8166867},
	abstract = {Qmazda is a package of software tools for digital image analysis. They compute shape, color and texture attributes in arbitrary regions of interest, implement selected algorithms of discriminant analysis and machine learning, and enable texture based image segmentation. The algorithms generalize a concept of texture to three-dimensional data to enable analysis of volumetric images from magnetic resonance imaging or computed tomography scanners. The tools support a complete workflow - from image examples as an input to classification rules as an output. The extracted knowledge can be further used in custom made image analysis systems. Here we also present an application of {QMaZda} to identify defective barley kernels. The cereal seeds variability is high, therefore, characterization and discriminant analysis of such the biological objects is challenging and non-trivial. The software is available free of charge and open source, with executables for Windows, Linux and {OS} X platforms. © 2017 Division of Signal Processing and Electronic Systems, Poznan University of Technology.},
	pages = {217--221},
	booktitle = {Signal Processing - Algorithms, Architectures, Arrangements, and Applications Conference Proceedings, {SPA}},
	publisher = {{IEEE} Computer Society},
	author = {Szczypinski, P.M. and Klepaczko, A. and Kociolek, M.},
	date = {2017},
	note = {{ISSN}: 23260262},
	keywords = {Artificial intelligence, Biological objects, cereal grains, Cereal grains, Classification rules, Computed tomography scanners, Computer architecture, Computer operating systems, Computer software, Computerized tomography, {DH}-{HEMTs}, Digital image analysis, Discriminant analysis, feature extraction, Feature extraction, High definition video, Image analysis, Image analysis systems, Image processing, Image segmentation, Image texture, Integrated circuits, Learning systems, machine learning, Magnetic resonance imaging, Open source software, Open systems, Pattern recognition, Regions of interest, Signal processing, Signal processing algorithms, Three-dimensional data, xno}
}

@inproceedings{tua_software_2019,
	title = {Software Defect Prediction Using Software Metrics with Naïve Bayes and Rule Mining Association Methods},
	volume = {1},
	doi = {10.1109/ICST47872.2019.9166448},
	abstract = {Software defect prediction ({SDP}) can help testers decide allocation of resources rationally to find defects effectively, so as to improve software quality. Naive Bayes ({NB}) is one of the most used classification algorithms because of the simplicity of the algorithm and easy to implement. The purpose of this study is to add the process of selecting features using {ARM} in the software prediction process using the {NB} method in the hope that it can improve the performance of the method using software metrics. Software metrics have an association with one another in completing software, so this cannot be ignored. Results of the empirical evaluation of scenario 1 (one) showed an increase with the values of parameter precision, recall, f-measure and accuracy of 0.101, 0.190, 0.154 and 0.180, and scenario 2 (second) also increased by 0.106, 0.182, 0.159 and 0.163, also as in scenario 3 (third) the proposed method shows good performance compared to using {SVM}, {NN} and {DTREE} with an average performance of 0.960 while the others are 0.855, 0.859 and 0.861. From the empirical results of the three scenarios made, the proposed performance method is better than the other methods.},
	pages = {1--5},
	booktitle = {2019 5th International Conference on Science and Technology ({ICST})},
	author = {Tua, Fernando Maruli and Danar Sunindyo, Wikan},
	date = {2019-07},
	keywords = {association rule mining, Barium compounds, Classification algorithm, Clustering algorithms, Computer software selection and evaluation, Data mining, Defects, Empirical evaluations, F measure, Feature extraction, feature selection, Forecasting, k-means, Naive bayes, Naïve Bayes, Prediction process, Predictive models, Rule mining, Software, software defect prediction, Software defect prediction, software engineering, Software metrics, Software quality, Support vector machines, Testing, xno}
}

@inproceedings{karthik_defect_2010,
	title = {Defect association and complexity prediction by mining association and clustering rules},
	volume = {7},
	doi = {10.1109/ICCET.2010.5485608},
	abstract = {Number of defects remaining in a system provides an insight into the quality of the system. Software defect prediction focuses on classifying the modules of a system into fault prone and non-fault prone modules. This paper focuses on predicting the fault prone modules as well as identifying the types of defects that occur in the fault prone modules. Software defect prediction is combined with association rule mining to determine the associations that occur among the detected defects and the effort required for isolating and correcting these defects. Clustering rules are used to classify the defects into groups indicating their complexity: {SIMPLE}, {MODERATE} and {COMPLEX}. Moreover the defects are used to predict the effect on the project schedules and the nature of risk concerning the completion of such projects.},
	pages = {V7--569--V7--573},
	booktitle = {2010 2nd International Conference on Computer Engineering and Technology},
	author = {Karthik, R. and Manikandan, N.},
	date = {2010-04},
	keywords = {Association rule mining, Association rules, Associative processing, Clustering, Clustering rules, Complexity predictions, Data mining, Defect Associations, Defect classification, Defect Classification, Defect correction effort, Defect Correction Effort, Defects, Fault diagnosis, Fault-prone, Fault-prone modules, Forecasting, Information technology, Mining associations, Prediction methods, Project management, Project schedules, Resource management, Risk analysis, Software defect prediction, Software systems, Testing, xyes},
	file = {Karthik and Manikandan - 2010 - Defect association and complexity prediction by mi.pdf:C\:\\Users\\michalm\\Zotero\\storage\\TCX4Z455\\Karthik and Manikandan - 2010 - Defect association and complexity prediction by mi.pdf:application/pdf}
}

@inproceedings{shao_software_2017,
	title = {Software defect prediction based on class-association rules},
	doi = {10.1109/ICRSE.2017.8030774},
	abstract = {Although there have lots of studies on using static code attributes to identify defective software modules, there still have many challenges. For instance, it is difficult to implement the Apriori-type algorithm to predict defects by learning from an imbalanced dataset. For more accurate and understandable defect prediction, a novel approach based on class-association rules algorithm is proposed. Class-association rules are looked as a separate class label, which is a specific type of association rules that explores the relationship between attributes and categories. In an empirical comparison with four datasets, the novel approach is superior to other four classification techniques and accordingly, proved it's valuable for defect prediction.},
	pages = {1--5},
	booktitle = {2017 Second International Conference on Reliability Systems Engineering ({ICRSE})},
	author = {Shao, Yuanxun and Liu, Bin and Li, Guoqi and Wang, Shihai},
	date = {2017-07},
	keywords = {Apriori, association rule, Classification algorithms, Itemsets, Modeling, Prediction algorithms, prediction performance, rule pruning, Software, Software algorithms, software defect prediction, Training, xassociation-rules, xyes}
}

@inproceedings{thapa_software_2020,
	title = {Software Defect Prediction Using Atomic Rule Mining and Random Forest},
	doi = {10.1109/CITISIA50690.2020.9371797},
	abstract = {This research aims to improve software defect prediction in terms of accuracy and processing time. The new proposed algorithm is based on the Random Forest Algorithm that classifies and distributes the data based on tree module. It has value either 1 for defective module or 0 for the non-defective module. Random Forest Algorithm selects a feature from a subset of features which has been already classified. Random Forest Algorithm uses a number of trees for the prediction. For this research, datasets were tested with 10 and 15 sets of trees. Results showed an improvement in accuracy and processing time when the proposed system was used compared to the current solution for the software defect model generation and prediction. The proposed solution achieved an accuracy of 90.09\% whereas processing time dropped by 54.14\%. Processing time decreased from 19.78s to 9.07s during the prediction for over 100 records. Accuracy was improved from 89.97\% to 90.09\%. The proposed solution uses Atomic Rule Mining with Random Forest Algorithm for software defect prediction. It consists of classification and prediction process by using the Random Forest Algorithm during storing data that is carried out using Atomic Rule Mining.},
	pages = {1--8},
	booktitle = {2020 5th International Conference on Innovative Technologies in Intelligent Systems and Industrial Applications ({CITISIA})},
	author = {Thapa, Suroj and Alsadoon, Abeer and Prasad, P.W.C. and Al-Dala’in, Thair and Rashid, Tarik A.},
	date = {2020-11},
	keywords = {Apriori, Atomic Rule Mining, Atoms, Classification algorithms, Data mining, Decision trees, Deep Learning, Defects, Forecasting, Forestry, Intelligent systems, Number of trees, Prediction algorithms, Prediction process, Processing time, Random Forest, Random forest algorithm, Random forests, Rule mining, Software, Software algorithms, Software defect prediction, Software Defect Prediction, Software defects, Tree modules, Vegetation, xno}
}

@inproceedings{monden_heuristic_2012,
	title = {A Heuristic Rule Reduction Approach to Software Fault-proneness Prediction},
	volume = {1},
	doi = {10.1109/APSEC.2012.103},
	abstract = {Background: Association rules are more comprehensive and understandable than fault-prone module predictors (such as logistic regression model, random forest and support vector machine). One of the challenges is that there are usually too many similar rules to be extracted by the rule mining. Aim: This paper proposes a rule reduction technique that can eliminate complex (long) and/or similar rules without sacrificing the prediction performance as much as possible. Method: The notion of the method is to removing long and similar rules unless their confidence level as a heuristic is high enough than shorter rules. For example, it starts with selecting rules with shortest length (length=1), and then it continues through the 2nd shortest rules selection (length=2) based on the current confidence level, this process is repeated on the selection for longer rules until no rules are worth included. Result: An empirical experiment has been conducted with the Mylyn and Eclipse {PDE} datasets. The result of the Mylyn dataset showed the proposed method was able to reduce the number of rules from 1347 down to 13, while the delta of the prediction performance was only. 015 (from. 757 down to. 742) in terms of the F1 prediction criteria. In the experiment with Eclipsed {PDE} dataset, the proposed method reduced the number of rules from 398 to 12, while the prediction performance even improved (from. 426 to. 441.) Conclusion: The novel technique introduced resolves the rule explosion problem in association rule mining for software proneness prediction, which is significant and provides better understanding of the causes of faulty modules.},
	pages = {838--847},
	booktitle = {2012 19th Asia-Pacific Software Engineering Conference},
	author = {Monden, Akito and Keung, Jacky and Morisaki, Shuji and Kamei, Yasutaka and Matsumoto, Ken-Ichi},
	date = {2012-12},
	note = {{ISSN}: 1530-1362},
	keywords = {association rule mining, Association rules, data mining, defect prediction, Educational institutions, empirical study, Explosions, Measurement, Predictive models, Software, software quality, xassociation-rules, xyes},
	file = {Monden et al. - 2012 - A Heuristic Rule Reduction Approach to Software Fa.pdf:C\:\\Users\\michalm\\Zotero\\storage\\5KLR4JGE\\Monden et al. - 2012 - A Heuristic Rule Reduction Approach to Software Fa.pdf:application/pdf}
}

@article{he_ensemble_2019,
	title = {Ensemble {MultiBoost} Based on {RIPPER} Classifier for Prediction of Imbalanced Software Defect Data},
	volume = {7},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2019.2934128},
	abstract = {Identifying defective software entities is essential to ensure software quality during software development. However, the high dimensionality and class distribution imbalance of software defect data seriously affect software defect prediction performance. In order to solve this problem, this paper proposes an Ensemble {MultiBoost} based on {RIPPER} classifier for prediction of imbalanced Software Defect data, called {EMR}\_SD. Firstly, the algorithm uses principal component analysis ({PCA}) method to find out the most effective features from the original features of the data set, so as to achieve the purpose of dimensionality reduction and redundancy removal. Furthermore, the combined sampling method of adaptive synthetic sampling ({ADASYN}) and random sampling without replacement is performed to solve the problem of data class imbalance. This classifier establishes association rules based on attributes and classes, using {MultiBoost} to reduce deviation and variance, so as to achieve the purpose of reducing classification error. The proposed prediction model is evaluated experimentally on the {NASA} {MDP} public datasets and compared with existing similar algorithms. The results show that {EMR}\_SD algorithm is superior to {DNC}, {CEL} and other defect prediction techniques in most evaluation indicators, which proves the effectiveness of the algorithm.},
	pages = {110333--110343},
	journaltitle = {{IEEE} Access},
	author = {He, Haitao and Zhang, Xu and Wang, Qian and Ren, Jiadong and Liu, Jiaxin and Zhao, Xiaolin and Cheng, Yongqiang},
	date = {2019},
	keywords = {Class distributions, class imbalance, Classification algorithms, Classification errors, combined sampling, Computer software selection and evaluation, Defect prediction, Defects, Dimensionality reduction, Evaluation indicators, Feature extraction, Forecasting, High dimensionality, {MultiBoost}, {NASA}, Prediction algorithms, Predictive analytics, Predictive models, Principal component analysis, Redundancy removal, rule learning, Software, Software algorithms, Software defect prediction, Software design, Software entities, Software quality, xyes},
	file = {Full Text:C\:\\Users\\michalm\\Zotero\\storage\\SKE8UCK9\\He et al. - 2019 - Ensemble MultiBoost Based on RIPPER Classifier for.pdf:application/pdf;Full Text:C\:\\Users\\michalm\\Zotero\\storage\\XKMMDBBD\\He et al. - 2019 - Ensemble MultiBoost Based on RIPPER Classifier for.pdf:application/pdf}
}

@inproceedings{naufal_software_2019,
	title = {Software defect detection based on selected complexity metrics using fuzzy association rule mining and defective module oversampling},
	doi = {10.1109/JCSSE.2019.8864165},
	abstract = {Software defect is a major problem in software development. The cost of software development will be minimized when the software defects are detected earlier. Complexity metric is a mathematic calculation to calculate code complexity. It could be used to consider software defect detection. But, not all of complexity metrics influent on the occurrence of software defect, therefore it needs feature selection to select the most influent complexity metrics. Correlation-based Feature Selection ({CFS}) is used for selecting the most influent complexity metrics. This study conducted experiments on {NASA} Metric Data Program ({MDP}) datasets. {NASA} {MDP} contains software defect history logs based on several complexity metrics. But, there is an imbalanced distribution of defective and not defective modules in {NASA} {MDP}. The distribution of defective modules is less than not defective modules. It can reduce software defect detection performance. The distribution of defective module need to be reproduced. In this study, Synthetic Minority Oversampling Technique ({SMOTE}) is used to balance the distribution between defective and not defective modules. Software defect detection using Fuzzy Association Rule Mining ({FARM}) which is combined with the selection of complexity metrics using {CFS} and dataset balancing using {SMOTE} has sensitivity 85.51\% and accuracy 91.63\% in detecting software defective modules on {NASA} {MDP} dataset.},
	pages = {330--335},
	booktitle = {2019 16th International Joint Conference on Computer Science and Software Engineering ({JCSSE})},
	author = {Naufal, Mohammad Farid and Kusuma, Selvia Ferdiana},
	date = {2019-07},
	note = {{ISSN}: 2642-6579},
	keywords = {Data mining, Feature extraction, Software, {NASA}, Measurement, Training, {CFS}, Complexity Metric, Complexity theory, Fuzzy Association Rule Mining, {SMOTE}, Software Defects, xno}
}

@inproceedings{pravin_efficient_2012,
	title = {An efficient programming rule extraction and detection of violations in software source code using neural networks},
	doi = {10.1109/ICoAC.2012.6416837},
	abstract = {The larger size and complexity of software source code builds many challenges in bug detection. Data mining based bug detection methods eliminate the bugs present in software source code effectively. Rule violation and copy paste related defects are the most concerns for bug detection system. Traditional data mining approaches such as frequent Itemset mining and frequent sequence mining are relatively good but they are lacking in accuracy and pattern recognition. Neural networks have emerged as advanced data mining tools in cases where other techniques may not produce satisfactory predictive models. The neural network is trained for possible set of errors that could be present in software source code. From the training data the neural network learns how to predict the correct output. The processing elements of neural networks are associated with weights which are adjusted during the training period.},
	pages = {1--4},
	booktitle = {2012 Fourth International Conference on Advanced Computing ({ICoAC})},
	author = {Pravin, A. and Srinivasan, S.},
	date = {2012-12},
	note = {{ISSN}: 2377-6927},
	keywords = {Data mining, Software, Computer bugs, Inspection, Biological neural networks, Data Mining, Decision Trees, Defect Detection, Neural Networks Association Rules, Programming, Programming Rule, xyes}
}

@inproceedings{gao_empirical_2019,
	title = {Empirical Study: Are Complex Network Features Suitable for Cross-Version Software Defect Prediction?},
	doi = {10.1109/ICSESS47205.2019.9040793},
	abstract = {Software defect prediction can identify possible defective software modules and improve testing efficiency. Traditional software defect prediction mainly focuses on using code features and process-based features for research. The rules of complex network are suitable for software. Using complex network features to represent defect information provides a new idea for software defect prediction. In this paper, we first select 18 versions of 9 open source projects through certain rules and then build a logistic regression model based on three kinds of features (complex network features, traditional code features, merged features) to evaluate the predictive defect ability of complex network features. The results show that: (1) Compared with traditional code features, complex network features have better ability to predict defects for cross-versions software defect prediction; (2) Merged features are not as good as complex network features in defect prediction for cross-version software defect prediction, but still better than traditional code features.},
	pages = {1--5},
	booktitle = {2019 {IEEE} 10th International Conference on Software Engineering and Service Science ({ICSESS})},
	author = {Gao, Houleng and Lu, Minyan and Pan, Cong and Xu, Biao},
	date = {2019-10},
	note = {{ISSN}: 2327-0594},
	keywords = {Feature extraction, Predictive models, Defects, Software defect prediction, Empirical studies, Open source software, Forecasting, Open source projects, Open systems, Software testing, Complex networks, Software modules, Defect prediction, Logistic regression, Logistic Regression modeling, Network features, Testing efficiency, Software systems, Logistics, Software algorithms, complex network features, cross-version software defect prediction, logistic regression, xno}
}

@article{singh_fuzzy_2017,
	title = {Fuzzy Rule-Based Approach for Software Fault Prediction},
	volume = {47},
	issn = {2168-2232},
	doi = {10.1109/TSMC.2016.2521840},
	abstract = {Knowing faulty modules prior to testing makes testing more effective and helps to obtain reliable software. Here, we develop a framework for automatic extraction of human understandable fuzzy rules for software fault detection/classification. This is an integrated framework to simultaneously identify useful determinants (attributes) of faults and fuzzy rules using those attributes. At the beginning of the training, the system assumes every attribute (feature) as a useless feature and then uses a concept of feature attenuating gate to select useful features. The learning process opens the gates or closes them more tightly based on utility of the features. Our system can discard derogatory and indifferent attributes and select the useful ones. It can also exploit subtle nonlinear interaction between attributes. In order to demonstrate the effectiveness of the framework, we have used several publicly available software fault data sets and compared the performance of our method with that of some existing methods. The results using tenfold cross-validation setup show that our system can find useful fuzzy rules for fault prediction.},
	pages = {826--837},
	number = {5},
	journaltitle = {{IEEE} Transactions on Systems, Man, and Cybernetics: Systems},
	author = {Singh, Pradeep and Pal, Nikhil R. and Verma, Shrish and Vyas, Om Prakash},
	date = {2017-05},
	keywords = {Feature extraction, Software, Software metrics, Forecasting, Learning systems, Software testing, Fault detection, Software reliability, Software fault prediction, Computer software, Fuzzy inference, Fuzzy rules, Rule generation, Software metrices, Automatic extraction, Feature modulating gates, Integrated frameworks, Nonlinear interactions, Software fault detection, machine learning, fuzzy rule generation, Logic gates, software fault prediction, software metric selection, xyes, xfuzzy, xcross-project}
}

@inproceedings{cheng_software_2011,
	title = {Software fault detection using program patterns},
	doi = {10.1109/ICSESS.2011.5982308},
	abstract = {Effective detection of software faults is an important activity of software development process. The main difficulty of detecting software fault is finding faults in a large and complex software system. In this paper, we propose an approach that applies program patterns to detect and locate software fault so that programmer can fix bug and increase software quality. The advantage of the proposed approach is that the defect-prone code segments can be detected. To facilitate the programmer to detect program bugs, this approach also includes a Graphic User Interface to locate the defect-prone code segments.},
	pages = {278--281},
	booktitle = {2011 {IEEE} 2nd International Conference on Software Engineering and Service Science},
	author = {Cheng, Ko-Li and Chang, Ching-Pao and Chu, Chih-Ping},
	date = {2011-07},
	note = {{ISSN}: 2327-0594},
	keywords = {Software quality, Software engineering, Association rules, Computer bugs, Programming, Defect Prediction, Program pattern, Program segement, xyes}
}

@inproceedings{anezakis_verification_2018,
	title = {Verification of the effectiveness of fuzzy rule-based fault prediction: A replication study},
	doi = {10.1109/INISTA.2018.8466331},
	abstract = {The prediction success of faulty modules in a software helps practitioners to plan the budget of software maintenance that leads developers to improve the reliability of software systems. Despite various learning algorithms and statistical methods, fault prediction needs novel methods for enhancing the success of the prediction. Fault prediction can be performed using fuzzy rules that are new for this field. In this work, fuzzy rule-based fault prediction approach, which was developed by Singh et al. [11], is replicated to validate the success of fuzzy rule-based fault prediction in open-source data sets. The steps of the experiment and the steps of Singh et al's work, which are applied for replication, both are same. Classification is performed after generating clusters that are constituted using fuzzy rules in normalized data sets. According to the prediction results obtained by applying 10*10 cross-validation, fuzzy rule-based fault prediction produces less errors in open-source data sets when it is compared with industrial data sets. In addition to this, the results validate the findings of Singh et al.'s work in terms of some performance parameters of the fault prediction.},
	pages = {1--8},
	booktitle = {2018 Innovations in Intelligent Systems and Applications ({INISTA})},
	author = {Anezakis, Vardis-Dimitris and Öztürk, Muhammed Maruf},
	date = {2018-07},
	keywords = {Feature extraction, Software metrics, {NASA}, Open source software, Forecasting, Classification (of information), Intelligent systems, Budget control, Software reliability, Fault prediction, Measurement, Learning algorithms, Fuzzy inference, Fuzzy rules, Performance parameters, Fault data, Fuzzy rule based, Industrial datum, Open source datum, Replication study, Prediction algorithms, fault data sets, fault prediction, Fuzzy rule, Modulation, modulator learning, software metrics, xyes}
}

@inproceedings{naufal_software_2015,
	title = {Software complexity metric-based defect classification using {FARM} with preprocessing step {CFS} and {SMOTE} a preliminary study},
	doi = {10.1109/ICITSI.2015.7437685},
	abstract = {One criteria for assessing the software quality is ensuring that there is no defect in the software which is being developed. Software defect classification can be used to prevent software defects. More earlier software defects are detected in the software life cycle, it will minimize the software development costs. This study proposes a software defect classification using Fuzzy Association Rule Mining ({FARM}) based on complexity metrics. However, not all complexity metrics affect on software defect, therefore it requires metrics selection process using Correlation-based Feature Selection ({CFS}) so it can increase the classification performance. This study will conduct experiments on the {NASA} {MDP} open source dataset that is publicly accessible on the {PROMISE} repository. This datasets contain history log of software defects based on software complexity metric. In {NASA} {MDP} dataset the data distribution between defective and not defective modules are not balanced. It is called class imbalanced problem. Class imbalance problem can affect on classification performance. It needs a technique to solve this problem using oversampling method. Synthetic Minority Oversampling Technique ({SMOTE}) is used in this study as oversampling method. With the advantages possessed by {FARM} in learning on dataset which has quantitative data attribute and combined with the software complexity metrics selection process using {CFS} and oversampling using {SMOTE}, this method is expected has a better performance than the previous methods.},
	pages = {1--6},
	booktitle = {2015 International Conference on Information Technology Systems and Innovation ({ICITSI})},
	author = {Naufal, Mohammad Farid and Rochimah, Siti},
	date = {2015-11},
	keywords = {Data mining, Feature extraction, Software, Software metrics, Computer software selection and evaluation, Defects, Software design, {NASA}, Software defects, Open source software, Learning systems, Program debugging, Association rules, Artificial intelligence, Fuzzy rules, Life cycle, Classification performance, Faulting, Bugs, Class imbalance problems, Computational complexity, Correlation based feature selections ({CFS}), Fuzzy association rule, Problem solving, Software development costs, Synthetic minority over-sampling techniques, Fault, Training, Complexity theory, Fuzzy Association Rule Mining, Correlation-based Feature Selection, Defect, Machine Learning, Software Defect Classification, Synthetic Minority Oversampling Technique, xno}
}

@inproceedings{singh_comprehensive_2017,
	title = {Comprehensive model for software fault prediction},
	doi = {10.1109/ICICI.2017.8365311},
	abstract = {Software Fault prediction ({SFP}) is an important task in the fields of software engineering to develop a cost effective software. Most of the software fault prediction is performed on same project date i.e., training and testing with same projects fault data. In case of unavailability of fault training data which is possible for the new project, data from the similar types/category of other projects can be used to train the model for the prediction. The software projects has been categorized into three categories by Boehm. The project within a certain group will be having good similarities with other projects within the group. So it is more suitable to train using the projects from same group. In this work we proposed to develop a model with similar category of data to predict the fault of another project belongs to same category. On basis of {KLOC} we have taken five organic software projects and performed various cross project and within project experiments. To generate a comprehensive generalized model for organic software's fault prediction, we have modeled various rule based to learner. Various rule-based learners used for comparison are {JRip}, {CART}, Conjunctive Rule, C4.5, {NNge}, {OneR}, Ridor, {PART}, and decision table-Naive Bayes hybrid classifier ({DTNB}).},
	pages = {1103--1108},
	booktitle = {2017 International Conference on Inventive Computing and Informatics ({ICICI})},
	author = {Singh, Pradeep},
	date = {2017-11},
	keywords = {Predictive models, Software, Testing, Cost engineering, Forecasting, Software testing, Training and testing, Training data, Fault prediction, Comprehensive model, Software fault prediction, Rule based, Cost effectiveness, Decision tables, Generalized models, Hybrid classifier, Three categories, Training, Computational modeling, Data models, Rule based Learner, xyes, xcross-project},
	file = {Singh - 2017 - Comprehensive model for software fault prediction.pdf:C\:\\Users\\michalm\\Zotero\\storage\\5EF5YECT\\Singh - 2017 - Comprehensive model for software fault prediction.pdf:application/pdf}
}

@inproceedings{mutlu_automatic_2018,
	title = {Automatic Rule Generation of Fuzzy Systems: A Comparative Assessment on Software Defect Prediction},
	doi = {10.1109/UBMK.2018.8566479},
	abstract = {Fuzzy rule base systems are expert systems rely on fuzzy set theory. Here the knowledge of human expert is transfered to the artificial model via fuzzy rules. Therefore, preciseness, completeness and coverage of fuzzy rules in a fuzzy system is vital for the accuracy and plausibility of fuzzy reasoning. However, in such cases where the human expert is unable to supply the rules sufficiently, data-based automatic rule generation methods attract attention. In this study, 2 linear and 2 evolutionary approaches of automatic fuzzy rule generation methods are investigated. The investigated linear solutions contain Wang-Mendel Method and E2E-{HFS}, while {MOGUL} and {IVTURS}-{FARC} are the selected evolutionary approaches. Wang-Mendel and {MOGUL} is commonly considered as basic methods of the group they belong to. {IVTURS}-{FARC} is distinguished with its ability to handle interval valued fuzzy sets. Among the rest of the algorithms, E2E-{HFS} is unique with its weak dependency to data. Because it only use some simple properties of corresponding input variable. In order to compare the completeness and the accuracy of automatically generated fuzzy rules, several experiments are performed on different software defect prediction datasets, and the classification performance of resulting fuzzy systems is evaluated. Provided results show that even if training of evolutionary approaches seem to be more precise, similar accuracy can be achieved by linear approaches, and they perform better regarding the experiments on unseen data.},
	pages = {209--214},
	booktitle = {2018 3rd International Conference on Computer Science and Engineering ({UBMK})},
	author = {Mutlu, Begum and Sezer, Ebru A. and Akcayol, M. Ali},
	date = {2018-09},
	keywords = {Software, Automatically generated, Defects, Software defect prediction, Forecasting, Classification (of information), Expert systems, Computer software, Fuzzy inference, Fuzzy rules, Fuzzy systems, Rule generation, Fuzzy logic, Fuzzy sets, Classification performance, Comparative assessment, Evolutionary rules, Fuzzy inference systems, Fuzzy set theory, Interval-valued fuzzy sets, Linguistics, Software Defect Prediction, Evolutionary Rule Learning, Fuzzy Inference Systems, Fuzzy Rule Generation, Genetics, xyes, xfuzzy},
	file = {Mutlu et al. - 2018 - Automatic Rule Generation of Fuzzy Systems A Comp.pdf:C\:\\Users\\michalm\\Zotero\\storage\\BCXGVN26\\Mutlu et al. - 2018 - Automatic Rule Generation of Fuzzy Systems A Comp.pdf:application/pdf}
}

@inproceedings{oyetoyan_comparison_2013,
	title = {A Comparison of Different Defect Measures to Identify Defect-Prone Components},
	doi = {10.1109/IWSM-Mensura.2013.34},
	abstract = {(Background) Defect distribution in software systems has been shown to follow the Pareto rule of 20-80. This motivates the prioritization of components with the majority of defects for testing activities. (Research goal) Are there significant variations between defective components and architectural hotspots identified by other defect measures? (Approach) We have performed a study using post-release data of an industrial Smart Grid application with a well-maintained defect tracking system. Using the Pareto principle, we identify and compare defect-prone and hotspots components based on four defect metrics. Furthermore, we validated the quantitative results against qualitative data from the developers. (Results) Our results show that at the top 25\% of the measures 1) significant variations exist between the defective components identified by the different defect metrics and that some of the components persist as defective across releases 2) the top defective components based on number of defects could only identify about 40\% of critical components in this system 3) other defect metrics identify about 30\% additional critical components 4) additional quality challenges of a component could be identified by considering the pair wise intersection of the defect metrics. (Discussion and Conclusion) Since a set of critical components in the system is missed by using largest-first or smallest-first prioritization approaches, this study, therefore, makes a case for an all-inclusive metrics during defect model construction such as number of defects, defect density, defect severity and defect correction effort to make us better understand what comprises defect-prone components and architectural hotspots, especially in critical applications.},
	pages = {181--190},
	booktitle = {2013 Joint Conference of the 23rd International Workshop on Software Measurement and the 8th International Conference on Software Process and Product Measurement},
	author = {Oyetoyan, Tosin Daniel and Conradi, Reidar and Cruzes, Daniela Soares},
	date = {2013-10},
	keywords = {Predictive models, Software, Testing, Measurement, architectural hotspots, critical system, defect correction effort, defect density, defect distribution, defect measures, defect metrics, defect severity, defect-prone component, Maintenance engineering, Object oriented modeling, Smart Grid, Smart grids, xno}
}

@inproceedings{rosli_design_2011,
	title = {The design of a software fault prone application using evolutionary algorithm},
	doi = {10.1109/ICOS.2011.6079246},
	abstract = {Most of the current project management software's are utilizing resources on developing areas in software projects. This is considerably essential in view of the meaningful impact towards time and cost-effective development. One of the major areas is the fault proneness prediction, which is used to find out the impact areas by using several approaches, techniques and applications. Software fault proneness application is an application based on computer aided approach to predict the probability that the software contains faults. The application will uses object oriented metrics and count metrics values from open source software as input values to the genetic algorithm for generation of the rules to classify the software modules in the categories of Faulty and Non Faulty modules. At the end of the process, the result will be visualized using genetic algorithm applet, bar and pie chart. This paper will discussed the detail design of software fault proneness application by using genetic algorithm based on the object oriented approach and will be presented using the Unified Modeling Language ({UML}). The aim of the proposed design is to develop an automated tool for software development group to discover the most likely software modules to be high problematic in the future.},
	pages = {338--343},
	booktitle = {2011 {IEEE} Conference on Open Systems},
	author = {Rosli, Marshima Mohd and Teo, Noor Hasimah Ibrahim and Yusop, Nor Shahida Mohamad and Mohammad, Noor Shahriman},
	date = {2011-09},
	keywords = {Software, Software design, Open source software, Forecasting, Genetic algorithms, Object oriented programming, Open systems, Software testing, Project management, Software fault, Fault prediction, Fault proneness, Measurement, Computer bugs, Application programs, Cost effectiveness, Genetic programming, Fault-proneness prediction, Computer aided-approach, Object oriented approach, Object oriented metrics, Software fault proneness, Unified Modeling Language, fault prediction, Object oriented modeling, fault proneness, software fault prone, software testing, Unified modeling language, xno}
}

@inproceedings{kaur_evaluation_2017,
	title = {Evaluation of imbalanced learning with entropy of source code metrics as defect predictors},
	doi = {10.1109/ICTUS.2017.8286041},
	abstract = {This paper evaluates imbalanced learning algorithms with entropy of source code metrics as predictor variables. Four open source software systems are studied. These systems are {ECLIPSE} {JDT}, {EQUINOX}, {MYLYN} and {ECLIPSE} {PDE} {UI}. The results of this paper indicate that imbalanced learning algorithms perform better than classical learning methods in terms of Recall, G-mean 1, G-mean 2 and F-measures. For recall measure Condensed Nearest Neighbor rule +Tomek links ({CNNTL}) perform best, for G-mean 1, G-mean 2 and F-measure Random undersampling ({RUS}) perform best.},
	pages = {403--409},
	booktitle = {2017 International Conference on Infocom Technologies and Unmanned Systems (Trends and Future Directions) ({ICTUS})},
	author = {Kaur, Kamaldeep and Name, Jasmeet Kaur and Malhotra, Jyotsana},
	date = {2017-12},
	keywords = {Defects, Open source software, Learning systems, Open systems, Class imbalance learning, Codes (symbols), Measurement, Defect prediction, Imbalanced Learning, Learning algorithms, Software systems, Computer programming languages, Condensed nearest neighbor rule, Entropy, Open source software systems, Predictor variables, Random under samplings, Source code metrics, Prediction algorithms, Software algorithms, defect prediction, class imbalance learning, entropy of source code metrics, xno}
}

@inproceedings{alhazzaa_trade-offs_2019,
	title = {Trade-Offs between Early Software Defect Prediction versus Prediction Accuracy},
	doi = {10.1109/CSCI49370.2019.00216},
	abstract = {In any software development organization, reliability is crucial. Defect prediction is key in providing management with the tools for release planning. To predict defects we ask the question of how much data is required to make usable predictions? When testing, a rule of thumb is to start defect prediction after 60\% of system test has been accomplished. In an operational phase, managers cannot usually determine what constitutes 60\% of a release and might not want to wait that long to start defect prediction. Here we discuss the trade-offs between the need of early predictions versus making more accurate predictions.},
	pages = {1144--1150},
	booktitle = {2019 International Conference on Computational Science and Computational Intelligence ({CSCI})},
	author = {Alhazzaa, Lamees and Amschler Andrews, Anneliese},
	date = {2019-12},
	keywords = {Predictive models, Software, Software design, Software defect prediction, Forecasting, Economic and social effects, Prediction accuracy, Software reliability, Artificial intelligence, Accurate prediction, Commerce, Defect prediction, Early prediction, Operational phase, Release planning, Software development organizations, Estimation, Data models, change-point, defects, estimation, Mathematical model, prediction, software reliability, xno}
}

@inproceedings{miholca_improved_2018,
	title = {An Improved Approach to Software Defect Prediction using a Hybrid Machine Learning Model},
	doi = {10.1109/SYNASC.2018.00074},
	abstract = {Software defect prediction is an intricate but essential software testing related activity. As a solution to it, we have recently proposed {HyGRAR}, a hybrid classification model which combines Gradual Relational Association Rules ({GRARs}) with {ANNs}. {ANNs} were used to learn gradual relations that were then considered in a mining process so as to discover the interesting {GRARs} characterizing the defective and non-defective software entities, respectively. The classification of a new entity based on the discriminative {GRARs} was made through a non-adaptive heuristic method. In current paper, we propose to enhance {HyGRAR} through autonomously learning the classification methodology. Evaluation experiments performed on two open-source data sets indicate that the enhanced {HyGRAR} classifier outperforms the related approaches evaluated on the same two data sets.},
	pages = {443--448},
	booktitle = {2018 20th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing ({SYNASC})},
	author = {Miholca, Diana-Lucia},
	date = {2018-09},
	keywords = {Data mining, Software, software defect prediction, Software metrics, Neural networks, Training, Machine Learning, Computational modeling, Artificial Neural Networks, Drugs, Gradual Relational Association Rules, xyes}
}

@article{riaz_rough_2018,
	title = {Rough Noise-Filtered Easy Ensemble for Software Fault Prediction},
	volume = {6},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2018.2865383},
	abstract = {Software fault prediction is the very important research topic for software quality assurance. Data-driven approaches provide robust mechanisms to deal with software fault prediction. However, the prediction performance of the model highly depends on the quality of the data set. Many software data sets suffer from the problem of class imbalance. In this regard, undersampling is a popular data pre-processing method in dealing with the class imbalance problem; easy ensemble presents a robust approach to achieve a high classification rate and address the biases toward majority class samples. However, imbalance class is not the only issue that harms the performance of classifiers. Some noisy data and irrelevant and redundant features may also reduce the performance of predictive accuracy of the classifier. In this paper, we propose two-stage data pre-processing, which incorporates feature selection and rough set-based K nearest neighbour rule ({KNN}) noise filter afore executing easy ensemble rough-{KNN} noise-filtered easy ensemble ({RKEE}). In the first stage, we eliminate the irrelevant and redundant features by the feature ranking algorithm, and in the second stage, we handle the imbalance class problem by using rough-{KNN} noise filter to eliminate noisy samples from both the minority and the majority class and also handle the uncertainty and the overlapping problem from both the minority and the majority class. Experimental evaluation on real-world software projects, such as {NASA} and Eclipse data set, is performed in order to demonstrate the effectiveness of our proposed approach. Furthermore, this paper comprehensively investigates the influencing factor in our approach, such as the impact of the rough set theory on noise-filter, the relationship between model performance and imbalance ratio, and so on. Comprehensive experiments indicate that the proposed approach shows outstanding performance with significance in terms of area-under-the-curve.},
	pages = {46886--46899},
	journaltitle = {{IEEE} Access},
	author = {Riaz, Saman and Arshad, Ali and Jiao, Licheng},
	date = {2018},
	keywords = {Data mining, Feature extraction, feature selection, Software, Computer software selection and evaluation, {NASA}, Forecasting, Classification (of information), Quality assurance, Class imbalance, Data handling, Software fault prediction, Computer software, Classification algorithm, Data preprocessing, Easy Ensemble, Filtering theory, Noise filters, Noise measurements, Personnel training, Processing, Rough set theory, Classification algorithms, Training, class imbalance, data preprocessing, easy ensemble, noise filter, Noise measurement, rough set theory, Rough sets, xno}
}

@inproceedings{li_mining_2010,
	title = {Mining Frequent Patterns from Software Defect Repositories for Black-Box Testing},
	doi = {10.1109/IWISA.2010.5473578},
	abstract = {Software defects are usually detected by inspection, black-box testing or white-box testing. Current software defect mining work focuses on mining frequent patterns without distinguishing these different kinds of defects, and mining with respect to defect type can only give limited guidance on software development due to overly broad classification of defect type. In this paper, we present four kinds of frequent patterns from defects detected by black-box testing (called black-box defect) based on a kind of detailed classification named {ODC}-{BD} (Orthogonal Defect Classification for Blackbox Defect). The frequent patterns include the top 10 conditions (data or operation) which most easily result in defects or severe defects, the top 10 defect phenomena which most frequently occur and have a great impact on users, association rules between function modules and defect types. We aim to help project managers, black-box testers and developers improve the efficiency of software defect detection and analysis using these frequent patterns. Our study is based on 5023 defect reports from 56 large industrial projects and 2 open source projects.},
	pages = {1--4},
	booktitle = {2010 2nd International Workshop on Intelligent Systems and Applications},
	author = {Li, Ning and Li, Zhanhuai and Zhang, Lijun},
	date = {2010-05},
	keywords = {Data mining, Defects, Software design, Software quality, Software defects, Open source software, Intelligent systems, Association rules, Software testing, Project management, Computer software, Black-box testing, Defect classification, Inspection, Associative processing, Frequent Itemsets, Software defect, Itemsets, Programming, Pattern analysis, Software development management, xyes, xassociation-rules}
}

@inproceedings{chai_software_2018,
	title = {A Software Defect Management System Based on Knowledge Base},
	doi = {10.1109/QRS-C.2018.00118},
	abstract = {Software testing is an effective way to improving software quality, a software defect is identified before it goes live, a massive amount of bug-related data is accumulated during software testing, there is a point in studying how to improve the working efficiency of software testing through integration and use of such data. In this thesis, a software defect management system based on knowledge base is designed, where the three-tier knowledge base architecture is used to manage defects, data mining is performed with factual knowledge generated from the testing to derive rule knowledge that will be used for defect prediction, and the appropriate strategy knowledge is configured to manage bugs, so as to improve the working efficiency of software testing.},
	pages = {652--653},
	booktitle = {2018 {IEEE} International Conference on Software Quality, Reliability and Security Companion ({QRS}-C)},
	author = {Chai, Haiyan and Zhang, Nan and Liu, Bojiang and Tang, Longli},
	date = {2018-07},
	keywords = {Data mining, Software quality, Software testing, Computer bugs, Knowledge based systems, data mining, software testing, defect management, knowledge base, xyes}
}

@inproceedings{guo_prediction_2010,
	title = {The prediction of software aging trend based on user intention},
	doi = {10.1109/YCICT.2010.5713081},
	abstract = {Owing to the limitation of traditional software aging trend prediction method that based on time and based on measurement in dealing with sudden large scale concurrent questions, this paper proposes a new software aging trend prediction method which is based on user intention. This method predicts the trend of software aging according to the quantity of user requests for each components during the moment of system operation, and the software aging damage with each component is requested once.The experiment indicates, compared with the measurement method, this method has highter accuracy in dealing with sudden large scale concurrent questions.},
	pages = {206--209},
	booktitle = {2010 {IEEE} Youth Conference on Information, Computing and Telecommunications},
	author = {Guo, Jun and Ju, Ying and Wang, Yunsheng and Li, Xianli and Zhang, Bin},
	date = {2010-11},
	keywords = {association rule mining, Software, Association rules, Aging, cumulation damage, Memory management, multivariate linear regression analysis, Servers, software aging, Software measurement, user intention prediction, xno}
}

@inproceedings{jin_software_2010,
	title = {Software Fault Prediction Model Based on Adaptive Dynamical and Median Particle Swarm Optimization},
	volume = {1},
	doi = {10.1109/MMIT.2010.11},
	abstract = {Software quality prediction can play a role of importance in software management, and thus in improve the quality of software systems. By mining software with data mining technique, predictive models can be induced that software managers the insights they need to tackle these quality problems in an efficient way. This paper deals with the adaptive dynamic and median particle swarm optimization ({ADMPSO}) based on the {PSO} classification technique. {ADMPSO} can act as a valid data mining technique to predict erroneous software modules. The predictive model in this paper extracts the relationship rules of software quality and metrics. Information entropy approach is applied to simplify the extraction rule set. The empirical result shows that this method set of rules can be streamlined and the forecast accuracy can be improved.},
	pages = {44--47},
	booktitle = {2010 Second International Conference on Multimedia and Information Technology},
	author = {Jin, Cong and Dong, En-Mei and Qin, Li-Na},
	date = {2010-04},
	keywords = {Data mining, Predictive models, Computer software selection and evaluation, Software quality, Forecasting, Classification (of information), Software Quality, Fault prediction, Particle swarm optimization ({PSO}), Software fault prediction, Software modules, Software systems, Set of rules, Classification technique, Quality of softwares, Mathematical models, Predictive control systems, Empirical results, Adaptive dynamics, Classification, Data mining techniques, Extraction rule, Forecast accuracy, Information entropy, Information technology, Mining software, Quality problems, Relationship rules, Software management, Software managers, Software quality prediction, Computer science, Conference management, Multimedia systems, Particle swarm optimization, Quality management, xyes}
}

@inproceedings{diamantopoulos_towards_2015,
	title = {Towards Interpretable Defect-Prone Component Analysis Using Genetic Fuzzy Systems},
	doi = {10.1109/RAISE.2015.13},
	abstract = {The problem of Software Reliability Prediction is attracting the attention of several researchers during the last few years. Various classification techniques are proposed in current literature which involve the use of metrics drawn from version control systems in order to classify software components as defect-prone or defect-free. In this paper, we create a novel genetic fuzzy rule-based system to efficiently model the defect-proneness of each component. The system uses a Mamdani-Assilian inference engine and models the problem as a one-class classification task. System rules are constructed using a genetic algorithm, where each chromosome represents a rule base (Pittsburgh approach). The parameters of our fuzzy system and the operators of the genetic algorithm are designed with regard to producing interpretable output. Thus, the output offers not only effective classification, but also a comprehensive set of rules that can be easily visualized to extract useful conclusions about the metrics of the software.},
	pages = {32--38},
	booktitle = {2015 {IEEE}/{ACM} 4th International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering},
	author = {Diamantopoulos, Themistoklis and Symeonidis, Andreas},
	date = {2015-05},
	keywords = {Software, Defects, Forecasting, Genetic algorithms, Software engineering, Software component, Software reliability, Artificial intelligence, Software fault prediction, Measurement, Fuzzy inference, Fuzzy systems, Fuzzy logic, Classification technique, Component analysis, Genetic fuzzy systems, One-class Classification, Pittsburgh approach, Version control system, software fault prediction, Genetics, defect-prone components, genetic fuzzy systems, Sociology, Software Reliability Prediction, xyes}
}

@inproceedings{chun-sheng_extension_2014,
	title = {Extension of local association rules mining algorithm based on apriori algorithm},
	doi = {10.1109/ICSESS.2014.6933577},
	abstract = {The support is generally higher when the classical apriori algorithm is used as mining data based on association rules, if the support is small low then redundant frequent item set and redundant rules are produced large, so the local effective association rules has a larger confidence and a smaller support can not be mined out, which is the fatal defects of the classical apriori algorithm. According to the defects, the effectiveness of local rules is proved at first, meanwhile, two kinds of the correction algorithms are given: the one is apriori-con algorithm based on confidence and the other is apriori algorithm based on classification which is further divided into three kinds, apriori-class-int algorithm based on interest classification, apriori-class-pre algorithm based on forecast classification and apriori-class-clr algorithm based on clustering classification. The correctness of the theory is proved in the article and the effective of the correction algorithms is showed by cases.},
	pages = {340--343},
	booktitle = {2014 {IEEE} 5th International Conference on Software Engineering and Service Science},
	author = {Chun-Sheng, Zhang and Yan, Li},
	date = {2014-06},
	note = {{ISSN}: 2327-0594},
	keywords = {Clustering algorithms, Association rules, Algorithm design and analysis, Classification algorithms, Itemsets, Apriori algorithm, association rules, defect, extended algorithm, Medical services, xno}
}

@inproceedings{liu_research_2019,
	title = {Research on Fault Prediction and Health Management Technology Based on Machine Learning},
	doi = {10.1109/QR2MSE46217.2019.9021182},
	abstract = {With the continuous development of fault detection and identification technology of complex equipment systems, the traditional fault identification technology has been slightly insufficient. Now artificial intelligence is applied to the field of large-scale information system fault identification. In order to find and judge information system faults more accurately, this paper proposes a fault identification method based on machine learning. Firstly, the fault data characteristics are analyzed and the fault information identification method is obtained. Then, based on the fault information feature words, the algorithm for correlating invalid rules is obtained. Based on the association failure model, the fault identification algorithm is obtained. Finally, the information system fault data management and fault identification prototype system is developed.},
	pages = {789--794},
	booktitle = {2019 International Conference on Quality, Reliability, Risk, Maintenance, and Safety Engineering ({QR}2MSE)},
	author = {Liu, Bojiang and Wu, Lijin and Han, Xinyu and Tang, Longli},
	date = {2019-08},
	keywords = {Predictive models, Software, Machine learning, Fault tolerance, machine learning, Training, fault prediction, Data models, association failure rule, Fault tolerant systems, feature word extraction, information system, xyes}
}

@article{cai_design_2019,
	title = {Design Rule Spaces: A New Model for Representing and Analyzing Software Architecture},
	volume = {45},
	issn = {1939-3520},
	doi = {10.1109/TSE.2018.2797899},
	abstract = {In this paper, we propose an architecture model called Design Rule Space ({DRSpace}). We model the architecture of a software system as multiple overlapping {DRSpaces}, reflecting the fact that any complex software system must contain multiple aspects, features, patterns, etc. We show that this model provides new ways to analyze software quality. In particular, we introduce an Architecture Root detection algorithm that captures {DRSpaces} containing large numbers of a project's bug-prone files, which are called Architecture Roots ({ArchRoots}). After investigating {ArchRoots} calculated from 15 open source projects, the following observations become clear: from 35 to 91 percent of a project's most bug-prone files can be captured by just 5 {ArchRoots}, meaning that bug-prone files are likely to be architecturally connected. Furthermore, these {ArchRoots} tend to live in the system for significant periods of time, serving as the major source of bug-proneness and high maintainability costs. Moreover, each {ArchRoot} reveals multiple architectural flaws that propagate bugs among files and this will incur high maintenance costs over time. The implication of our study is that the quality, in terms of bug-proneness, of a large, complex software project cannot be fundamentally improved without first fixing its architectural flaws.},
	pages = {657--682},
	number = {7},
	journaltitle = {{IEEE} Transactions on Software Engineering},
	author = {Cai, Yuanfang and Xiao, Lu and Kazman, Rick and Mo, Ran and Feng, Qiong},
	date = {2019-07},
	keywords = {Computer software selection and evaluation, Open source software, Program debugging, Reverse engineering, Code smell, Computer architecture, Technical debts, Defect prediction, Analytical models, Bug localizations, Computer bugs, Production facility, Software architecture, Software systems, defect prediction, bug localization, code smells, Production facilities, reverse-engineering, technical debt, xyes}
}

@inproceedings{watanabe_identifying_2016,
	title = {Identifying recurring association rules in software defect prediction},
	doi = {10.1109/ICIS.2016.7550867},
	abstract = {Association rule mining discovers patterns of co-occurrences of attributes as association rules in a data set. The derived association rules are expected to be recurrent, that is, the patterns recur in future in other data sets. This paper defines the recurrence of a rule, and aims to find a criteria to distinguish between high recurrent rules and low recurrent ones using a data set for software defect prediction. An experiment with the Eclipse Mylyn defect data set showed that rules of lower than 30 transactions showed low recurrence. We also found that the lower bound of transactions to select high recurrence rules is dependent on the required precision of defect prediction.},
	pages = {1--6},
	booktitle = {2016 {IEEE}/{ACIS} 15th International Conference on Computer and Information Science ({ICIS})},
	author = {Watanabe, Takashi and Monden, Akito and Kamei, Yasutaka and Morisaki, Shuji},
	date = {2016-06},
	keywords = {association rule mining, Data mining, Computer software selection and evaluation, Defects, Software quality, Software defect prediction, Empirical studies, Forecasting, Association rules, Software Quality, Measurement, Defect prediction, Computer software, Data set, Co-occurrence, Information science, Lower bounds, Required precision, data mining, defect prediction, empirical study, software quality, Decision support systems, xno}
}

@inproceedings{fan_high-frequency_2018,
	title = {High-Frequency Keywords to Predict Defects for Android Applications},
	volume = {02},
	doi = {10.1109/COMPSAC.2018.10273},
	abstract = {Android defect prediction has proved to be useful to reduce the manual testing effort for finding bugs. In recent years, researchers design metrics related to defects and analyze historical information to predict whether files contain defects using machine learning. However, those models learn to predict defects based on the characteristics of programs while ignoring the internal information, e.g., the functional and semantic information within the source code. This paper proposes a model, {HIRER}, to learn the functional and semantic information to predict whether files contain defects automatically for Android applications. Specifically, {HIRER} learns internal information within the source code based on the high-frequency keywords extracted from programs' Abstract Syntax Trees ({ASTs}). It gets rule-based programming patterns from high-frequency keywords and uses Deep Belief Network ({DBN}), a deep neutral network, to learn functional and semantic features from the programming patterns. We implement a defect testing system with five machine learning techniques based on {HIRER} to predict defective files in source code automatically. Then, we apply it on four open source Android applications. The results show that learned functional and semantic features can predict more defects than traditional metrics. In different versions of {MMS}, Gallery2, Bluetooth, Calendar open source applications, {HIRER} improves the {AUC} of the predicted results respectively in average.},
	pages = {442--447},
	booktitle = {2018 {IEEE} 42nd Annual Computer Software and Applications Conference ({COMPSAC})},
	author = {Fan, Yaqing and Cao, Xinya and Xu, Jing and Xu, Sihan and Yang, Hongji},
	date = {2018-07},
	note = {{ISSN}: 0730-3157},
	keywords = {Feature extraction, Predictive models, Defects, Open source software, Forecasting, Machine learning techniques, Semantics, Trees (mathematics), Codes (symbols), Artificial intelligence, Measurement, Defect prediction, Application programs, Abstract Syntax Trees, Android, Android (operating system), Computer programming languages, Deep belief network ({DBN}), Deep learning, Functional programming, High frequency {HF}, Historical information, Open source application, defect prediction, Programming, Androids, high-frequency, Humanoid robots, xno}
}

@inproceedings{lenarduzzi_are_2020,
	title = {Are {SonarQube} Rules Inducing Bugs?},
	doi = {10.1109/SANER48275.2020.9054821},
	abstract = {The popularity of tools for analyzing Technical Debt, and particularly the popularity of {SonarQube}, is increasing rapidly. {SonarQube} proposes a set of coding rules, which represent something wrong in the code that will soon be reflected in a fault or will increase maintenance effort. However, our local companies were not confident in the usefulness of the rules proposed by {SonarQube} and contracted us to investigate the fault-proneness of these rules. In this work we aim at understanding which {SonarQube} rules are actually fault-prone and to understand which machine learning models can be adopted to accurately identify fault-prone rules. We designed and conducted an empirical study on 21 well-known mature open-source projects. We applied the {SZZ} algorithm to label the fault-inducing commits. We analyzed the fault-proneness by comparing the classification power of seven machine learning models. Among the 202 rules defined for Java by {SonarQube}, only 25 can be considered to have relatively low fault-proneness. Moreover, violations considered as “bugs” by {SonarQube} were generally not fault-prone and, consequently, the fault-prediction power of the model proposed by {SonarQube} is extremely low. The rules applied by {SonarQube} for calculating technical debt should be thoroughly investigated and their harmfulness needs to be further confirmed. Therefore, companies should carefully consider which rules they really need to apply, especially if their goal is to reduce fault-proneness.},
	pages = {501--511},
	booktitle = {2020 {IEEE} 27th International Conference on Software Analysis, Evolution and Reengineering ({SANER})},
	author = {Lenarduzzi, Valentina and Lomio, Francesco and Huttunen, Heikki and Taibi, Davide},
	date = {2020-02},
	note = {{ISSN}: 1534-5351},
	keywords = {machine learning, code smells, architectural smells, coding style, {SonarQube}, static analysis, Technical Debt, xyes, xsonarqube},
	file = {Lenarduzzi et al. - 2020 - Are SonarQube Rules Inducing Bugs.pdf:C\:\\Users\\michalm\\Zotero\\storage\\2YIQLJ3W\\Lenarduzzi et al. - 2020 - Are SonarQube Rules Inducing Bugs.pdf:application/pdf}
}

@inproceedings{karre_defect_2015,
	title = {A defect dependency based approach to improve software quality in integrated software products},
	abstract = {Integrated software products are complex in design. They are prone to defects caused by integrated and non-integrated modules of the entire integrated software suite. In such software products, a small proportion of defects are fixed as soon as they are reported. Rest of the defects are targeted for fixes in future product release cycles. Among such targeted defects, most of them seem to be insignificant and innocuous in the current version but have the potential to become acute in future versions. In this paper, we propose an approach to study defect dependency of the reported defect using a dependency metric. Identifying the dependency of a defect in an integrated product suite can help the product stake-owners to prioritize them and help improve software quality.},
	pages = {110--117},
	booktitle = {2015 International Conference on Evaluation of Novel Approaches to Software Engineering ({ENASE})},
	author = {Karre, Sai Anirudh and Reddy, Y. Raghu},
	date = {2015-04},
	keywords = {Data mining, Software, Testing, Computer software selection and evaluation, Defects, Software products, Classification (of information), Software Quality, Measurement, Product design, Dependency metric, Integrated module, Integrated products, Integrated software, Integrated software suite, Rule-based classification, Training, Defect Dataset, Defect Dependency, Dependency Metric, Influenza, Integrated Software Products, Rule-based Classification, xno}
}

@inproceedings{shaikh_attribute_2020,
	title = {Attribute Rule performance in Data Mining for Software Deformity Prophecy Datasets Models},
	doi = {10.1109/AECT47998.2020.9194187},
	abstract = {In recently, all the developers, programmer and software engineers, they are working specially on software component and software testing to compete the software technology in the world. For this competition, they are using different kind of sources to analysis the software reliability and importance. Nowadays Data mining is one of source, which is used in software for overcome the problem of software fault which occur during the software test and its analysis. This kind of problem leads software deformity prophecy in software. In this research paper, we are also trying to overcome the software deformity prophecy problem with the help of our proposed solution called {ONER} rule attribute. We have used {REPOSITORY} datasets models, these datasets models are defected and non-defected datasets models. Our analysis class of interest is defected models. In our research, we have analyzed the efficiency of our proposed solution methods. The experiments results showed that using of {ONER} with discretize, have improved the efficiency of correctly classified instances in all. Using percentage split and training datasets with {ONER} discretize rule attribute have improved correctly classified in all datasets models. The analysis of positive accuracy f-measure is also increased in percentage split during the use of {ONER} with discretize but in some datasets models, the training data and cross validation is better with use of {ONER} rule attribute. The area under curve ({ROC}) in both scenarios using {ONER} rule attribute and discretize with {ONER} rule attribute is almost same or equal with each other.},
	pages = {1--6},
	booktitle = {2019 International Conference on Advances in the Emerging Computing Technologies ({AECT})},
	author = {Shaikh, Salahuddin and Changan, Liu and Malik, Maaz Rasheed},
	date = {2020-02},
	keywords = {Data mining, Software, Testing, Measurement, Data Mining, Machine Learning, Data models, Object oriented modeling, Attribute Rule, Datasets Model, Deformable models, {ONER}, Software Deformity Prophecy, xyes},
	file = {Shaikh et al. - 2020 - Attribute Rule performance in Data Mining for Soft.pdf:C\:\\Users\\michalm\\Zotero\\storage\\LSYLLI85\\Shaikh et al. - 2020 - Attribute Rule performance in Data Mining for Soft.pdf:application/pdf}
}

@inproceedings{ba_propred_2012,
	title = {{ProPRED}: A probabilistic model for the prediction of residual defects},
	doi = {10.1109/MESA.2012.6275569},
	abstract = {In this paper, we propose {ProPRED}, a probabilistic model for predicting residual defects based on Bayesian Networks ({BN}) in the software development lifecycle. With the chain rule for {BN}, {ProPRED} can be used to take the evidence of the influential factors to the activities (Analyze and Design, Development, Maintain, and Review and Test) that bring about the defects introduction and removal to reason and predict the probable residual defects. We refine and classify the influential factors to the four basic activities, and construct the {ProPRED}. Giving a case study, we conclude that the {ProPRED} improve its performance in reasoning under uncertainty and convenience in decision-making and quality control.},
	pages = {247--251},
	booktitle = {Proceedings of 2012 {IEEE}/{ASME} 8th {IEEE}/{ASME} International Conference on Mechatronic and Embedded Systems and Applications},
	author = {Ba, Jie and Wu, Shujian},
	date = {2012-07},
	keywords = {Predictive models, Software, Object oriented modeling, Bayesian methods, Bayesian Network, Cognition, defect prediction method, Gaussian distribution, influential factor, probabilistic model, Probability distribution, residual defect, xno}
}

@inproceedings{mutlu_new_2019,
	title = {A New Fuzzy Rule Generation Scheme based on Multiple-Selection of Influencing Factors},
	doi = {10.1109/FUZZ-IEEE.2019.8858990},
	abstract = {Automatic rule generation of fuzzy systems is a extensively studied topic which is generally handled by using a labeled dataset. This paper presents a new method for this problem which does not rely on training data. It only requests a tiny information which can be roughly acquired from any data and/or domain expert. By using this information, it selects single or multiple influencing factor(s) to determine the consequent part of rules. The experiments were performed on software fault prediction problem, and the resulting rules are compared with the rules obtained from 3 existing rule generation methods which are data-based, expert-based and partially data-based solutions. Results shows that, the proposed method is able to outperform its counterparts in most of the cases.},
	pages = {1--6},
	booktitle = {2019 {IEEE} International Conference on Fuzzy Systems ({FUZZ}-{IEEE})},
	author = {Mutlu, Begum and Sezer, Ebru A.},
	date = {2019-06},
	note = {{ISSN}: 1558-4739},
	keywords = {Software, Testing, Machine learning, Training data, Fuzzy systems, Fuzzy sets, Linguistics, software fault prediction, Fuzzy rule generation, fuzzy systems, xyes, xrbs},
	file = {Mutlu and Sezer - 2019 - A New Fuzzy Rule Generation Scheme based on Multip.pdf:C\:\\Users\\michalm\\Zotero\\storage\\BHGT3R57\\Mutlu and Sezer - 2019 - A New Fuzzy Rule Generation Scheme based on Multip.pdf:application/pdf}
}

@inproceedings{pandey_test_2010,
	title = {Test effort optimization by prediction and ranking of fault-prone software modules},
	doi = {10.1109/ICRESH.2010.5779531},
	abstract = {Identification of fault-prone or not fault-prone modules is very essential to improve the reliability and quality of a software system. Once modules are categorized as fault-prone or not fault-prone, test effort are allocated accordingly. Testing effort and efficiency are primary concern and can be optimized by prediction and ranking of fault-prone modules. This paper discusses a new model for prediction and ranking of fault-prone software modules for test effort optimization. Model utilizes the classification capability of data mining techniques and knowledge stored in software metrics to classify the software module as fault-prone or not fault-prone. A decision tree is constructed using {ID}3 algorithm for the existing project data. Rules are derived form the decision tree and integrated with fuzzy inference system to classify the modules as either fault-prone or not fault-prone for the target data. The model is also able to rank the fault-prone module on the basis of its degree of fault-proneness. The model accuracy are validated and compared with some other models by using the {NASA} projects data set of {PROMOSE} repository.},
	pages = {136--142},
	booktitle = {2010 2nd International Conference on Reliability, Safety and Hazard - Risk-Based Technologies and Physics-of-Failure Methods ({ICRESH})},
	author = {Pandey, Ajeet Kumar and Goyal, Neeraj Kumar},
	date = {2010-12},
	keywords = {software metrics, software testing, fault-prone modules, fuzzy inference system ({FIS}), {ID}3 algorithm, xno}
}

@inproceedings{wang_top-k_2018,
	title = {A Top-k Learning to Rank Approach to Cross-Project Software Defect Prediction},
	doi = {10.1109/APSEC.2018.00048},
	abstract = {Cross-project defect prediction ({CPDP}) has recently attracted increasing attention in the field of Software Engineering. Most of the previous studies, which treated it as a binary classification problem or a regression problem, are not practical for software testing activities. To provide developers with a more valuable ranking of the most severe entities (e.g., classes and modules), in this paper, we propose a top-k learning to rank ({LTR}) approach in the scenario of {CPDP}. In particular, we first convert the number of defects into graded relevance to a specific query according to the three-sigma rule; then, we put forward a new data resampling method called {SMOTE}-{PENN} to tackle the imbalanced data problem. An empirical study on the {PROMISE} dataset shows that {SMOTE}-{PENN} outperforms the other six competitive resampling algorithms and {RankNet} performs the best for the proposed approach framework. Thus, our work could lay a foundation for efficient search engines for top-ranked defective entities in real software testing activities without local historical data for a target project.},
	pages = {335--344},
	booktitle = {2018 25th Asia-Pacific Software Engineering Conference ({APSEC})},
	author = {Wang, Feng and Huang, Jinxiao and Ma, Yutao},
	date = {2018-12},
	note = {{ISSN}: 2640-0715},
	keywords = {Predictive models, Software, Software engineering, Software testing, Training data, Computer bugs, relevance, Computer science, resampling, top-k ranking, transfer learning, Wilcoxon signed-rank test, xno}
}

@inproceedings{ibarguren_consolidated_2017,
	title = {The Consolidated Tree Construction algorithm in imbalanced defect prediction datasets},
	doi = {10.1109/CEC.2017.7969629},
	abstract = {In this short paper, we compare well-known rule/tree classifiers in software defect prediction with the {CTC} decision tree classifier designed to deal with class imbalanced. It is well-known that most software defect prediction datasets are highly imbalance (non-defective instances outnumber defective ones). In this work, we focused only on tree/rule classifiers as these are capable of explaining the decision, i.e., describing the metrics and thresholds that make a module error prone. Furthermore, rules/decision trees provide the advantage that they are easily understood and applied by project managers and quality assurance personnel. The {CTC} algorithm was designed to cope with class imbalance and noisy datasets instead of using preprocessing techniques (oversampling or undersampling), ensembles or cost weights of misclassification. The experimental work was carried out using the {NASA} datasets and results showed that induced {CTC} decision trees performed better or similar to the rest of the rule/tree classifiers.},
	pages = {2656--2660},
	booktitle = {2017 {IEEE} Congress on Evolutionary Computation ({CEC})},
	author = {Ibarguren, Igor and Pérez, Jesús M. and Mugerza, Javier and Rodriguez, Daniel and Harrison, Rachel},
	date = {2017-06},
	keywords = {Software, {NASA}, Decision trees, Measurement, Algorithm design and analysis, Prediction algorithms, Software algorithms, xyes},
	file = {Submitted Version:C\:\\Users\\michalm\\Zotero\\storage\\KMXS79CF\\Ibarguren et al. - 2017 - The Consolidated Tree Construction algorithm in im.pdf:application/pdf}
}

@inproceedings{danphitsanuphan_code_2012,
	title = {Code Smell Detecting Tool and Code Smell-Structure Bug Relationship},
	doi = {10.1109/SCET.2012.6342082},
	abstract = {This paper proposes an approach for detecting the so- called bad smells in software known as Code Smell. In considering software bad smells, object-oriented software metrics were used to detect the source code whereby Eclipse Plugins were developed for detecting in which location of Java source code the bad smell appeared so that software refactoring could then take place. The detected source code was classified into 7 types: Large Class, Long Method, Parallel Inheritance Hierarchy, Long Parameter List, Lazy Class, Switch Statement, and Data Class. This work conducted analysis by using 323 java classes to ascertain the relationship between the code smell and structural defects of software by using the data mining techniques of Naive Bayes and Association Rules. The result of the Naive Bayes test showed that the Lazy Class caused structural defects in {DLS}, {DE}, and Se. Also, Data Class caused structural defects in {UwF}, {DE}, and Se, while Long Method, Large Class, Data Class, and Switch Statement caused structural defects in {UwF} and Se. Finally, Parallel Inheritance Hierarchy caused structural defects in Se. However, Long Parameter List caused no structural defects whatsoever. The results of the Association Rules test found that the Lazy Class code smell caused structural defects in {DLS} and {DE}, which corresponded to the results of the Naive Bayes test.},
	pages = {1--5},
	booktitle = {2012 Spring Congress on Engineering and Technology},
	author = {Danphitsanuphan, Phongphan and Suwantada, Thanitta},
	date = {2012-05},
	keywords = {Software, Software metrics, Association rules, Computer bugs, Educational institutions, Java, xyes, xassociation-rules},
	file = {Danphitsanuphan and Suwantada - 2012 - Code Smell Detecting Tool and Code Smell-Structure.pdf:C\:\\Users\\michalm\\Zotero\\storage\\IHL3YBSW\\Danphitsanuphan and Suwantada - 2012 - Code Smell Detecting Tool and Code Smell-Structure.pdf:application/pdf}
}

@inproceedings{sun_improving_2010,
	title = {Improving the Precision of Dependence-Based Defect Mining by Supervised Learning of Rule and Violation Graphs},
	doi = {10.1109/ISSRE.2010.37},
	abstract = {Previous work has shown that application of graph mining techniques to system dependence graphs improves the precision of automatic defect discovery by revealing subgraphs corresponding to implicit programming rules and to rule violations. However, developers must still confirm, edit, or discard reported rules and violations, which is both costly and error-prone. In order to reduce developer effort and further improve precision, we investigate the use of supervised learning models for classifying and ranking rule and violation subgraphs. In particular, we present and evaluate logistic regression models for rules and violations, respectively, which are based on general dependence-graph features. Our empirical results indicate that (i) use of these models can significantly improve the precision and recall of defect discovery, and (ii) our approach is superior to existing heuristic approaches to rule and violation ranking and to an existing static-warning classifier, and (iii) accurate models can be learned using only a few labeled examples.},
	pages = {1--10},
	booktitle = {2010 {IEEE} 21st International Symposium on Software Reliability Engineering},
	author = {Sun, Boya and Podgurski, Andy and Ray, Soumya},
	date = {2010-11},
	note = {{ISSN}: 2332-6549},
	keywords = {Data mining, Computer software selection and evaluation, Defects, Precision and recall, Quality assurance, Heuristic methods, Software reliability, Measurement, Logistic regression, Computer bugs, Regression analysis, Supervised learning, Defect classification, Rule violation, Logistic regression models, Logistics, Error prones, Computer systems programming, Defect discovery, Dependence graphs, Empirical results, Graph features, Graph mining, Heuristic approach, Ranking rules, Subgraphs, System dependence graph, Classification algorithms, Programming, logistic regression, Computational modeling, defect classification, defect mining, dependence graph, xno}
}

@inproceedings{anwar_using_2012,
	title = {Using Association Rules to Identify Similarities between Software Datasets},
	doi = {10.1109/QUATIC.2012.66},
	abstract = {A number of V\&V datasets are publicly available. These datasets have software measurements and defectiveness information regarding the software modules. To facilitate V\&V, numerous defect prediction studies have used these datasets and have detected defective modules effectively. Software developers and managers can benefit from the existing studies to avoid analogous defects and mistakes if they are able to find similarity between their software and the software represented by the public datasets. This paper identifies the similar datasets by comparing association patterns in the datasets. The proposed approach finds association rules from each dataset and identifies the overlapping rules from the 100 strongest rules from each of the two datasets being compared. Afterwards, average support and average confidence of the overlap is calculated to determine the strength of the similarity between the datasets. This study compares eight public datasets and results show that {KC}2 and {PC}2 have the highest similarity 83\% with 97\% support and 100\% confidence. Datasets with similar attributes and almost same number of attributes have shown higher similarity than the other datasets.},
	pages = {114--119},
	booktitle = {2012 Eighth International Conference on the Quality of Information and Communications Technology},
	author = {Anwar, Saba and Rana, Zeeshan Ali and Shamail, Shafay and Awais, Mian M.},
	date = {2012-09},
	keywords = {Defects, Software engineering, Association rules, Software developer, Software modules, Defect prediction, Association patterns, dataset similarity, Software Measurement, Software measures, defect prediction, association rules, software measures, xno}
}

@inproceedings{lopes_margarido_classification_2011,
	title = {Classification of defect types in requirements specifications: Literature review, proposal and assessment},
	abstract = {Requirements defects have a major impact throughout the whole software lifecycle. Having a specific defects classification for requirements is important to analyse the root causes of problems, build checklists that support requirements reviews and to reduce risks associated with requirements problems. In our research we analyse several defects classifiers; select the ones applicable to requirements specifications, following rules to build defects taxonomies; and assess the classification validity in an experiment of requirements defects classification performed by graduate and undergraduate students. Not all subjects used the same type of defect to classify the same defect, which suggests that defects classification is not consensual. Considering our results we give recommendations to industry and other researchers on the design of classification schemes and treatment of classification results.},
	pages = {1--6},
	booktitle = {6th Iberian Conference on Information Systems and Technologies ({CISTI} 2011)},
	author = {Lopes Margarido, Isabel and Faria, João Pascoal and Vidal, Raul Moreira and Vieira, Marco},
	date = {2011-06},
	note = {{ISSN}: 2166-0735},
	keywords = {Software, Testing, Defects, Software engineering, Software life cycles, software, Specifications, Taxonomies, Inspection, Classification of defects, Classification results, Classification scheme, Defects classification, Information systems, Literature reviews, requirements, Requirements specifications, Root cause, Students, Support requirements, Undergraduate students, Programming, defects, classification, Documentation, taxonomy, Taxonomy, xno}
}

@inproceedings{falessi_towards_2015,
	title = {Towards an open-source tool for measuring and visualizing the interest of technical debt},
	doi = {10.1109/MTD.2015.7332618},
	abstract = {Current tools for managing technical debt are able to report the principal of the debt, i.e., the amount of effort required to fix all the quality rules violated in a project. However, they do not report the interest, i.e., the disadvantages the project had or will have due to quality rules violations. As a consequence, the user lacks support in understanding how much the principal should be reduced and why. We claim that information about the interest is, at least, as important as the information about the principal; the interest should be quantified and treated as a first-class entity like the principal. In this paper we aim to advance the state of the art of how the interest is measured and visualized. The goal of the paper is to describe {MIND}, an open-source tool which is, to the best of our knowledge, the first tool supporting the quantification and visualization of the interest. {MIND}, by analyzing historical data coming from Redmine and Git repositories, reports the interest incurring in a software project in terms of how many extra defects occurred, or will occur, due to quality rules violations. We evaluated {MIND} by using it to analyze a software project stored in a dataset of more than a million lines of code. Results suggest that {MIND} accurately measures the interest of technical debt.},
	pages = {1--8},
	booktitle = {2015 {IEEE} 7th International Workshop on Managing Technical Debt ({MTD})},
	author = {Falessi, Davide and Reichel, Andreas},
	date = {2015-10},
	keywords = {Predictive models, Open source software, Monitoring, Mathematical model, technical debt, Current measurement, Data visualization, defect proneness, interest, maintainability, quality rules, xno}
}

@inproceedings{liu_rule_2010,
	title = {Rule Engine based on improvement Rete algorithm},
	doi = {10.1109/ICACIA.2010.5709916},
	abstract = {Rete algorithm is the mainstream of the algorithm in the rules engine; it provides efficient local entities data and rules pattern matching method. But Rete algorithm applied in rules engine has defects in performance and demand aspect, this paper applies three methods to improve the Rete algorithm in the rule engine: rule decomposition, Alpha-Node-Hashing and Beta-Node-Indexing. Rules engine is enterprise applications, business logic framework for extracting rules from software application, and make it more flexible. This paper describes the principle of Rule Engine at first, secondly show the Rete algorithm, and finally we do some improvement to Rete algorithm. It makes Rule Engine widely meet the application demand and be greatly improved the efficiency.},
	pages = {346--349},
	booktitle = {The 2010 International Conference on Apperceiving Computing and Intelligence Analysis Proceeding},
	author = {Liu, Di and Gu, Tao and Xue, Jiang-Ping},
	date = {2010-12},
	keywords = {Pattern matching, Algorithm design and analysis, Classification algorithms, Software algorithms, Alpha-Node-Hashing, Beta-Node-Indexing, Business, Engines, Production, Rete algorithm, rule decomposition, Rule Engine, xyes}
}

@article{menzies_local_2013,
	title = {Local versus Global Lessons for Defect Prediction and Effort Estimation},
	volume = {39},
	issn = {1939-3520},
	doi = {10.1109/TSE.2012.83},
	abstract = {Existing research is unclear on how to generate lessons learned for defect prediction and effort estimation. Should we seek lessons that are global to multiple projects or just local to particular projects? This paper aims to comparatively evaluate local versus global lessons learned for effort estimation and defect prediction. We applied automated clustering tools to effort and defect datasets from the {PROMISE} repository. Rule learners generated lessons learned from all the data, from local projects, or just from each cluster. The results indicate that the lessons learned after combining small parts of different data sources (i.e., the clusters) were superior to either generalizations formed over all the data or local lessons formed from particular projects. We conclude that when researchers attempt to draw lessons from some historical data source, they should 1) ignore any existing local divisions into multiple sources, 2) cluster across all available data, then 3) restrict the learning of lessons to the clusters from other sources that are nearest to the test data.},
	pages = {822--834},
	number = {6},
	journaltitle = {{IEEE} Transactions on Software Engineering},
	author = {Menzies, Tim and Butcher, Andrew and Cok, David and Marcus, Andrian and Layman, Lucas and Shull, Forrest and Turhan, Burak and Zimmermann, Thomas},
	date = {2013-06},
	keywords = {Data mining, Software, Measurement, clustering, Estimation, defect prediction, Data models, Java, Context, effort estimation, Telecommunications, xno}
}

@article{lin_coded_2018,
	title = {Coded Quickest Classification With Applications in Bandwidth-Efficient Smart Grid Monitoring},
	volume = {13},
	issn = {1556-6021},
	doi = {10.1109/TIFS.2018.2837658},
	abstract = {Cyber-physical systems, such as smart grids, have received lots of attention recently. Unfortunately, security breaches in cyber-physical systems can result in catastrophic consequences, thus needing to be carefully monitored. For example, abnormal voltage quality events, which are more likely to happen because of unstable renewable energy sources in smart grids, harm delicate electronic devices. We thus focus on the quickest classification, or multi-hypothesis quickest change detection, which jointly detects and classifies multiple abnormal events. Both the classification delay and misclassification probability need to be low. Multiple smart meters are adopted, where each meter transmits its local decision to a fusion center for making the final decision. For energy saving, the bandwidth (link capacity) between each meter and the fusion center is limited to be one bit. Moreover, some meters may be faulty and mislead the final decision. To combat these faulty meters under the limited bandwidth, a code-based framework for quickest classification is proposed. Our contribution is two-fold. First, a new local decision rule based on the stochastic ordering theory is proposed. Compared with existing matrix-cumulative-sums algorithm, the newly proposed local decision rule has lower complexity and comparable performance. Second, a new fusion method based on codebook switching and minimum Hamming distance rule is developed. Compared with existing fault-tolerant methods, the newly-developed method can significantly lower the misclassification probabilities.},
	pages = {3122--3136},
	number = {12},
	journaltitle = {{IEEE} Transactions on Information Forensics and Security},
	author = {Lin, Shih-Chun and Liu, Chien-Chi and Hsieh, Min-Yen and Su, Shih-Tang and Chung, Wei-Ho},
	date = {2018-12},
	keywords = {Fault tolerance, Monitoring, Smart grids, Fault tolerant systems, Delays, distributed systems, error-correcting codes, Indexes, Meters, Multi-hypothesis quickest detection, xno}
}

@article{xiaolong_rfc_2021,
	title = {{RFC}: A feature selection algorithm for software defect prediction},
	volume = {32},
	issn = {1004-4132},
	doi = {10.23919/JSEE.2021.000032},
	abstract = {Software defect prediction ({SDP}) is used to perform the statistical analysis of historical defect data to find out the distribution rule of historical defects, so as to effectively predictdefects in the new software. However, there are redundant and irrelevant features in the software defect datasets affecting the performance of defect predictors. In order to identify and remove the redundant and irrelevant features in software defectdatasets, we propose Relief F-based clustering ({RFC}), a cluster-based feature selection algorithm. Then, the correlation between features is calculated based on the symmetric uncertainty. According to the correlation degree, {RFC} partitions features into kclusters based on the k-medoids algorithm, and finally selects the representative features from each cluster to form the final feature subset. In the experiments, we compare the proposed {RFC} with classical feature selection algorithms on nine National Aeronautics and Space Administration ({NASA}) software defectprediction datasets in terms of area under curve ({AUC}) and F-value. The experimental results show that {RFC} can effectively improve the performance of {SDP}.},
	pages = {389--398},
	number = {2},
	journaltitle = {Journal of Systems Engineering and Electronics},
	author = {Xiaolong, Xu and Wen, Chen and Xinheng, Wang},
	date = {2021-04},
	keywords = {Clustering algorithms, Feature extraction, feature selection, Software, Defects, Based clustering, Correlation between features, Correlation degree, Distribution rule, Feature selection algorithm, K-medoids algorithms, {NASA}, Software defect prediction, Software defects, Systems engineering, Prediction algorithms, Software algorithms, cluster, Correlation, Partitioning algorithms, software defect prediction ({SDP}), xno}
}

@inproceedings{gainaru_taming_2012,
	title = {Taming of the Shrew: Modeling the Normal and Faulty Behaviour of Large-scale {HPC} Systems},
	doi = {10.1109/IPDPS.2012.107},
	abstract = {{HPC} systems are complex machines that generate a huge volume of system state data called "events". Events are generated without following a general consistent rule and different hardware and software components of such systems have different failure rates. Distinguishing between normal system behaviour and faulty situation relies on event analysis. Being able to detect quickly deviations from normality is essential for system administration and is the foundation of fault prediction. As {HPC} systems continue to grow in size and complexity, mining event flows become more challenging and with the upcoming 10 Pet flop systems, there is a lot of interest in this topic. Current event mining approaches do not take into consideration the specific behaviour of each type of events and as a consequence, fail to analyze them according to their characteristics. In this paper we propose a novel way of characterizing the normal and faulty behaviour of the system by using signal analysis concepts. All analysis modules create {ELSA} (Event Log Signal Analyzer), a toolkit that has the purpose of modelling the normal flow of each state event during a {HPC} system lifetime, and how it is affected when a failure hits the system. We show that these extracted models provide an accurate view of the system output, which improves the effectiveness of proactive fault tolerance algorithms. Specifically, we implemented a filtering algorithm and short-term fault prediction methodology based on the extracted model and test it against real failure traces from a large-scale system. We show that by analyzing each event according to its specific behaviour, we get a more realistic overview of the entire system.},
	pages = {1168--1179},
	booktitle = {2012 {IEEE} 26th International Parallel and Distributed Processing Symposium},
	author = {Gainaru, Ana and Cappello, Franck and Kramer, William},
	date = {2012-05},
	note = {{ISSN}: 1530-2075},
	keywords = {Data mining, Predictive models, Fault detection, Fault prediction, Fault tolerance, Analytical models, Complex machines, Distributed parameter networks, Entire system, Event analysis, Event mining, Failure rate, Filtering algorithm, Hardware and software components, Normal flow, Proactive fault, Signal analysis, Signal analyzers, System administration, System output, System state, Prediction algorithms, Correlation, fault detection, fault tolerance, large-scale {HPC} systems, Large-scale systems, signal analysis, xno}
}

@inproceedings{xianyong_expert_2010,
	title = {An expert system for comprehensive diagnosis in store management system},
	volume = {2},
	doi = {10.1109/ICACC.2010.5486652},
	abstract = {Aiming at the condition that the fault mechanism of a store management system is complicated and it is difficult for diagnosis, an expert system for the equipment based on both neural networks and generate rules is discussed. At first, expert experience including fault phenomenons and reasons are summarized. Considering that expert knowledge is miscellaneous and is contact with too many parts, a new coding strategy is proposed by adopting Classification processing methods. {NN} is the primary tool of establishing Inference model and generate rules are mainly used for assistance, fault diagnosis algorithm of the expert system is designed. Then the expert system is realized based on Matlab Guide tool, results of the simulation proved the methods effective.},
	pages = {371--374},
	booktitle = {2010 2nd International Conference on Advanced Computer Control},
	author = {Xianyong, Jing and Zhanchen, Liu and Zenghui, Xie and Yingchun, Li},
	date = {2010-03},
	keywords = {Neural networks, Algorithm design and analysis, Fault diagnosis, Mathematical model, Design engineering, Diagnostic expert systems, Displays, Engineering management, Error correction, expert system, fault diagnosis, Inference algorithms, Neural Network({NN}), rule, Store Management System({SMS}), xno}
}

@inproceedings{xu_novel_2013,
	title = {A novel fuzzy classification to enhance software regression testing},
	doi = {10.1109/CIDM.2013.6597217},
	abstract = {An effective system regression testing for consecutive releases of very large software systems, such as modern telecommunications systems, depends considerably on the selection of test cases for execution. Classification models can classify, early in the test planning phase, those test cases that are likely to detect faults in the upcoming regression test. Due to the high uncertainties in regression test, classification models based on fuzzy logic are very useful. Recently, methods have been proposed for automatically generating fuzzy if-then rules by applying complicated rule generation procedures to numerical data. In this research, we introduce and demonstrate a new rule-based fuzzy classification ({RBFC}) modeling approach as a method for identifying high effective test cases. The modeling approach, based on test case metrics and the proposed rule generation technique, is applied to extracting fuzzy rules from numerical data. In addition, it also provides a convenient way to modify rules according to the costs of different misclassification errors. We illustrate our modeling technique with a case study of large-scale industrial software systems and the results showed that test effectiveness and efficiency was significantly improved.},
	pages = {53--58},
	booktitle = {2013 {IEEE} Symposium on Computational Intelligence and Data Mining ({CIDM})},
	author = {Xu, Zhiwei and Liu, Yi and Gao, Kehan},
	date = {2013-04},
	keywords = {Software, Testing, Software engineering, Fuzzy sets, Educational institutions, Equations, Numerical models, xno}
}

@inproceedings{dautov_technique_2018,
	title = {A technique to aggregate classes of analog fault diagnostic data based on association rule mining},
	doi = {10.1109/ISQED.2018.8357294},
	abstract = {Analog circuits are widely used in different fields such as medicine, military, aviation and are critical for the development of reliable electronic systems. Testing and diagnosis are important tasks which detect and localize defects in the circuit under test as well as improve quality of the final product. Output responses of fault-free and faulty behavior of analog circuit can be represented by infinite set of values due to tolerances of internal components. The data mining methods may improve quality of fault diagnosis in the case of big data processing. The technique of aggregation the classes of fault diagnostic responses, based on association rule mining, is proposed. The technique corresponds to the simulation before test concept: a fault dictionary is generated by collecting the coefficients of wavelet transformation for fault-free and faulty conditions as the preprocessing of output signals. Classificator is based on k-nearest neighbors method (k-{NN}) and association rule mining algorithm. The fault diagnostic technique was trained and tested using data obtained after simulation of fault-free and faulty behavior of the analog filter. In result the accuracy in classifying faulty conditions and fault coverage have consisted of more than 99,09\% and more than 99,08\% correspondingly. The proposed technique is completely automated and can be extended.},
	pages = {238--243},
	booktitle = {2018 19th International Symposium on Quality Electronic Design ({ISQED})},
	author = {Dautov, Ruslan and Mosin, Sergey},
	date = {2018-03},
	keywords = {Data mining, Testing, Circuit faults, Computational modeling, Analog circuits, Integrated circuit modeling, Monte Carlo methods, xno}
}

@article{ding_online_2018,
	title = {Online Failure Prediction for Railway Transportation Systems Based on Fuzzy Rules and Data Analysis},
	volume = {67},
	issn = {1558-1721},
	doi = {10.1109/TR.2018.2828113},
	abstract = {Nowadays, software systems have been more and more complex, which causes great challenges to maintain the availability of the systems. Online failure prediction provides an effective approach to guaranteeing the validity of the systems. Most of the current technologies for online failure prediction require some prior knowledge, such as the model of the system or failure patterns. This paper proposes a new method based on fuzzy rules and time series analysis. Specifically, fuzzy rules are used to model the relationships among different variables, whereas univariate time series analysis is used to describe the evolution of each variable. Thus, for a dependent variable, we have two predicted values: one is from the time series model, and the other is computed from fuzzy rules with fuzzy inference. If the difference between the two values exceeds a threshold, then we declare that there would be a failure in some time period ahead. Different from the existing methods, the proposed method considers not only the evolutionary trend of each variable but also the relationships among different variables. Moreover, we do not need any prior knowledge such as system model or failure patterns. We use a railway transportation system as an example to illustrate our method.},
	pages = {1143--1158},
	number = {3},
	journaltitle = {{IEEE} Transactions on Reliability},
	author = {Ding, Zuohua and Zhou, Yuan and Pu, Geguang and Zhou, {MengChu}},
	date = {2018-09},
	keywords = {Predictive models, Time series analysis, Hidden Markov models, Autoregressive integrated moving average ({ARIMA}), Autoregressive processes, Data analysis, failure prediction, fuzzy rules, Rail transportation, railway transportation system, time series analysis, xno}
}

@inproceedings{wang_cascading_2020,
	title = {Cascading Failure Path Prediction based on Association Rules in Cyber-Physical Active Distribution Networks},
	doi = {10.1109/QRS-C51114.2020.00083},
	abstract = {Cascading failures may lead to large scale outages, which brings about significant economic losses and serious social impacts. It is very important to predict cross-domain cascading failures paths for identification of weak nodes, which contributes to the control policies for preventing cascading failures and blocking their propagation between cyber domain and physical domain in cyber-physical active distribution networks. This paper proposes an algorithm based on the Frequent-Patterns-Growth ({FP}-Growth) to predict cascading failure paths, which predicts the potential failure node set by analyzing a large number of simulation datum and mining the hidden association relationship among datum. To demonstrate the effectiveness of the proposed cascading failure path prediction approach, an empirical study on a cyber-physical active distribution network, named {CEPRI}-{CPS} from Electric Power Research Institute of China, is performed, and the result shows the robustness of cyber-physical active distribution networks can be improved with prediction approach in this paper.},
	pages = {458--464},
	booktitle = {2020 {IEEE} 20th International Conference on Software Quality, Reliability and Security Companion ({QRS}-C)},
	author = {Wang, Chong and Dong, Yunwei and Sun, Pengpeng and Lu, Yin},
	date = {2020-12},
	keywords = {Predictive models, Software reliability, Power system protection, Analytical models, Economics, Prediction algorithms, association rules, cascading failure path prediction, cyber-physical active distribution networks, {FP}-Growth algorithm, Power system faults, xno}
}

@article{xu_defect_2021,
	title = {Defect Prediction With Semantics and Context Features of Codes Based on Graph Representation Learning},
	volume = {70},
	issn = {1558-1721},
	doi = {10.1109/TR.2020.3040191},
	abstract = {To optimize the process of software testing and to improve software quality and reliability, many attempts have been made to develop more effective methods for predicting software defects. Previous work on defect prediction has used machine learning and artificial software metrics. Unfortunately, artificial metrics are unable to represent the features of syntactic, semantic, and context information of defective modules. In this article, therefore, we propose a practical approach for identifying software defect patterns via the combination of semantics and context information using abstract syntax tree representation learning. Graph neural networks are also leveraged to capture the latent defect information of defective subtrees, which are pruned based on a fix-inducing change. To validate the proposed approach for predicting defects, we define mining rules based on the {GitHub} workflow and collect 6052 defects from 307 projects. The experiments indicate that the proposed approach performs better than the state-of-the-art approach and five traditional machine learning baselines. An ablation study shows that the information about code concepts leads to a significant increase in accuracy.},
	pages = {613--625},
	number = {2},
	journaltitle = {{IEEE} Transactions on Reliability},
	author = {Xu, Jiaxi and Wang, Fei and Ai, Jun},
	date = {2021-06},
	keywords = {Data mining, Software, software engineering, Semantics, Measurement, Computer bugs, Deep learning, defect prediction, Software development management, graph representation learning, software defect dataset, Syntactics, xyes}
}

@inproceedings{liu_multidimensional_2017,
	title = {A multidimensional time-series association rules algorithm based on spark},
	doi = {10.1109/FSKD.2017.8393066},
	abstract = {Fault prediction of industrial systems has been a hot research orientation in recent years, which allows the maintainer to know the operation conditions and the fault to be occurred in advance so as to reduce the risk of fault and the economic loss. In general, association rules learning is one of the most effective methods in fault prediction of industrial systems, however, traditional methods based on association rules are not suitable for sparse time-series data that are common in industrial systems (e.g. transmission line data). Although some methods based on clustering to reduce the dimension of data have been proposed, these methods may lose some of the key rules from the dataset and reduce the effectiveness of the results. In order to solve the problem, we propose a novel algorithm called Multidimensional Time-series Association Rules({MTAR}) in this paper, which can fully utilize the information and find out more valuable rules from multidimensional time-series data. Meanwhile, we implement the parallelization of the algorithm based on the parallel computing framework Spark, which can improve the performance of the algorithm greatly. Experiments are conducted on the transmission line dataset in Power Grid System to show the effectiveness and the efficiency of the proposed approach.},
	pages = {1946--1952},
	booktitle = {2017 13th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery ({ICNC}-{FSKD})},
	author = {Liu, {DongYue} and Wu, Bin and Gu, Chao and Ma, Yan and Wang, Bai},
	date = {2017-07},
	keywords = {Clustering algorithms, Data mining, Predictive models, Prediction algorithms, Data models, association rules, multidimensional time-series data, parallel computing framework, Power Grid System, Power grids, Power transmission lines, xno}
}

@inproceedings{liu_coded_2017,
	title = {Coded Quickest Classification for Multiple Power Quality Events in Smart Grid},
	doi = {10.1109/GLOCOM.2017.8254457},
	abstract = {The goal of the smart grid is to develop a more reliable, secure, and environmentally friendly power grid. Unfortunately, power quality ({PQ}) events are more likely to happen due to unstable renewable energy sources in smart grids. We thus focus on the quickest classification, or multihypothesis quickest change detection, which jointly detects and classifies multiple abnormal {PQ} events. Both the classification delay and misclassification probability are aimed to be minimized. Multiple smart meters in the grid are used, where each meter transmits its local decision to a fusion center for making final decisions. For energy saving, the capacity between each meter and the fusion center is limited to be one bit. Moreover, some meters may be faulty and misleading the final decision. To combat these faulty meters under limited link capacity, a code- based framework for quickest classification is proposed. Our contribution is twofold. First, new local decision rule based on stochastic ordering theory is proposed, which has lower complexity and competing performance compared with existing matrix Cumulative Sums ({CUSUM}). Second, a new fusion method based on codebook switching and minimum Hamming distance rule is developed, which can significantly lower the misclassification probability.},
	pages = {1--6},
	booktitle = {{GLOBECOM} 2017 - 2017 {IEEE} Global Communications Conference},
	author = {Liu, Chien-Chi and Lin, Shih-Chun and Chung, Wei-Ho},
	date = {2017-12},
	keywords = {Fault tolerance, Power quality, Monitoring, Smart grids, Fault tolerant systems, Indexes, Meters, xno}
}

@article{souri_formal_2020,
	title = {Formal Verification of a Hybrid Machine Learning-Based Fault Prediction Model in Internet of Things Applications},
	volume = {8},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2020.2967629},
	abstract = {By increasing the complexity of the Internet of Things ({IoT}) applications, fault prediction become an important challenge in interactions between human, and smart devices. Fault prediction is one of the key factors to achieve better arranging the {IoT} applications. Most of the current research studies evaluated the fault prediction methods using simulation environments. However, formal verification of the correctness of a fault prediction method has not been reported yet. This paper presents a behavioral modeling and formal verification of a hybrid machine learning-based fault prediction model with Multi-Layer Perceptron ({MLP}) and Particle Swarm Optimization ({PSO}) algorithms. In particular, the {PSO} is used for feature selection. Then, the fault prediction is considered as a behavior to be verified formally. The fault prediction behavior is divided into two types of behaviors: dimension reduction behavior and prediction behavior. For each of the behaviors, one formal model is designed. The behavioral models designed are mapped into the Labeled Transition System ({LTS}). The Process Analysis Toolkit ({PAT}) model checker is employed to evaluate the behavioral models. The accuracy of the fault prediction method is done by some existing specifications such as deadlock-free and reachability properties in terms of linear temporal logic formulas. Also, the verification of the fault prediction behaviors is used to detect the defect metrics of information-centric {IoT} applications. Experimental results showed that our proposed verification method has minimum verification time and memory usage for evaluating critical specification rules than other research studies.},
	pages = {23863--23874},
	journaltitle = {{IEEE} Access},
	author = {Souri, Alireza and Mohammed, Amin Salih and Yousif Potrus, Moayad and Malik, Mazhar Hussain and Safara, Fatemeh and Hosseinzadeh, Mehdi},
	date = {2020},
	keywords = {Predictive models, Software, Testing, Measurement, fault prediction, formal verification, Internet of Things, Internet of Things applications, Machine learning algorithms, multi-layer perceptron, particle swarm optimization, process analysis toolkit, xno}
}

@inproceedings{chen_inspection_2013,
	title = {Inspection flow of yield impacting systematic defects},
	doi = {10.1109/eMDC.2013.6756065},
	abstract = {Yield impacting systematic defects finding is no longer just relied on Design Rule Checking ({DRC}) provided by designer or Lithography Rule Checking ({LRC}) provided by post-optical proximity correction ({OPC}) results. An inspection flow is proposed in this paper, which is combining the inspection {KLA} tool and Hotspot Pattern Analyzer ({HPA}) database software to do the systematic defects filtering, sorting, grouping, and classification on the data base after hot scan inspection. 2nd time high sensitive inspection is done with new care area, which is reduced into one ten-thousandth of original inspection area. Following this inspection flow, we can identify the process window more accuracy.},
	pages = {1--3},
	booktitle = {2013 e-Manufacturing Design Collaboration Symposium ({eMDC})},
	author = {Chen, Chimin and Yang, {ChengHua} and Liao, Hsiang-Chou and Luoh, Tuung and Yang, Ling-Wu and Yang, Tahone and Chen, Kuang-Chao and Lu, Chih-Yuan and Liu, Donghua and Fan, Jeff and Lv, Rong},
	date = {2013-09},
	keywords = {Defects, Integrated circuit layout, Manufacture, Database software, Design rule checking, Finite element method, Hot Scan, Inspection, Inspection flow, Pattern Grouping, Photolithography, Proximity correction, {PWQ}, Systematic defects, {FEM}, Finite element analysis, Joints, Lead, Systematic Defect, Systematics, xno}
}

@inproceedings{gowda_false_2018,
	title = {False Positive Analysis of Software Vulnerabilities Using Machine Learning},
	doi = {10.1109/CCEM.2018.00010},
	abstract = {Dynamic Application Security Testing is conducted with the help of automated tools that have built-in scanners which automatically crawl all the webpages of the application and report security vulnerabilities based on certain set of pre-defined scan rules. Such pre-defined rules cannot fully determine the accuracy of a vulnerability and very often one needs to manually validate these results to remove the false positives. Eliminating false positives from such results can be a quite painful and laborious task. This article proposes an approach of eliminating false positives by using machine learning . Based on the historic data available on false positives, suitable machine learning models are deployed to predict if the reported defect is a real vulnerability or a false positive},
	pages = {3--6},
	booktitle = {2018 {IEEE} International Conference on Cloud Computing in Emerging Markets ({CCEM})},
	author = {Gowda, Sumanth and Prajapati, Divyesh and Singh, Ranjit and Gadre, Swanand S.},
	date = {2018-11},
	keywords = {Predictive models, Software, Testing, Learning systems, Decision trees, Machine learning, Machine learning models, Software vulnerabilities, Commerce, Automated tools, Cloud computing, Dynamic applications, False positive, Security of data, Security vulnerabilities, Software security, vulnerabilities, Prediction algorithms, Machine Learning, Data models, False Positive Analysis, Software Security, xno}
}

@inproceedings{elberzhager_optimizing_2014,
	title = {Optimizing Quality Assurance Strategies through an Integrated Quality Assurance Approach – Guiding Quality Assurance with Assumptions and Selection Rules},
	doi = {10.1109/SEAA.2014.12},
	abstract = {Quality assurance activities are often still expensive or do not offer the expected quality. A recent trend aimed at overcoming this problem is tighter integration of several quality assurance techniques such as analysis and testing in order to exploit synergy effects and thus reduce costs or improve the coverage of quality assurance activities. However, one main challenge in exploiting such benefits is that knowledge about the relationships between many different factors is needed, such as the quality assurance techniques considered, the number of defects, the remaining defect-proneness, or product and budget data. Such knowledge is often not available. Based on a combined analysis and testing methodology called In {QA}, we developed an iterative rule-based procedure that considers several factors in order to gather knowledge and allows deriving different strategies to guide the quality assurance activities. We derived several specific and reasonable strategies to demonstrate the approach.},
	pages = {402--405},
	booktitle = {2014 40th {EUROMICRO} Conference on Software Engineering and Advanced Applications},
	author = {Elberzhager, Frank and Bauer, Thomas},
	date = {2014-08},
	note = {{ISSN}: 2376-9505},
	keywords = {Testing, Defects, Forecasting, Quality assurance, Iterative methods, Software engineering, Budget control, Application programs, Knowledge based systems, Defect proneness, Integration testing, Analysis and testing, Quality control, Integration, analysis, assumptions, Combined analysis, Electronic guidance systems, Integrated quality, Quality assurance strategies, rules, Inspection, prediction, Context, Calibration, Concrete, guidance, integration, testing, tool prototype, xyes}
}

@inproceedings{herzig_empirically_2015,
	title = {Empirically Detecting False Test Alarms Using Association Rules},
	volume = {2},
	doi = {10.1109/ICSE.2015.133},
	abstract = {Applying code changes to software systems and testing these code changes can be a complex task that involves many different types of software testing strategies, e.g. system and integration tests. However, not all test failures reported during code integration are hinting towards code defects. Testing large systems such as the Microsoft Windows operating system requires complex test infrastructures, which may lead to test failures caused by faulty tests and test infrastructure issues. Such false test alarms are particular annoying as they raise engineer attention and require manual inspection without providing any benefit. The goal of this work is to use empirical data to minimize the number of false test alarms reported during system and integration testing. To achieve this goal, we use association rule learning to identify patterns among failing test steps that are typically for false test alarms and can be used to automatically classify them. A successful classification of false test alarms is particularly valuable for product teams as manual test failure inspection is an expensive and time-consuming process that not only costs engineering time and money but also slows down product development. We evaluating our approach on system and integration tests executed during Windows 8.1 and Microsoft Dynamics {AX} development. Performing more than 10,000 classifications for each product, our model shows a mean precision between 0.85 and 0.90 predicting between 34\% and 48\% of all false test alarms.},
	pages = {39--48},
	booktitle = {2015 {IEEE}/{ACM} 37th {IEEE} International Conference on Software Engineering},
	author = {Herzig, Kim and Nagappan, Nachiappan},
	date = {2015-05},
	note = {{ISSN}: 1558-1225},
	keywords = {Testing, Cost engineering, Software engineering, Association rules, Software testing, Alarm systems, Codes (symbols), Software systems, Product development, Integration testing, Empirical data, Engineering time, Integration, Manual inspection, Mean precision, Microsoft windows, Safety engineering, Software testing strategies, Test infrastructures, Windows operating system, Inspection, Manuals, xno}
}

@inproceedings{wang_understanding_2021,
	title = {Understanding and Facilitating the Co-Evolution of Production and Test Code},
	doi = {10.1109/SANER50967.2021.00033},
	abstract = {Software products frequently evolve. When the production code undergoes major changes such as feature addition or removal, the corresponding test code typically should co-evolve. Otherwise, the outdated test may be ineffective in revealing faults or cause spurious test failures, which could confuse developers and waste {QA} resources. Despite its importance, maintaining such co-evolution can be time- and resource-consuming. Existing work has disclosed that, in practice, test code often fails to co-evolve with the production code. To facilitate the co-evolution of production and test code, this work explores how to automatically identify outdated tests. To gain insights into the problem, we conducted an empirical study on 975 open-source Java projects. By manually analyzing and comparing the positive cases, where the test code co-evolves with the production code, and the negative cases, where the co-evolution is not observed, we found that various factors (e.g., the different language constructs modified in the production code) can determine whether the test code should be updated. Guided by the empirical findings, we proposed a machine-learning based approach, {SITAR}, that holistically considers different factors to predict test changes. We evaluated {SITAR} on 20 popular Java projects. These results show that {SITAR}, under the within-project setting, can reach an average precision and recall of 81.4\% and 76.1\%, respectively, for identifying test code that requires update, which significantly outperforms rule-based baseline methods. {SITAR} can also achieve promising results under the cross-project setting and multiclass prediction, which predicts the exact change types of test code.},
	pages = {272--283},
	booktitle = {2021 {IEEE} International Conference on Software Analysis, Evolution and Reengineering ({SANER})},
	author = {Wang, Sinan and Wen, Ming and Liu, Yepang and Wang, Ying and Wu, Rongxin},
	date = {2021-03},
	note = {{ISSN}: 1534-5351},
	keywords = {Feature extraction, Testing, Software products, Baseline methods, Empirical findings, Empirical studies, Gain insight, Java programming language, Language constructs, Multiclass prediction, Open source software, Precision and recall, Reengineering, Turing machines, Machine learning, Semantics, Java, Production, Syntactics, Conferences, mining software repositories, Software evolution, test maintenance, xno}
}

@inproceedings{chen_effects_2016,
	title = {Effects of online fault detection mechanisms on Probabilistic Timing Analysis},
	doi = {10.1109/DFT.2016.7684067},
	abstract = {In real time systems, random caches have been proposed as a way to simplify software timing analysis, by avoiding corner cases usually found in deterministic systems. Using this random approach, one can obtain an application's probabilistic Worst Case Execution Time ({pWCET}) to be used for timing analysis. As with deterministic systems, technology scaling in cache memories is making transient and permanent faults more likely, which in turn affects the system's timing behavior. To mitigate these effects, one can introduce a detection mechanism that classifies a fault as transient or permanent, with the goal of disabling permanently faulty cache blocks to avoid future accesses. In this paper, we compare the effects of two online detection mechanisms for permanent faults, namely rule-based detection and Dynamic Hidden Markov Model (D-{HMM}) based detection, for the generation of safe {pWCET} estimates. Experimental results show that different mechanisms can greatly affect safe {pWCET} margins, and that by using D-{HMM} the {pWCET} of the system can be improved compared to rule-based detection.},
	pages = {41--46},
	booktitle = {2016 {IEEE} International Symposium on Defect and Fault Tolerance in {VLSI} and Nanotechnology Systems ({DFT})},
	author = {Chen, Chao and Panerati, Jacopo and Beltrame, Giovanni},
	date = {2016-09},
	note = {{ISSN}: 2377-7966},
	keywords = {Defects, Fault detection, Fault tolerance, Cache memory, Different mechanisms, Hidden Markov models, Interactive computer systems, Real time systems, Rule based detection, Software timing analysis, Timing circuits, Transient and permanent fault, Worst-case execution time, Reliability, Detection mechanism, Deterministic systems, Markov processes, Nanotechnology, On-line fault detection, {VLSI} circuits, Benchmark testing, Biological system modeling, Silicon, xno}
}

@inproceedings{zhu_stream_2011,
	title = {Stream prediction using representative episode rules},
	doi = {10.1109/ICDMW.2011.160},
	abstract = {Stream prediction based on episode rules of the form "whenever a series of antecedent event types occurs, another series of consequent event types appears eventually"has received intensive attention due to its broad applications such as reading sequence forecasting, stock trend analyzing, road traffic monitoring, and software fault preventing. Many previous works focus on the task of discovering a full set of episode rules or matching a single predefined episode rule, little emphasis has been attached to the systematic methodology of stream prediction. This paper fills the gap by constructing an efficient and effective episode predictor over an event stream which works on a three-step process of rule extracting, rule matching and result reporting. Aiming at this goal, we first propose an algorithm Extractor to extract all representative episode rules based on frequent closed episodes and their generators, then we introduce an approach Matcher to simultaneously match multiple episode rules by finding the latest minimal and non-overlapping occurrences of their antecedents, and finally we devise a strategy Reporter to report each prediction result containing a prediction interval and a series of event types. Experiments on both synthetic and real-world datasets demonstrate that our methods are efficient and effective in the stream environment.},
	pages = {307--314},
	booktitle = {2011 {IEEE} 11th International Conference on Data Mining Workshops},
	author = {Zhu, Huisheng and Wang, Peng and Wang, Wei and Shi, Baile},
	date = {2011-12},
	note = {{ISSN}: 2375-9259},
	keywords = {Data mining, Itemsets, Prediction algorithms, Automata, Frequency measurement, Frequent closed episode, Generator, Generators, Iron, Minimal and non-overlapping occurrence, Representative episode rule, Stream prediction, xno}
}

@inproceedings{meena_detection_2018,
	title = {Detection and Classification of Power Quality Disturbances Using Stockwell Transform and Rule Based Decision Tree},
	doi = {10.1109/ICSEDPS.2018.8536064},
	abstract = {This manuscript describes an approach based on Stockwell Transform and rule based decision tree for detection and classification of single stage power quality ({PQ}) disturbances. Power quality disturbances are generated in {MATLAB} software using mathematical relations in conformity with the {IEEE} Standard-1159. The single stage power quality disturbances like sag in voltage, swell in voltage, momentary interruption ({MI}), harmonics, impulsive transient ({IT}), oscillatory transient ({OT}) and notch are studied in the presented research work. Pure sine wave is taken as reference for detection of {PQ} disturbances. Various plots of signals with {PQ} disturbances have been obtained and features extracted from these disturbances are given as input to the rule based decision tree for classification purpose. Effectiveness of proposed algorithm is established by testing 30 data sets of each {PQ} events obtained by varying the parameters.},
	pages = {227--232},
	booktitle = {2018 International Conference on Smart Electric Drives and Power System ({ICSEDPS})},
	author = {Meena, Mahaveer and Mahela, Om Prakash and Kumar, Mahendra and Kumar, Neeraj},
	date = {2018-06},
	keywords = {Feature extraction, Power quality, Harmonic analysis, power quality disturbance, rule based decision tree, Stockwell transform, Transforms, Transient analysis, Voltage fluctuations, xno}
}

@inproceedings{wu_evolutionary_2011,
	title = {An evolutionary approach to evaluate the quality of software systems},
	doi = {10.1109/IWACI.2011.6160036},
	abstract = {Modern software applications are characterized as large, complex, and component-based systems. These applications can be viewed as modeling solutions that are created to cope with daily living in both the public and the private organizations, as well as in every business enterprise. A model solution is subject to evolutionary improvement; the more the improvement, the better the quality of software. An improvement can be carried out by means of defect prediction at the component level of the software systems. This paper discusses an evolutionary computing approach to model defects in complex adaptive software systems based on mathematical elements, graphs, sets, and rough sets, in addition to domain specific rules that are necessary for defect collections and defect analyses. This approach is applied to the evaluation of software quality, and it is fundamental for automation of such an evaluation tool.},
	pages = {381--386},
	booktitle = {The Fourth International Workshop on Advanced Computational Intelligence},
	author = {Wu, Binghui Helen},
	date = {2011-10},
	keywords = {Software quality, Analytical models, Software architecture, Software systems, Programming, Rough sets, xyes},
	file = {Wu - 2011 - An evolutionary approach to evaluate the quality o.pdf:C\:\\Users\\michalm\\Zotero\\storage\\NIWVQNDM\\Wu - 2011 - An evolutionary approach to evaluate the quality o.pdf:application/pdf}
}

@article{lin_tas-model-based_2018,
	title = {A {TAS}-Model-Based Algorithm for Rule Redundancy Detection and Scene Scheduling in Smart Home Systems},
	volume = {12},
	issn = {1937-9234},
	doi = {10.1109/JSYST.2017.2771349},
	abstract = {In smart home systems, users enjoy the comfort and convenience by rule subscription and execution. However, with the increase in the complex and the number of rules, there is an increased risk of redundancy within the process of rule customization and execution. Obviously, redundancy will add weight to the system, affect the administrative operation, and reduce the system efficiency. To address the above-mentioned issues, a formal model Trigger-Actuator-Status for rules is devised. Such rules are defined as a tuple, which contains triggers, actuators, and states. Then, rules are processed with the methods of classification, combination, and analytics to generate redundant types so as to describe the redundancy relationship among rules. After that, a redundancy detection algorithm is proposed according to the determined relationship. Besides, in order to eliminate and avoid the occurrence of redundancy, redundancy for scenarios based on rule redundancy is detected and solved. The experimental results show that the proposed scheme can provide more accurate classification results and identify part or full redundancy within and among rules. Meanwhile, our scheme enables redundancy elimination in the phase of rule setting and scenario setup as well as redundancy avoidance in the stage of rule execution and scenario start-up, which significantly reduces the number of rules to greatly enhance the efficiency of the systems.},
	pages = {3018--3029},
	number = {3},
	journaltitle = {{IEEE} Systems Journal},
	author = {Lin, Zhaowen and Wu, Tin-Yu and Sun, Yan and Xu, Jie and Obaidat, Mohammad S.},
	date = {2018-09},
	keywords = {Redundancy, Actuators, Algorithm design and analysis, Dataset preprocessing, Sensors, Smart homes, Wireless sensor networks, machine learning, Databases, software defect predict, software metric, software quality assurance, xno}
}

@inproceedings{xu_classification_2020,
	title = {The Classification and Propagation of Program Comments},
	abstract = {Natural language comments are like bridges between human logic and software semantics. Developers use comments to describe the function, implementation, and property of code snippets. This kind of connections contains rich information, like the potential types of a variable and the pre-condition of a method, among other things. In this paper, we categorize comments and use natural language processing techniques to extract information from them. Based on the semantics of programming languages, different rules are built for each comment category to systematically propagate comments among code entities. Then we use the propagated comments to check the code usage and comments consistency. Our demo system finds 37 bugs in real-world projects, 30 of which have been confirmed by the developers. Except for bugs in the code, we also find 304 pieces of defected comments. The 12 of them are misleading and 292 of them are not correct. Moreover, among the 41573 pieces of comments we propagate, 87 comments are for private native methods which had neither code nor comments. We also conduct a user study where we find that propagated comments are as good as human-written comments in three dimensions of consistency, naturalness, and meaningfulness.},
	pages = {1394--1396},
	booktitle = {2020 35th {IEEE}/{ACM} International Conference on Automated Software Engineering ({ASE})},
	author = {Xu, Xiangzhe},
	date = {2020-09},
	note = {{ISSN}: 2643-1572},
	keywords = {Data mining, Software, Program debugging, Extract informations, {NAtural} language processing, Natural language processing systems, Natural languages, Real world projects, Semantics, Semantics of programming languages, Software engineering, Three dimensions, User study, Computer bugs, comment, Computer languages, Natural language processing, programm analysis, xno}
}

@inproceedings{prior_you_2011,
	title = {"You Want to do What?" Breaking the Rules to Increase Customer Satisfaction},
	doi = {10.1109/AGILE.2011.15},
	abstract = {Customer satisfaction. Every customer support organization strives to increase and maintain a higher level of customer satisfaction. This is a story of how one software organization attempted to increase customer satisfaction by increasing predictability and response times to reported defects. Along the way they were able to establish trust with the customer support organization by implementing a process that seemed counterintuitive to its stakeholders.},
	pages = {269--273},
	booktitle = {2011 Agile Conference},
	author = {Prior, Mike},
	date = {2011-08},
	keywords = {Software, Measurement, Agile development, Customer satisfaction, Production, Lead, backlogs, cycle times, Organizations, queues, software defect management, work in progress, Writing, xno}
}

@article{rajapaksha_sqaplanner_2021,
	title = {{SQAPlanner}: Generating Data-Informed Software Quality Improvement Plans},
	issn = {1939-3520},
	doi = {10.1109/TSE.2021.3070559},
	abstract = {Software Quality Assurance ({SQA}) planning aims to define proactive plans, such as defining maximum file size, to prevent the occurrence of software defects in future releases. To aid this, defect prediction models have been proposed to generate insights as the most important factors that are associated with software quality. Such insights that are derived from traditional defect models are far from actionable—i.e., practitioners still do not know what they should do or avoid to decrease the risk of having defects, and what is the risk threshold for each metric. A lack of actionable guidance and risk threshold can lead to inefficient and ineffective {SQA} planning processes. In this paper, we investigate the practitioners' perceptions of current {SQA} planning activities, current challenges of such {SQA} planning activities, and propose four types of guidance to support {SQA} planning. We then propose and evaluate our {AI}-Driven {SQAPlanner} approach, a novel approach for generating four types of guidance and their associated risk thresholds in the form of rule-based explanations for the predictions of defect prediction models. Finally, we develop and evaluate a visualization for our {SQAPlanner} approach. Through the use of qualitative survey and empirical evaluation, our results lead us to conclude that {SQAPlanner} is needed, effective, stable, and practically applicable. We also find that 80\% of our survey respondents perceived that our visualization is more actionable. Thus, our {SQAPlanner} paves a way for novel research in actionable software analytics—i.e., generating actionable guidance on what should practitioners do and not do to decrease the risk of having defects to support {SQA} planning.},
	pages = {1--1},
	journaltitle = {{IEEE} Transactions on Software Engineering},
	author = {Rajapaksha, Dilini and Tantithamthavorn, Chakkrit and Bergmeir, Christoph and Buntine, Wray and Jiarpakdee, Jirayus and Grundy, John},
	date = {2021},
	keywords = {Predictive models, Software, Computer software selection and evaluation, Defects, Software quality, Software defects, Defect prediction models, Forecasting, Predictive analytics, Air navigation, Defect model, Empirical evaluations, Planning process, Qualitative surveys, Quality assurance, Risk threshold, Risks, Software quality improvements, Surveys, Visualization, Artificial intelligence, Tools, Actionable Software Analytics, Explainable {AI}, Planning, Software Quality Assurance, {SQA} Planning, xyes},
	file = {Submitted Version:C\:\\Users\\michalm\\Zotero\\storage\\L4PA8RXG\\Rajapaksha et al. - 2021 - SQAPlanner Generating Data-Informed Software Qual.pdf:application/pdf}
}

@inproceedings{de_castro_ribeiro_detection_2018,
	title = {Detection and Classification of Faults in Aeronautical Gas Turbine Engine: a Comparison Between two Fuzzy Logic Systems},
	doi = {10.1109/FUZZ-IEEE.2018.8491444},
	abstract = {Gas turbines are the most common engine used in the majority of commercial aircraft. Due to its criticality, to detect and classify faults in a gas turbine is extremely important. In this work, a type-1 and singleton fuzzy logic system trained by steepest descent method is used for detecting and classifying gas turbine faults. The data set was obtained through simulations on the software Propulsion Diagnostic Method Evaluation Strategy created by the National Aeronautics and Space Administration. Results are compared to those obtained with a type-1 fuzzy classifier with rule extraction by Wang and Mendel method. Analysis of results shows the effectiveness of the proposed model. When compared to the Wang and Mendel fuzzy classifier, it requires fewer rules to achieve a better performance.},
	pages = {1--7},
	booktitle = {2018 {IEEE} International Conference on Fuzzy Systems ({FUZZ}-{IEEE})},
	author = {de Castro Ribeiro, Mateus Gheorghe and Calderano, Pedro Henrique Souza and Amaral, Renan Piazzaroli Finotti and de Menezes, Ivan Fabio Mota and Tanscheit, Ricardo and Vellasco, Marley Maria Bernardes Rebuzzi and de Aguiar, Eduardo Pestana},
	date = {2018-07},
	keywords = {Computer circuits, {NASA}, Classification (of information), Fault detection, Aircraft, Fuzzy systems, Fuzzy logic, Fuzzy sets, Aeronautical gas turbines, Commercial aircraft, Data set, Diagnostic methods, Error detection, Fuzzy classifiers, Fuzzy logic system, Gas turbines, Gases, Rule extraction, Steepest descent method, Sensors, Classification, Engines, Indexes, Aeronautical Gas Turbine, Atmospheric modeling, Detection., Fuzzy Logic System, Turbines, xno}
}

@inproceedings{hashim_automated_2010,
	title = {Automated visual inspection for metal parts based on morphology and fuzzy rules},
	doi = {10.1109/ICCAIE.2010.5735137},
	abstract = {Automated visual inspection system ({AVIS}) is a method of analyzing, classifying, detection defects for products at the production line. Usually, this inspection is either conducted by human, machine or both. In this paper, we explain an algorithm that capable to classify mechanical products in real time. The system is consists of two parts: hardware and software. The algorithm used the web-camera attaching to an adjustable arm to capture various image. Our main objective is to develop an image processing algorithm and fuzzy reasoning that can compute both the area and circularity of mechanical shapes and hence classify them according to their categories. The result shows the accuracy of classification is 80.5 \% for group classification and 98\% for individual classification of mechanical parts.},
	pages = {527--531},
	booktitle = {2010 International Conference on Computer Applications and Industrial Electronics},
	author = {Hashim, Haider Sh. and Abdullah, Siti Norul Huda Sheikh and Prabuwono, Anton Satria},
	date = {2010-12},
	keywords = {Feature extraction, Visualization, Hardware and software, Fuzzy rules, Algorithms, Inspection, Image processing algorithm, Automated visual inspection, Automated visual inspection systems, Computer applications, Computer vision, Fuzzy reasoning, Group classification, Industrial electronics, Mathematical morphology, Mechanical parts, Mechanical product, Metal part, Metal parts, Morphology, Production line, Real time, Visual inspection, fuzzy rules, Image edge detection, mathematical morphology, metal part, Pragmatics, Shape, visual inspection, xno}
}

@article{ma_dynamic_2011,
	title = {Dynamic Hybrid Fault Modeling and Extended Evolutionary Game Theory for Reliability, Survivability and Fault Tolerance Analyses},
	volume = {60},
	issn = {1558-1721},
	doi = {10.1109/TR.2011.2104997},
	abstract = {We introduce a new layered modeling architecture consisting of dynamic hybrid fault modeling and extended evolutionary game theory for reliability, survivability, and fault tolerance analyses. The architecture extends traditional hybrid fault models and their relevant constraints in the Agreement algorithms with survival analysis, and evolutionary game theory. The dynamic hybrid fault modeling (i) transforms hybrid fault models into time- and covariate-dependent models; (ii) makes real-time prediction of reliability more realistic, and allows for real-time prediction of fault-tolerance; (iii) sets the foundation for integrating hybrid fault models with reliability and survivability analyses by integrating them with evolutionary game modeling; and (iv) extends evolutionary game theory by stochastically modeling the survival (or fitness) and behavior of `game players.' To analyse survivability, we extend dynamic hybrid fault modeling with a third-layer, operational level modeling, to develop the three-layer survivability analysis approach (dynamic hybrid fault modeling constitutes the tactical and strategic levels). From the perspective of evolutionary game modeling, the two mathematical fields, i.e., survival analysis and agreement algorithms, which we applied for developing dynamic hybrid fault modeling, can also be utilized to extend the power of evolutionary game theory in modeling complex engineering, biological (ecological), and social systems. Indeed, a common property of the areas where our extensions to evolutionary game theory can be advantageous is that the risk analysis and management are a core issue. Survival analysis (including competing risks analysis, and multivariate survival analysis) offers powerful modeling tools to analyse time-, space-, and/or covariate-dependent uncertainty, vulnerability, and/or frailty which `game players' may experience. The agreement algorithms, which are not limited to the agreement algorithms from distributed computing, when applied to extend evolutionary game modeling, can be any problem (game system) specific rules (algorithms or models) that can be utilized to dynamically check the consensus among game players. We expect that the modeling architecture and approaches discussed in the study should be implemented as a software environment to deal with the necessary sophistication. Evolutionary computing should be particularly convenient to serve as the core optimization engine, and should simplify the implementation. Accordingly, a brief discussion on the software architecture is presented.},
	pages = {180--196},
	number = {1},
	journaltitle = {{IEEE} Transactions on Reliability},
	author = {Ma, Zhanshan and Krings, Axel W.},
	date = {2011-03},
	keywords = {Analytical models, Reliability, Algorithm design and analysis, fault tolerance, Agreement algorithms, Brain models, Byzantine generals problem, dynamic hybrid fault models, extended evolutionary game theory modeling, Game theory, Heuristic algorithms, reliability, survivability, survival analysis, wireless sensor networks, xno}
}

@inproceedings{menzies_local_2011,
	title = {Local vs. global models for effort estimation and defect prediction},
	doi = {10.1109/ASE.2011.6100072},
	abstract = {Data miners can infer rules showing how to improve either (a) the effort estimates of a project or (b) the defect predictions of a software module. Such studies often exhibit conclusion instability regarding what is the most effective action for different projects or modules. This instability can be explained by data heterogeneity. We show that effort and defect data contain many local regions with markedly different properties to the global space. In other words, what appears to be useful in a global context is often irrelevant for particular local contexts. This result raises questions about the generality of conclusions from empirical {SE}. At the very least, {SE} researchers should test if their supposedly general conclusions are valid within subsets of their data. At the very most, empirical {SE} should become a search for local regions with similar properties (and conclusions should be constrained to just those regions).},
	pages = {343--351},
	booktitle = {2011 26th {IEEE}/{ACM} International Conference on Automated Software Engineering ({ASE} 2011)},
	author = {Menzies, Tim and Butcher, Andrew and Marcus, Andrian and Zimmermann, Thomas and Cok, David},
	date = {2011-11},
	note = {{ISSN}: 1938-4300},
	keywords = {Data mining, Software, Defects, Software engineering, Software modules, Defect prediction, Principal component analysis, Data miners, Effort Estimation, Estimation, Global models, validation, Context, Couplings, defect/effort estimation, empirical {SE}, Runtime, {USA} Councils, xyes, xdataset}
}

@inproceedings{shiling_time_2019,
	title = {Time Series Prediction and Pattern Recognition of Fault Decomposition Gas for High Voltage Composite Electrical Appliances Based on Grey System {OBGM} (1,N) Model},
	doi = {10.1109/ICCC47050.2019.9064088},
	abstract = {The {GIL}/{GIS} is widely used in the power system. The application of grey system to forecast and analyze the time series of gas decomposition and fault pattern recognition has good engineering value for its operation condition monitoring. Based on the programming of {MATLAB} software environment, time series prediction model based on grey system {OBGM} (1,N) is realized. Based on model, grey system time series analysis algorithm is applied to predict the time-varying sequence of the volume fraction of {SF}6 gas decomposition products and the characteristics of the partial discharge Atlas of the high-voltage combined electrical equipment {GIL}/{GIS} in fault state. Parametric time series, grey system is further applied to data mining and analysis of association rules of time series state change, and the time series set is applied to cluster analysis of typical fault types of high-voltage {GIL}/{GIS} power equipment. The grey time series prediction and grey relational clustering analysis of small sample test data can be carried out by using the grey system to analyze large data of high voltage {GIL}/{GIS} power equipment condition assessment. The multi-dimensional and the grey system information fusion technology proposed in this paper is especially suitable for the application of the small sample and the poor data in the high-voltage {GIL}/{GIS} power equipment operation condition detection technology. The time series prediction and the pattern recognition technology of the fault decomposition gas based on the grey system {OBGM} (1, N) model proposed can provide some technical support for the operation and maintenance of high voltage combinations. The characteristic parameters of partial discharge pattern and grey relational clustering analysis have the good engineering and the theoretical value.},
	pages = {943--947},
	booktitle = {2019 {IEEE} 5th International Conference on Computer and Communications ({ICCC})},
	author = {Shiling, Zhang and Jianchao, Wang},
	date = {2019-12},
	keywords = {Data mining, Predictive models, Forecasting, {MATLAB}, Cluster analysis, Condition monitoring, Electrical appliances, Electrical equipment, Operation and maintenance, Operation condition monitoring, Partial discharge pattern, Partial discharges, Pattern recognition systems, Pattern recognition technologies, Relational clustering, System theory, Time series analysis, Time series prediction, Monitoring, Mathematical model, Discharges (electric), Gas insulation, grey relational clustering analysis, grey system {OBGM}(1, grey time series prediction, high voltage composite electrical appliances, N) model, xno}
}

@inproceedings{wang_novel_2014,
	title = {A novel approach to the design and implementation of mutation operators for object-oriented programming language},
	doi = {10.1109/WICT.2014.7077310},
	abstract = {Software testing allows programmers to determine and guarantee the quality of software system. It is one of the essential activities in software development process. Mutation analysis is a branch of software testing. It is classified as a fault-based software testing technology. Unlike other software testing technologies, mutation analysis assesses the quality of software test cases and therefore improves the efficiency of software testing by measuring and improving the quality of test cases set. Mutation analysis works by generating mutants, which describe the faults that may occur in the software programs, from original program. Mutants are generated by applying mutation operators on original software program. Mutation operator is a rule that specifies the syntactic variations of strings. There have been several works about mutation analysis support system for conventional languages such as C and {FORTRAN}. However, there are a few for object-oriented languages such as C++ and Java. This article aims to propose a novel approach to design and implement mutation operators for object-oriented programming language. The essential idea of proposed method is the usage of {JavaML} language as the intermediate representation to implement mutation operators: it first converts the original program into {JavaML} document; then implements mutation operator for {JavaML} document and gets the mutated {JavaML} document with the help of {DOM} - a tool to process {JavaML} document; finally it converts the mutated {JavaML} document into mutant program.},
	pages = {102--106},
	booktitle = {2014 4th World Congress on Information and Communication Technologies ({WICT} 2014)},
	author = {Wang, Qianqian and Haga, Hirohide},
	date = {2014-12},
	keywords = {Software, Object oriented programming, Software testing, {XML}, Java, Encapsulation, Mutation Analysis, Object Oriented Langauge, Software Testing, xno}
}

@article{mahela_algorithm_2020,
	title = {An algorithm for the protection of distribution feeder using Stockwell and Hilbert transforms supported features},
	issn = {2096-0042},
	doi = {10.17775/CSEEJPES.2020.00170},
	abstract = {Faults' recognition in the distribution feeders ({DFs}) is extremely important for improving the reliability of distribution system. Therefore, this manuscript proposed the technique to identify the faults on the {DF} using Stockwell Transform ({ST}) dependent variance feature and Hilbert transform ({HT}) by utilizing current signals. By element to element multiplication of H-index, computed using {HT} aided decompositions of current waveforms and {VS}-index, calculated through {ST} aided decomposition of current waveforms. By utilizing the decision rules, various faults are classified. Different faults studied in this work are line to ground, double line, double line to ground and 3-Φ to ground. For the high fault impedance also, this technique is effectively utilized. Further, variation in the fault incidence angles also performed to test the performance of the proposed technique. To perform the proposed algorithm, {IEEE}-13 bus system is developed in {MATLAB}/Simulink software. Algorithm effectively classified the faults with accuracy greater than 98\%. Algorithm is also successfully validated on {IEEE}-34 bus test system. Further, algorithm successfully validated on the practical power system network. It is recognized that the developed method performed better than the discrete Wavelet transform ({DWT}) and ruled decision tree based protection scheme reported in literature.},
	pages = {1--9},
	journaltitle = {{CSEE} Journal of Power and Energy Systems},
	author = {Mahela, Om Prakash and Sharma, Jaya and Kumar, Bipul and Khan, Baseem and Alhelou, Hassan Haes},
	date = {2020},
	keywords = {Circuit faults, Fault diagnosis, Classification algorithms, Indexes, Transforms, Distribution networks, fault classification, fault recognition, H-index, Signal processing algorithms, {VS}-index, xno}
}

@inproceedings{keow_power_2010,
	title = {Power quality problem classification based on Wavelet Transform and a Rule-Based method},
	doi = {10.1109/PECON.2010.5697666},
	abstract = {This paper describes a Wavelet Transform and Rule-Based method for detection and classification of various events of power quality disturbances. In this model, wavelet Multi-Resolution Analysis ({MRA}) technique was used to decompose the signal into its various details and approximation signals, and unique features from the 1st, 4th, 7th and 8th level detail are obtained as criteria for classifying the type of disturbance occurred. These features and together with the duration of disturbance of occurrence obtained from 1st level of detail, they form the criteria for a Rule-Based software algorithm for detecting different kinds of power quality disturbances effectively. It is presented in this paper that the choice of sampling frequency is important since it affects the average energy profile of the details and eventually may cause error in detection of power quality disturbances. The model is tested by using {MATLAB} toolbox. The simulation produces satisfactory result in identifying the disturbance and proof that it is possible to use this model for power disturbance classification. Since the method can reduce the number of parameters needed in classification, less memory space and computing time are required for its implementation. Thus it stands up to be a suitable model to be used in real time implementation through a {dsPIC}-based embedded system.},
	pages = {677--682},
	booktitle = {2010 {IEEE} International Conference on Power and Energy},
	author = {Keow, Chuah Heng and Nallagownden, Perumal and Rao, K. S. Rama},
	date = {2010-11},
	keywords = {Feature extraction, Classification algorithms, Harmonic analysis, Transient analysis, Approximation methods, Filter bank, multi resolution analysis, Multiresolution analysis, power quality, Rule-Based algorithm, wavelet, xno}
}

@inproceedings{bandyopadhyay_using_2012,
	title = {Using mean current vector to develop a rule base for identification of {IGBT} faults in induction motor drives},
	doi = {10.1109/ICPEN.2012.6492316},
	abstract = {One of the most common fault in Pulse Width Modulation ({PWM}) Voltage Source Inverter ({VSI}) feeding an induction motor is open gate drive of the switching device ({IGBT}). In addition to these extreme fault cases, there may be less severe faults cases due to improper contact points, problematic solder joints, poor connections etc. Main objective of this work is to develop a rule base that not only can segregate these two different types of fault cases but also can identify the power switch in which such a fault has occurred. Rigorous simulation results using {PSIM} software for creating all variations of the above mentioned fault cases in a {PWM} {VSI} driven induction motor is presented for diagnosing the condition of an inverter. The three phase line currents feeding the motor are recorded and transformed to d-q reference frame using Park's Transformation for further analysis. From d and q current components thus obtained, the mean current vector is calculated. The loci of the mean current vector are plotted on the d-q plane and their patterns are observed. Shapes of these loci have been found to be effective in classifying different fault modes.},
	pages = {1--6},
	booktitle = {2012 1st International Conference on Power and Energy in {NERIST} ({ICPEN})},
	author = {Bandyopadhyay, I. and Das, S. and Purkait, P. and Koley, C.},
	date = {2012-12},
	keywords = {Rule base, Concordia pattern, {IGBT} open base, Mean current vector, Park's Tranformation, {PWM}, {VSI} Drive, xno}
}

@inproceedings{zhang_detecting_2011,
	title = {Detecting resource leaks through dynamical mining of resource usage patterns},
	doi = {10.1109/DSNW.2011.5958824},
	abstract = {Resource management is crucial to software productions. Resources must be carefully acquired and released, or a resource leak might occur. For open source projects, resource leaks can be easily introduced during code check-in, and it is laborious to review, identify, report, and fix such leaks. Recently, there has been a growing interest in data mining {API} usage patterns to discover potential bugs such as resource leaks. However, the usage patterns mined are specific to a certain library, which cannot be applied to detect bugs in other libraries. In this paper, we present an idea called {MODE}, “Mine Once, Detect Everywhere”, to address the universality of such patterns, and use them to detect potential resource leaks automatically before code check-in. We propose an efficient algorithm to record the most valuable {API} calls that are related to resource usage during program execution, and mine resource usage patterns from the traces with a sequence miner. To verify the effectiveness of the patterns, experiments are given to use them to detect real resource leaks in large open source projects.},
	pages = {265--270},
	booktitle = {2011 {IEEE}/{IFIP} 41st International Conference on Dependable Systems and Networks Workshops ({DSN}-W)},
	author = {Zhang, Huxing and Wu, Gang and Chow, Kingsum and Yu, Zhidong and Xing, {XueZhi}},
	date = {2011-06},
	note = {{ISSN}: 2325-6664},
	keywords = {Software, Association rules, Computer bugs, Data Mining, Java, Dynamical Analysis, Integrated circuits, Libraries, Resource Leak Detection, xno}
}

@inproceedings{mao_variable_2011,
	title = {Variable Precision Rough Set-Based Fault Diagnosis for Web Services},
	doi = {10.1109/TrustCom.2011.215},
	abstract = {Web service is the emergent technology for constructing more complex and flexible software system for business applications. However, some new features of Web service-based software such as heterogeneity and loose coupling bring great trouble to the latter fault debugging and diagnosis. In the paper, variable precision rough set-based diagnosis framework is presented. In such debugging model, {SOAP} message monitoring and service invocation instrument are used to record service interface information. Meanwhile, factors of execution context are also viewed as conditional attributes of knowledge representation system. The final execution result is treated as the decision attribute, and failure ontology is utilized to classify system's failure behaviors. Based on this extended information system, variable precision rough set reasoning is performed to generate the probability association rules, which are the clues for locating the possible faulty services. In addition, the experiment on a real-world Web services system is performed to demonstrate the feasibility and effectiveness of our proposed method.},
	pages = {1550--1555},
	booktitle = {2011IEEE 10th International Conference on Trust, Security and Privacy in Computing and Communications},
	author = {Mao, Chengying},
	date = {2011-11},
	note = {{ISSN}: 2324-9013},
	keywords = {Ontology, Program debugging, Association rules, Computer system recovery, Failure analysis, Software systems, Embedded systems, Rough set theory, Rough set, Business applications, Decision attribute, Embedded software, Execution context, Failure behaviors, Knowledge representation, Program diagnostics, Service interfaces, Service invocation, Service-based, {SOAP} messages, Variable precision, Variable precision rough sets, Web services, Information systems, association rule, Cognition, Business, fault diagnosis, failure, Linux, rough set, service interface, Wireless communication, xno}
}

@inproceedings{moeinfar_using_2017,
	title = {Using models at run-time to measure quality of {SAS} in the large-scale software systems},
	doi = {10.1109/IKT.2017.8258625},
	abstract = {[Context/ Motivation] The adaptation process in self-adaptive software systems modifies the system based on new monitored conditions to make the system able to deserve {SLA} ({QoS}). Some Researchers assume the environment as a closed-world. So, adaptation actions are predicted at design-time, to get applied at run-time. But uncertainty causes prediction about the all different conditions be impossible. [Objective] In this study we aim to handle the uncertainty consequences in the open-world by the use of two quality models presented in the process level and the product level. [Method] To reduce the cost of adaptation at run-time, an integrated approach for modeling and verifying the requirements of {SAS} is used as well as a measurement method to measure the level of satisficing quality factors using Model Driven Engineering ({MDE}). [Results] When the measurement (i.e. the deviation from desired behavior) is taken into account in early phases of development, not only the adaptation cost degrades, but also the undesired consequences do not propagate to the next phases. [Conclusion] To sum up, in this paper requirements are modeled based on {GORE} models. Then an {SAS} Quality model is augmented in the goal model. This embedded model is verified by {OMEGA}2/ {IFX} profile. The verified model is transformed to alternative architectural models by some defined transformation rules which is written in {ATLAS} language, implemented in Eclipse Modeling Framework ({EMF}).},
	pages = {99--103},
	booktitle = {2017 9th International Conference on Information and Knowledge Technology ({IKT})},
	author = {{MoeinFar}, Rayehe and Barforoush, Ahmad Abdollahzadeh},
	date = {2017-10},
	keywords = {Software systems, Computational modeling, Adaptation models, {MDE}, open-world, {QoS}, quality model, Quality of service, Requirements engineering, self-adaptive software, {SLA}, Synthetic aperture sonar, uncertainty, Uncertainty, xno}
}

@inproceedings{miura_fast_2013,
	title = {Fast and accurate design based binning based on hierarchical clustering with invariant feature vectors for {BEOL}},
	doi = {10.1109/ASMC.2013.6552744},
	abstract = {As design rules continue to shrink, systematic defects have become a serious problem. It becomes very important to review systematic defects effectively by a defect review {SEM} (scanning electron microscope) and to modify the design and the process to keep or improve an yield. In this paper, we propose a fast and accurate design based binning ({DBB}) method that is based on the hierarchical clustering with invariant feature vectors for back end of line ({BEOL}). In order to improve classification accuracy, we employ the hierarchical clustering method. Shift-, rotation-, and flip-invariant feature vectors are extracted from layout data. We propose two variations of {DBB} methods: one-step method and two-step method. The one-step method employs solely the hierarchical clustering. It can improve classification accuracy. However, computational time of the hierarchical clustering is high so that it is not practical to classify many defects by this method. In order to achieve both high accuracy and fast computation, we also propose two-step method that employs the hierarchical clustering after classifying defects by the {DBB} software used in the production line. We apply the proposed two methods to volume production data. The results show that the proposed two-step method can significantly improve accuracy against the production line {DBB} software, despite of slight decrease in purity and slight increase in computation time.},
	pages = {7--12},
	booktitle = {{ASMC} 2013 {SEMI} Advanced Semiconductor Manufacturing Conference},
	author = {Miura, Katsuyoshi and Soga, Yuki and Nakamae, Koji and Kadota, Kenichi and Aritake, Toshiyuki and Yamazaki, Yuichiro},
	date = {2013-05},
	note = {{ISSN}: 2376-6697},
	keywords = {Feature extraction, Software, Vectors, defect, Production, Systematics, accuracy, Accuracy, {DBB} (design based binning), {DBG} (design based grouping), geometric mean, hierarchical clustering, Layout, purity, xno}
}

@inproceedings{loutfi_online_2011,
	title = {Online condition monitoring network for critical equipment at Holcim's {STE}. genevieve plant},
	doi = {10.1109/CITCON.2011.5934563},
	abstract = {This paper describes a network architecture for remote monitoring and fault detection of critical rotating equipment. Holcim's Sphinx Monitoring System ({SMS}) is intended for equipment protection and early prediction of machine defects. {SMS} is based on a server-client model, where the server maintains a database of equipment configurations and evaluates each target with a unique set of rules that monitor the equipment's function and its integrated components' failure modes. Adaptive software algorithms monitor process and operational changes to tune the analysis criteria for evaluating vibration signals and detecting common machine faults. Network structure, software algorithms and other aspects of the system are further discussed and evaluated against samples of collected data, generated analysis results and physical inspections' outcomes.},
	pages = {1--11},
	booktitle = {2011 {IEEE}-{IAS}/{PCA} 53rd Cement Industry Technical Conference},
	author = {Loutfi, Moheb Y.},
	date = {2011-05},
	note = {{ISSN}: 2155-9155},
	keywords = {Software, Condition monitoring, Monitoring, Inspection, Databases, Radar, Vibrations, xno}
}

@inproceedings{casimiro_karyon_2013,
	title = {The {KARYON} project: Predictable and safe coordination in cooperative vehicular systems},
	doi = {10.1109/DSNW.2013.6615530},
	abstract = {{KARYON}, a kernel-based architecture for safety-critical control, is a European project that proposes a new perspective to improve performance of smart vehicle coordination. The key objective of {KARYON} is to provide system solutions for predictable and safe coordination of smart vehicles that autonomously cooperate and interact in an open and inherently uncertain environment. One of the main challenges is to ensure high performance levels of vehicular functionality in the presence of uncertainties and failures. This paper describes some of the steps being taken in {KARYON} to address this challenge, from the definition of a suitable architectural pattern to the development of proof-of-concept prototypes intended to show the applicability of the {KARYON} solutions. The project proposes a safety architecture that exploits the concept of architectural hybridization to define systems in which a small local safety kernel can be built for guaranteeing functional safety along a set of safety rules. {KARYON} is also developing a fault model and fault semantics for distributed, continuous-valued sensor systems, which allows abstracting specific sensor faults and facilitates the definition of safety rules in terms of quality of perception. Solutions for improved communication predictability are proposed, ranging from network inaccessibility control at lower communication levels to protocols for assessment of cooperation state at the process level. {KARYON} contributions include improved simulation and fault-injection tools for evaluating safety assurance according to the {ISO} 26262 safety standard. The results will be assessed using selected use cases in the automotive and avionic domains.},
	pages = {1--12},
	booktitle = {2013 43rd Annual {IEEE}/{IFIP} Conference on Dependable Systems and Networks Workshop ({DSN}-W)},
	author = {Casimiro, António and Kaiser, Jörg and Schiller, Elad M. and Costa, Pedro and Parizi, José and Johansson, Rolf and Librino, Renato},
	date = {2013-06},
	note = {{ISSN}: 2325-6664},
	keywords = {Computer architecture, Reliability, Computational modeling, Uncertainty, Kernel, Safety, Vehicles, xno}
}

@article{wang_security_2020,
	title = {Security Assessment of Blockchain in Chinese Classified Protection of Cybersecurity},
	volume = {8},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2020.3036004},
	abstract = {Classified protection is one of primary security policies of information system in many countries. With the increasing popularity of blockchain in various fields of applications, it is extremely necessary to promote classified protection for blockchain's risk assessment in order to push forward the sustainable development of blockchain. Taking the Level 3 in Chinese classified protection 2.0 as an example, this paper proposes the common evaluation rules on blockchain to ensure that blockchain can meet the needs of countries to build it as critical infrastructure. Both assessment requirements and enforcement proposals are presented and analyzed from the standpoint of blockchain's core technologies, e.g., peer-to-peer network, distributed ledger, contract's scripting system, and consensus mechanism. Moreover, the assessment results on three main platforms, Bitcoin, Ethereum, and Hyperledger, are summarized and analyzed in compliance with the control points specified in the level 3. Our investigation indicates that the current blockchain is able to satisfy the requirements of evaluation items in many aspects, such as software fault tolerance, resource control, backup and recovery, but further improvements are still needed for some aspects, including security audit, access control, identification and authentication, data integrity, etc., in order to satisfy the requirements of important fields on national security, economic development and human life.},
	pages = {203440--203456},
	journaltitle = {{IEEE} Access},
	author = {Wang, Di and Zhu, Yan and Zhang, Yi and Liu, Guowei},
	date = {2020},
	keywords = {Software, Access control, Blockchain, Compliance control, Core technology, Cyber security, Evaluation items, Evaluation rules, Fault tolerance, Fault tolerant computer systems, National security, Peer to peer networks, Resource control, Risk assessment, Security assessment, Security policy, Software fault tolerances, {XML}, assessment and analysis, classified protection of cybersecurity, consensus mechanism, Distributed ledger, Peer-to-peer computing, peer-to-peer network, Proposals, Risk management, Sustainable development, xno}
}

@article{yan_artdl_2020,
	title = {{ARTDL}: Adaptive Random Testing for Deep Learning Systems},
	volume = {8},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2019.2962695},
	abstract = {With recent breakthroughs in Deep Learning ({DL}), {DL} systems are increasingly deployed in safety-critical fields. Hence, some software testing methods are required to ensure the reliability and safety of {DL} systems. Since the rules of {DL} systems are inferred from training data, it is difficult to know the implementation rules about each behavior of {DL} systems. At the same time, Random Testing ({RT}) is a popular testing method and the knowledge about software implementation is not needed when we use {RT}. Therefore, {RT} is very suitable for the testing of {DL} systems. And the existing mechanisms for testing {DL} systems also depend heavily on {RT} by the labeled test data. In order to increase the effectiveness of {RT} for {DL} systems, we design, implement and evaluate the Adaptive Random Testing for {DL} systems ({ARTDL}), which is the first Adaptive Random Testing ({ART}) method to improve the effectiveness of {RT} for {DL} systems. {ARTDL} refers to the idea of {ART}. That is, fewer test cases are needed to detect failures by selecting the test case with the furthest distance from non-failure-causing test cases. Firstly, we propose the Feature-based Euclidean Distance ({FED}) as the distance metric that can be used to measure the difference between failure-causing inputs and non-failure-causing inputs. Secondly, we verify the availability of {FED} by presenting the failure pattern of {DL} models. Finally, we design {ARTDL} algorithm to generate the test cases that are more likely to cause failures based on the {FED}. We implement {ARTDL} to test top performing {DL} models in the field of image classification and automatic driving. The results show that, on average, the number of test cases used to find the first bug is reduced by 62.74\% through {ARTDL}, compared with {RT}.},
	pages = {3055--3064},
	journaltitle = {{IEEE} Access},
	author = {Yan, Min and Wang, Li and Fei, Aiguo},
	date = {2020},
	keywords = {Feature extraction, Software, Software testing, Measurement, Deep learning, adaptive random testing, Deep learning testing, distance metric, metamorphic testing, Subspace constraints, xno}
}

@inproceedings{cheng_embedded_2012,
	title = {Embedded Tutorial Summary: Diagnosis for Accelerating Yield and Failure Analysis},
	doi = {10.1109/ATS.2012.73},
	abstract = {Summary form only given. Software-based diagnosis of scan test failures has been an established method for localizing defects as part of the failure analysis process for digital semiconductor devices. Diagnosis software determines the defect type and location for each failing device based on the design description, scan test patterns, and tester fail data. This gives failure analysis engineers or engineers involved in non-destructive testing and evaluation, a proven, and highly effective way of defect localization and identification, complementing their traditional, hardware-based failure analysis machine approach. Recent disruptions in yield enhancement capabilities such as dramatic increase in design sensitive defects and longer failure analysis cycle times have necessitated the development of new technology. As a result, better diagnosis, better failure data collection and better statistics method have been developed to analyze volume diagnosis data to provide value in a range of applications including technology yield ramp and product yield improvement. This tutorial describes how diagnosis-driven yield analysis ({DDYA}) accelerates time to root cause of yield loss and identifies yield limiters from volume diagnosis data. The underlying technology key to the methodology is an extremely accurate and consequential single-die diagnosis of scan test failures. Furthermore, specialized statistical analysis is used to identify and separate systematic yield limiters in seemingly random fail data and select the most suitable devices for failure analysis. The presentation will cover fundamentals as well as recent advances in diagnosis technology and diagnosis results analysis that drive the adoption of this approach:1) Layout-aware diagnosis provides detailed defect classifications that are significantly more valuable for yield analysis than defect locations alone.2) Scan chain diagnosis identifies and localizing defects in the test structures, scan chains themselves, simplifying a process that can otherwise be tedious and performed using dedicated and costly equipment.3) Novel machine learning techniques separate the noise that exists in diagnosis data to determine the underlying root causes represented in a population of failing devices from test data alone. This dramatically reduces the number of costly physical failure analysis to identify systematic defects. {DFM} analysis has significant impact to yield. A new approach will be explored to correlate {DFM} analysis and diagnosis results to identify {DFM} rules that most succinctly describe the design-process induced systematic defects.},
	pages = {271--271},
	booktitle = {2012 {IEEE} 21st Asian Test Symposium},
	author = {Cheng, Wu-Tung and Kuo, Feng-Ming},
	date = {2012-11},
	note = {{ISSN}: 2377-5386},
	keywords = {Failure analysis, Diagnosis, Systematics, Failure Analysis, Graphics, Life estimation, Manufacturing, Object recognition, Tutorials, Yield, xno}
}

@inproceedings{denlinger_managing_2018,
	title = {Managing Defect Reduction to Achieve Reliability Growth},
	doi = {10.1109/RAM.2018.8463032},
	abstract = {The importance of reliability as well as a clear definition for a reliability requirement are presented. The concept of functional performance demonstrated early in product development with a very few hours of testing is presented as a more effective early warning sign that a program is behind schedule with respect to reliability goals. Utilizing the six types of defects encountered in a product development program as predictors of reliability is introduced. Types of failure modes and potential systemic causes of failure are discussed. The prediction relationship between manufacturing and design quality flaws with field failure mode experience is demonstrated. Systems dynamics modeling and empirical tracking of fix effectiveness can be used to predict the learning cycles required to launch a successful product. The concepts of defects per unit ({DPU}) (Defects in functional performance, product design quality and manufacturing process control); the Laplace estimate of proportion defective in a lot based on small sample size to predict the number of learning cycles required for each system program launch; and the rule of 10:4, where every 10 fixes of failure will lead to the discovery of four new causes of failure in the next set of prototypes, are introduced.},
	pages = {1--7},
	booktitle = {2018 Annual Reliability and Maintainability Symposium ({RAMS})},
	author = {Denlinger, Daniel},
	date = {2018-01},
	note = {{ISSN}: 2577-0993},
	keywords = {Testing, Defects, Forecasting, Software testing, Software reliability, Measurement, Failure modes, Functional performance, Learning cycle, Maintainability, Manufacture, Per unit, Prediction-relationships, Product design, Product development, Product development programs, Program Maturity, Reliability requirements, Rule of 10:4, Logic gates, Manufacturing, Defect per Unit, Learning Cycle, Manufacturing Rolled Throughput Yield, xno}
}

@inproceedings{sharma_protection_2020,
	title = {Protection of Distribution Feeder Using Stockwell Transform Supported Voltage Features},
	doi = {10.1109/PIICON49524.2020.9113014},
	abstract = {This paper presents a research work focussed on the identification of faults on the distribution feeder supported by Stockwell transform based summing of absolute values and median features using the voltage signals. A fault index is proposed which is obtained by the multiplication of H-index (obtained summation of absolute values S-matrix evaluated by {ST} supported decomposition of voltage) and {VS}-index (obtained median of absolute values S-matrix evaluated by {ST} supported decomposition of voltage). Classification of faults is achieved using decision rules. Investigated faults include phase to ground, fault between two phases, two phases to ground fault and fault involving all three phases and ground. Performance of algorithm is tested on high fault impedance and fault incidence angle. Proposed study is performed using {MATLAB} software in Simulink environment.},
	pages = {1--6},
	booktitle = {2020 {IEEE} 9th Power India International Conference ({PIICON})},
	author = {Sharma, Jaya and Kumar, Bipul and Mahela, Om Prakash and Ranjan Garg, Akhil},
	date = {2020-02},
	note = {{ISSN}: 2642-5289},
	keywords = {Distribution Feeder Fault, Hilbert transform, protection, Stockwell Transform, voltage, xno}
}

@article{leung_refining_2017,
	title = {Refining fault trees using aviation definitions for consequence severity},
	volume = {32},
	issn = {1557-959X},
	doi = {10.1109/MAES.2017.150171},
	abstract = {Unmanned aerial systems ({UAS}) are being considered for civilian tasks that are time consuming and costly for humans, such as package delivery or surveillance. To derive maximum benefit in these applications, {UAS} will need to be autonomous rather than remotely operated. Safety rules for autonomous, unoccupied vehicles have yet to be defined. As of February 2015, the Federal Aviation Administration ({FAA}) still requires that a {UAS} operator maintain line of sight with the {UAS}. As safety standards for autonomous flight emerge, it is expected that a very large number of {UAS} will take to the skies. Current methods for certification of aviation technologies are rooted in the assumption of manned flight in relatively low-density airspace. The massive number of expected {UAS} flights and the introduction of increasingly autonomous software autopilots mean that new methods for certifying {UAS} technologies are warranted. In particular, it is overconservative to assume that failures that could result in the loss of aircraft are necessarily catastrophic, as they would be in the case of a manned aircraft. In order to provide more flexibility in modeling faults, this article introduces methods that tune certification processes to better match {FAA} standards and to reduce overconservatism that can delay system certification. The basic idea of our work is to update the venerable fault tree, which is at the heart of most certification cases for new aviation technology. A fault tree quantifies risks associated with various failure events and maps the risk to a consequence, which is an unfavorable or hazardous outcome of those fault events. Failure events (also called fault modes) and their associated consequences (also called effects) are typically identified through a process called failure modes and effects analysis ({FMEA}). Failure modes are then mapped on to the fault tree, where related faults form branches that lead to an undesired consequence. Typically fault trees use a binary state to represent each event: either a fault event has occurred or it has not. The probability of fault occurrence is generally much, much lower than the probability of a nonoccurrence.Whereas conventional fault-tree analysis uses a binary state for each fault, our proposed method is different in that it uses a multilevel state. Introducing multiple severity levels allows for the consequences of events to be classified more precisely, recognizing that not all faults have the same (e.g. catastrophic) consequences. More precise classification of consequences promotes reduced overconservatism, particularly in safety-critical applications, like aviation, where sets of faults are classified by the worst case example from the set. Our work is not the first to generalize fault-tree analysis to use fault states with more than two values. Notably, fuzzy logic has been applied in the past to model multilevel fault states in general and aviation-specific safety systems. The limitation of prior work is that multiple severity levels have been assigned at the level of the fault mode (root cause) rather than at the level of the fault effect (final consequence). For safety analysis, unfortunately, it is not usually clear how to combine faults of different severity levels to determine their effect. Do two low-severity faults occurring simultaneously result in a higher severity consequence? There is no generic answer to this question for aviation applications, because the answer depends on the relationship between the two faults, which must be characterized on a case-by-case basis, for example by using an {FMEA}. Our proposed method modifies the structure of a traditional fault tree, by shifting focus away from individual faults and to groups of faults, called fault chains. A fault chain is a set of events that together result in an undesired consequence. Because the focus of the method is on consequences (rather than fault events) and because those consequences may take on multiple levels of severity, we call our method consequence severity level ({CSL}) analysis. The remainder of the article describes {CSL} analysis and its applications. As a starting point, Section {II} provides background on aviation severity levels and fault-tree analysis. Section {III} introduces {CSL} analysis. Afterward, Section {IV} presents a {UAS} case study for a representative application: the use of a {UAS} for an inspection application. Section V quantifies this application using both conventional and {CSL} analysis. To conclude, a final section summarizes key technical results and their impact for certification of future {UAS}.},
	pages = {4--14},
	number = {3},
	journaltitle = {{IEEE} Aerospace and Electronic Systems Magazine},
	author = {Leung, Tszhim J. and Rife, Jason},
	date = {2017-03},
	keywords = {Fuzzy logic, Logic gates, Accidents, Drones, {FAA}, Surveillance, Unmanned aerial vehicles, Vehicle safety, xno}
}

@inproceedings{kazman_case_2015,
	title = {A Case Study in Locating the Architectural Roots of Technical Debt},
	volume = {2},
	doi = {10.1109/ICSE.2015.146},
	abstract = {Our recent research has shown that, in large-scale software systems, defective files seldom exist alone. They are usually architecturally connected, and their architectural structures exhibit significant design flaws which propagate bugginess among files. We call these flawed structures the architecture roots, a type of technical debt that incurs high maintenance penalties. Removing the architecture roots of bugginess requires refactoring, but the benefits of refactoring have historically been difficult for architects to quantify or justify. In this paper, we present a case study of identifying and quantifying such architecture debts in a large-scale industrial software project. Our approach is to model and analyze software architecture as a set of design rule spaces ({DRSpaces}). Using data extracted from the project's development artifacts, we were able to identify the files implicated in architecture flaws and suggest refactorings based on removing these flaws. Then we built economic models of the before and (predicted) after states, which gave the organization confidence that doing the refactorings made business sense, in terms of a handsome return on investment.},
	pages = {179--188},
	booktitle = {2015 {IEEE}/{ACM} 37th {IEEE} International Conference on Software Engineering},
	author = {Kazman, Rick and Cai, Yuanfang and Mo, Ran and Feng, Qiong and Xiao, Lu and Haziyev, Serge and Fedak, Volodymyr and Shapochka, Andriy},
	date = {2015-05},
	note = {{ISSN}: 1558-1225},
	keywords = {Software engineering, Computer architecture, Technical debts, Economics, Architectural structure, Design rules, Economic models, Flawed structure, Industrial software, Large-scale software systems, Recent researches, Business, History, Microprocessors, Sonar detection, xno, xeee}
}

@inproceedings{xuedong_strategy_2017,
	title = {Strategy of diagnosis and test in {ICCS} of large-scale laser facility},
	doi = {10.1109/CAC.2017.8243207},
	abstract = {Integrated Computer Control System (abbr. {ICCS}) of large-scale laser facility uses a scalable software architecture to manage more than 10,000 control points to operate 48 powerful laser beamlines, provides for the integration of all elements of laser and target area distributed subsystems to form an overall operational control system. Fault detection and diagnosis test are the important technique to ensure the system's safety, reliability and healthy manage. This paper is an overview of the diagnostic and test strategy used in {ICCS} of large-scale laser facility, including design rules, task decomposition strategy, classification strategy, the scheme of on-line diagnosis and offline diagnosis, diagnosis process and data preconditioning about data standardization and quality control.},
	pages = {2563--2566},
	booktitle = {2017 Chinese Automation Congress ({CAC})},
	author = {Xuedong, Zhang and Xiaoli, Wang and Tianyou, Yun},
	date = {2017-10},
	keywords = {Software, Testing, Computer aided diagnosis, Fault detection, Hardware, Computer control systems, Data preconditioning, Data standardization, Distributed subsystems, Fault detection and diagnosis, {ICCS}, Integrated computer control systems, On-line diagnosis, Operational control, Quality control, Standardization, Fault diagnosis, Control systems, diagnosis and test, Laser theory, off-line diagnosis, on-line diagnosis, standardization, xno}
}

@inproceedings{de_araujo_neto_fault-tolerant_2018,
	title = {A Fault-Tolerant Agent-Based Architecture for Transient Servers in Fog Computing},
	doi = {10.1109/CAHPC.2018.8645859},
	abstract = {Cloud datacenters are exploring their idle resources and offering virtual machine as transient servers without availability guarantees. Spot instances are transient servers offered by Amazon {AWS}, with rules that define prices according to supply and demand. These instances will run for as long as the current price is lower than the maximum bid price given by users. Spot instances have been increasingly used for executing computation and memory intensive applications. By using dynamic fault tolerant mechanisms and appropriate strategies, users can effectively use spot instances to run applications at a cheaper price. This paper presents a resilient multi-strategy agent-based cloud computing architecture. The architecture combines machine learning and a statistical model to predict instance survival times, refine fault tolerance parameters and reduce total execution time. We evaluate our strategies and the experiments demonstrate high levels of accuracy, reaching a 94\% survival prediction success rate, which indicates that the model can be effectively used to define execution strategies to prevent failures at revocation events under realistic working conditions.},
	pages = {282--289},
	booktitle = {2018 30th International Symposium on Computer Architecture and High Performance Computing ({SBAC}-{PAD})},
	author = {de Araujo Neto, Jose Pergentino and Pianto, Donald M. and Ralha, Celia G.},
	date = {2018-09},
	note = {{ISSN}: 1550-6533},
	keywords = {Computer architecture, Fault tolerance, Cloud computing, Computational modeling, Servers, Silicon, Transient analysis, Agent-based Architecture, Checkpoint, Cloud Computing, Fault Tolerance, Resilient, Spot Instance, xno}
}

@inproceedings{sini_towards_2018,
	title = {Towards an automatic approach for hardware verification according to {ISO} 26262 functional safety standard},
	doi = {10.1109/IOLTS.2018.8474083},
	abstract = {The Failure Mode, Effect and Diagnostic Analysis ({FMEDA}) is a technique widely adopted by automotive industry to assess the level of reliability of hardware designs. Although very useful, it has the problem of taking a long time to complete and requires experts with extensive knowledge of the circuit under consideration. In this paper, it is presented a comparison between the analysis results obtained from an automatic tool developed by the authors with respect to the ones obtained by hand from a team of experts, followed by a critical review of the strengths and weaknesses, about the rules for automatic classification of the faults effects.},
	pages = {287--290},
	booktitle = {2018 {IEEE} 24th International Symposium on On-Line Testing And Robust System Design ({IOLTS})},
	author = {Sini, J. and Sonza Reorda, M. and Violante, M. and Sarson, P.},
	date = {2018-07},
	note = {{ISSN}: 1942-9401},
	keywords = {Software, Automatic classification, Reliability analysis, Software reliability, Measurement, Failure analysis, Computer software, Accident prevention, Automatic approaches, Automobile electronic equipment, Automotive industry, Circuit faults, Computer hardware, Critical review, Diagnostic analysis, Embedded systems, Functional Safety, Hardware, Hardware verification, {ISO} 26262, {ISO} Standards, Microcontrollers, Reliability, Systems analysis, Tools, Integrated circuit modeling, Safety, Automotive electronics, failure analysis, {ISO} 26262 standard, xno}
}

@article{wang_practical_2017,
	title = {Practical Network-Wide Packet Behavior Identification by {AP} Classifier},
	volume = {25},
	issn = {1558-2566},
	doi = {10.1109/TNET.2017.2720637},
	abstract = {Identifying the network-wide forwarding behaviors of a packet is essential for many network management applications, including rule verification, policy enforcement, attack detection, traffic engineering, and fault localization. Current tools that can perform packet behavior identification either incur large time and memory costs or do not support real-time updates. In this paper, we present {AP} Classifier, a control plane tool for packet behavior identification. {AP} Classifier is developed based on the concept of atomic predicates, which can be used to characterize the forwarding behaviors of packets. Experiments using the data plane network state of two real networks show that the processing speed of {AP} Classifier is faster than existing tools by at least an order of magnitude. Furthermore, {AP} Classifier uses very small memory and is able to support real-time updates.},
	pages = {2886--2899},
	number = {5},
	journaltitle = {{IEEE}/{ACM} Transactions on Networking},
	author = {Wang, Huazhe and Qian, Chen and Yu, Ye and Yang, Hongkun and Lam, Simon S.},
	date = {2017-10},
	keywords = {Computer aided diagnosis, Fault detection, Software defined networking, Failure analysis, Interactive computer systems, Real time systems, Computer networks, Data structures, Fault localization, {IEEE} transactions, Management applications, Packet classification, Packet networks, Policy enforcement, Ports (Computers), Practical networks, Throughput, Tools, Traffic Engineering, Fault diagnosis, Network-wide behavior, packet classification, Real-time systems, software-defined networking, xno}
}

@inproceedings{li_fractal_2010,
	title = {Fractal study on fault system of Carboniferous in Junggar Basin based on {GIS}},
	doi = {10.1109/GEOINFORMATICS.2010.5567793},
	abstract = {Fault system is a significant evidence of tectonic movement during crust tectonic evolution and may play an more important role in oil-gas accumulation process than other tectonic types in sedimentary basin. Carboniferous surface faults in Junggar Basin developed well and varied in size and distribution. There are about 200 faults in Carboniferous, and 187 of them are thrust faults. Chaos-fractals theories have been widely investigated and great progress has been made in the past three decades. One of the important conception-fractal dimension had become a powerful tool for describing non-linearity dynamical system characteristic. The clustered objects in nature are often fractal and fault system distribution in space is inhomogeneous, always occurs in groups, so we can describe spatial distribution of faults from the point of fractal dimension. Fractal dimension of fault system is a comprehensive factor associated with fault number, size, combination modes and dynamics mechanism, so it can evaluate the complexity of fault system quantitatively. The relationship between fault system and oil-gas accumulation is a focus and difficulty problem in petroleum geology, and fractal dimension is a new tool for describing fault distribution and predicting potential areas of hydrocarbon resources. Geographic Information System ({GIS}) is a kind of technological system collecting, storing, managing, computing, analyzing, displaying and describing the geospatial information supported by computer software and hardware. In the last 15-20 years, {GIS} have been increasingly used to address a wide variety of geoscience problems. Weights-of-evidence models use the theory of conditional probability to quantify spatial association between fractal dimension and oil-gas accumulation. The weights of evidence are combined with the prior probability of occurrence of oil-gas accumulation using Bayes'rule in a loglinear form under an assumption of conditional independence of the dimension maps to derive posterior probability of occurrence of oil-gas accumulation. In this paper, we first vectorize the fault system in Carboniferous of Junggar Basin in {GIS} software and store it as polyline layer in Geodatabase of {GIS} to manage and analyze, then calculate the fractal dimension of three types which are box dimension, information dimension and cumulative length dimension using spatial functions of {GIS}, in the last use weights-of-evidence model to calculate the correlation coefficients in {GIS} environment between oil-gas accumulation and three types of fractal dimension in order to quantity the importance of fault system.},
	pages = {1--5},
	booktitle = {2010 18th International Conference on Geoinformatics},
	author = {Li, Bo and Zhang, Tingshan and Ding, Guangming and Wang, Weiyuan and Xiang, Yu},
	date = {2010-06},
	note = {{ISSN}: 2161-0258},
	keywords = {Partial discharges, Analytical models, Weights of evidences, Gases, Computer hardware, Information systems, Bayes' rule, Box dimension, Combination modes, Conditional independences, Conditional probabilities, Correlation coefficient, Dynamical systems, Evidence model, Fault distribution, Fault system, Fractal dimension, Fractal studies, Geodatabase, Geographic information, Geographic information systems, Geosciences, Geospatial information, {GIS} software, Hydrocarbon resources, Hydrocarbons, Information dimensions, Junggar Basin, Non-Linearity, Oil-gas accumulation, Petroleum geology, Posterior probability, Prior probability, Probability, Sedimentary basin, Size distribution, Spatial distribution, Spatial functions, Surface faults, System characteristics, Technological system, Tectonic evolution, Tectonic movements, Tectonics, Thrust faults, Computational modeling, Correlation, fault system, fractal dimension, Fractals, Geographic Information System, Geographic Information Systems, Geology, oil-gas accumulation, Petroleum, weights-of-evidence model, xno}
}

@inproceedings{femmer_which_2017,
	title = {Which Requirements Artifact Quality Defects are Automatically Detectable? A Case Study},
	doi = {10.1109/REW.2017.18},
	abstract = {[Context:] The quality of requirements engineering artifacts, e.g. requirements specifications, is acknowledged to be an important success factor for projects. Therefore, many companies spend significant amounts of money to control the quality of their {RE} artifacts. To reduce spending and improve the {RE} artifact quality, methods were proposed that combine manual quality control, i.e. reviews, with automated approaches. [Problem:] So far, we have seen various approaches to automatically detect certain aspects in {RE} artifacts. However, we still lack an overview what can and cannot be automatically detected. [Approach:] Starting from an industry guideline for {RE} artifacts, we classify 166 existing rules for {RE} artifacts along various categories to discuss the share and the characteristics of those rules that can be automated. For those rules, that cannot be automated, we discuss the main reasons. [Contribution:] We estimate that 53\% of the 166 rules can be checked automatically either perfectly or with a good heuristic. Most rules need only simple techniques for checking. The main reason why some rules resist automation is due to imprecise definition. [Impact:] By giving first estimates and analyses of automatically detectable and not automatically detectable rule violations, we aim to provide an overview of the potential of automated methods in requirements quality control.},
	pages = {400--406},
	booktitle = {2017 {IEEE} 25th International Requirements Engineering Conference Workshops ({REW})},
	author = {Femmer, Henning and Unterkalmsteiner, Michael and Gorschek, Tony},
	date = {2017-09},
	keywords = {Semantics, Automation, Quality control, Manuals, Conferences, Writing, Artifact Quality, Automated Methods, Guidelines, Requirement Engineering, xno},
	file = {Submitted Version:C\:\\Users\\michalm\\Zotero\\storage\\NIK8I9EP\\Femmer et al. - 2017 - Which Requirements Artifact Quality Defects are Au.pdf:application/pdf}
}

@article{maddeh_decision_2021,
	title = {Decision tree-based Design Defects Detection},
	volume = {9},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2021.3078724},
	abstract = {Design defects affect project quality and hinder development and maintenance. Consequently, experts need to minimize these defects in software systems. A promising approach is to apply the concepts of refactoring at higher level of abstraction based on {UML} diagrams instead of code level. Unfortunately, we find in literature many defects that are described textually and there is no consensus on how to decide if a particular design violates model quality. Defects could be quantified as metrics based rules that represent a combination of software metrics. However, it is difficult to find manually the best threshold values for these metrics. In this paper, we propose a new approach to identify design defects at the model level using the {ID}3 decision tree algorithm. We aim to create a decision tree for each defect. We experimented our approach on four design defects: The Blob, Data class, Lazy class and Feature Envy defect, using 15 Object-Oriented metrics. The rules generated using decision tree give a very promising detection results for the four open source projects tested in this paper. In Lucene 1.4 project, we found that the precision is 67\% for a recall of 100\%. In general, the accuracy varies from 49\%, reaching for Lucene 1.4 project 80\%.},
	pages = {71606--71614},
	journaltitle = {{IEEE} Access},
	author = {Maddeh, Mohamed and Ayouni, Sarra and Alyahya, Sultan and Hajjej, Fahima},
	date = {2021},
	keywords = {Predictive models, Software, Decision trees, Anti-patterns, Measurement, Software algorithms, Object oriented modeling, Unified modeling language, bad smells, decision tree, model refactoring, object oriented metrics, xyes},
	file = {Full Text:C\:\\Users\\michalm\\Zotero\\storage\\VI9T3DA7\\Maddeh et al. - 2021 - Decision tree-based Design Defects Detection.pdf:application/pdf}
}

@inproceedings{mas_machuca_technology-related_2016,
	title = {Technology-related disasters: A survey towards disaster-resilient Software Defined Networks},
	doi = {10.1109/RNDM.2016.7608265},
	abstract = {Resilience against disaster scenarios is essential to network operators, not only because of the potential economic impact of a disaster but also because communication networks form the basis of crisis management. {COST} {RECODIS} aims at studying measures, rules, techniques and prediction mechanisms for different disaster scenarios. This paper gives an overview of different solutions in the context of technology-related disasters. After a general overview, the paper focuses on resilient Software Defined Networks.},
	pages = {35--42},
	booktitle = {2016 8th International Workshop on Resilient Networks Design and Modeling ({RNDM})},
	author = {Mas Machuca, Carmen and Secci, Stefano and Vizarreta, Petra and Kuipers, Fernando and Gouglidis, Antonios and Hutchison, David and Jouet, Simon and Pezaros, Dimitrios and Elmokashfi, Ahmed and Heegaard, Poul and Ristov, Sasko and Gusev, Marjan},
	date = {2016-09},
	keywords = {Power system protection, Software defined networking, Analytical models, Power system faults, Buildings, disaster survivability, Internet, resilience, Resilience, software defined networks, xno}
}

@inproceedings{rojas_contextualized_2014,
	title = {Contextualized early failure characterization of cantilever snap assemblies},
	doi = {10.1109/HUMANOIDS.2014.7041388},
	abstract = {Failure detection and correction is essential in robust systems. In robotics, failure detection has focused on traditional parts assembly, tool breakage, and threaded fastener assembly. However, not much work has focused on classifying failure into various sub-modes. This is an important step in order to provide accurate failure recovery. Our work implemented a contextualized failure characterization scheme for cantilever snap assemblies. A rule based approach was used through which assemblies whose trajectories deviated from the normal approach trajectory were identified in the beginning of the task. We not only identified failure but also the failure type that occurred. The method identified exemplars that characterized salient features for specific deviations from the initial approach trajectory in the assembly task. A contact-state map was generated through sampling the contact space during training. Contextualized statistical measures were used to classify trials during the testing phase. Our work classified failure deviations with 88\% accuracy. According to the statistic measures used, varying success was experienced in correlating failure deviation modes. Each case was analyzed using gaussian statistics and one and two standard deviations. Cases with trajectory deviations in one direction had 75\%, 92\% accuracy, cases with deviations in two directions had 61\%, 94\% accuracy, and cases with deviations in three directions had 69\%, 100\% accuracy. Our work provides further insights into the early failure characterization of complex geometrical parts which will serve to implement failure recovery techniques in the face of significant and unexpected errors.},
	pages = {380--387},
	booktitle = {2014 {IEEE}-{RAS} International Conference on Humanoid Robots},
	author = {Rojas, Juan and Harada, Kensuke and Onda, Hiromu and Yamanobe, Natsuki and Yoshida, Eiichi and Nagata, Kazuyuki},
	date = {2014-11},
	note = {{ISSN}: 2164-0580},
	keywords = {Testing, Training, Assembly, Force, Robots, Standards, Trajectory, xno}
}

@inproceedings{wang_state_2019,
	title = {State Assessment of Relay Protection Device Based on Defect Information},
	doi = {10.1109/EI247390.2019.9062256},
	abstract = {Considering the state assessment of relay protection device gray and fuzzy, an analytical method based on the combination of grey correlation analysis and apriori correlation analysis is proposed in this paper. Defect information data from the operation and maintenance of actual substations are used to assess and predict the status of relay protection devices which are affected by different manufacturers, different equipment types, operation time and maintenance records. Seven types of correlation rules are obtained in the research, and the analytical results can provide a strong scientific basis for targeted maintenance and operation plans based on the specific results in the correlation rules. The results provide a software package for the evaluation of relay protection devices in a regional substation of Shanghai power supply company of the State Grid.},
	pages = {1499--1503},
	booktitle = {2019 {IEEE} 3rd Conference on Energy Internet and Energy System Integration ({EI}2)},
	author = {Wang, Fa and Xiao, Jinxing and Feng, Jie and Xiong, Fenfang and Huang, Liangliang and Xu, Hongchao},
	date = {2019-11},
	keywords = {Defects, Electric power system protection, Operation and maintenance, Correlation analysis, Correlation methods, Different equipment, Grey correlation analysis, Maintenance, Maintenance and operation, Maintenance records, Power supply company, Relay protection, Relay protection devices, Maintenance engineering, Indexes, Correlation, Apriori correlation analysis, Companies, Condition assessment, Defect information, Power supplies, Protective relaying, Relay protection device, Substations, xno}
}

@inproceedings{wang_mining_2020,
	title = {Mining diagnostic knowledge from spacecraft data based on Spark cluster},
	doi = {10.1109/ICIEA48937.2020.9248423},
	abstract = {Compared with the data obtained from ground simulation experiments, the spacecraft telemetry data can better reflect the real working state of the spacecraft. How to effectively utilize telemetry data and extract effective information is an important issue. This paper uses diagnostic data from real spacecraft telemetry to mine diagnostic knowledge and build a diagnostic knowledge base. Compared with the traditional fault diagnosis method based on expert knowledge, the diagnostic knowledge mined can enrich the existing expert knowledge base. In this paper, the {FP}-Growth algorithm is used to mine the association rules of the parameters to obtain the diagnostic knowledge, and a satellite telemetry data diagnosis knowledge base is constructed. Mined diagnostic knowledge includes association rules among parameters, and the relationship between parameters and faults. In addition, due to the large number of telemetry parameters, the amount of data reaching {TB} level, the Spark distributed computing cluster is used to implement distributed and efficient computing of the algorithm. Finally, building a spacecraft telemetry data mining diagnostic platform with the Django architecture.},
	pages = {1762--1767},
	booktitle = {2020 15th {IEEE} Conference on Industrial Electronics and Applications ({ICIEA})},
	author = {Wang, Haoran and Yu, Jinsong and Tang, Diyin and Han, Danyang and Tian, Limei and Dai, Jing},
	date = {2020-11},
	note = {{ISSN}: 2158-2297},
	keywords = {Clustering algorithms, Data mining, Knowledge based systems, Classification algorithms, Software algorithms, association rules, big data mining, distributed computing, Space vehicles, Spark cluster, Telemetry, Telemetry data, xno}
}

@inproceedings{yang_future_2019,
	title = {The Future of Broadband Access Network Architecture and Intelligent Operations},
	doi = {10.1109/CyberC.2019.00060},
	abstract = {This paper presents an overview of the evolution towards a broadband optical access network. Network operations transformation with emphasis on automated broadband service monitoring, fault detection, fault recovery, and fault prediction is also discussed. However, in order to support broadband access network evolution and to accelerate telco operations transformation, traditional network operations, administration and maintenance ({OAM}) methods will not be sufficient. Therefore, we describe our key research contributions regarding new approaches to support closed-loop broadband network {OAM} that will eliminate human touches and automate non-physical fault recovery for daily routine tasks. We have designed and implemented a network knowledge engine ({NKE}) centered software framework to enable data collection, correlation, and analysis for troubleshooting complex issues in the broadband access network. Our solutions will address the following network management domains, including (a) knowledge management, (b) surveillance management, (c) incident management, and (d) problem management. In this paper, we focus on auto proactive and predictive trouble management which can verify and confirm a passive device problem location by {NKE}. We will describe a step-by-step methodology to control data process flow, optical fiber path discovery management, and auto-detection of failed passive devices by using the following techniques: (a) big data analytics, (b) knowledge graph, (c) machine learning ({ML}), and (d) artificial intelligence ({AI}) prediction rule and learning policy involving the spatial-temporal algorithm. With combinations of these technique, our preliminary study indicates that the accuracy of auto fault detection is 93\%, while the false positive rate is 0.01\%. As results, we demonstrated the promise of this novel technology framework in supporting transformation from expert's domain knowledge into machine knowledge.},
	pages = {308--316},
	booktitle = {2019 International Conference on Cyber-Enabled Distributed Computing and Knowledge Discovery ({CyberC})},
	author = {Yang, Charlie Chen-Yui and Li, Guangzhi and Liu, Xiang and Wu, Zonghuan and Zhang, Kaiyu},
	date = {2019-10},
	keywords = {Machine learning, Fault detection, Advanced Analytics, Big data, Broad-band access networks, Broadband networks, Computer programming, Computer system recovery, Data Analytics, False positive rates, Flow graphs, Incident Management, Intelligent operations, Knowledge graphs, Knowledge management, Network architecture, Network operations, Optical access networks, Optical fibers, Software frameworks, Wireless communication, Bandwidth, Big Data Analytics, Broadband Access Network, Broadband communication, Cable {TV}, Coaxial cables, Knowledge engineering, Knowledge Graph, {ML} and {AI}, Telephone sets, xno}
}

@inproceedings{zhou_autonomic_2016,
	title = {Autonomic Parallelism and Thread Mapping Control on Software Transactional Memory},
	doi = {10.1109/ICAC.2016.54},
	abstract = {Parallel programs need to manage the trade-off between the time spent in synchronization and computation. The time trade-off is affected by the number of active threads significantly. High parallelism may decrease computing time while increase synchronization cost. Furthermore thread locality on different cores may impact on program performance too, as the memory access time can vary from one core to another due to the complexity of the underlying memory architecture. Therefore the performance of a program can be improved by adjusting the number of active threads as well as the mapping of its threads to physical cores. However, there is no universal rule to decide the parallelism and the thread locality for a program from an offline view. Furthermore, an offline tuning is error-prone. In this paper, we dynamically manage parallelism and thread localities. We address multiple threads problems via Software Transactional Memory ({STM}). {STM} has emerged as a promising technique, which bypasses locks, to address synchronization issues through transactions. Autonomic computing offers designers a framework of methods and techniques to build autonomic systems with well-mastered behaviours. Its key idea is to implement feedback control loops to design safe, efficient and predictable controllers, which enable monitoring and adjusting controlled systems dynamically while keeping overhead low. We propose to design a feedback control loop to automate thread management at runtime and diminish program execution time.},
	pages = {189--198},
	booktitle = {2016 {IEEE} International Conference on Autonomic Computing ({ICAC})},
	author = {Zhou, Naweiluo and Delaval, Gwenaël and Robu, Bogdan and Rutten, Éric and Méhaut, Jean-François},
	date = {2016-07},
	keywords = {Throughput, Runtime, autonomic, feedback control, Feedback control, Instruction sets, Parallel processing, parallelism adaptation, synchronization, Synchronization, thread affinity, transactional memory, xno}
}

@article{mahela_recognition_2020,
	title = {Recognition of Complex Power Quality Disturbances Using S-Transform Based Ruled Decision Tree},
	volume = {8},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2020.3025190},
	abstract = {Deteriorated quality of power leads to problems, such as equipment failure, automatic device resets, data errors, failure of circuit boards, loss of memory, power supply issues, uninterrupted power supply ({UPS}) systems generate alarm, corruption of software, and heating of wires in distribution network. These problems become more severe when complex (multiple) power quality ({PQ}) disturbances appear. Hence, this manuscript introduces an algorithm for identification of the complex nature {PQ} events in which it is supported by Stockwell's transform ({ST}) and decision tree ({DT}) using rules. {PQ} events with complex nature are generated in view of {IEEE}-1159 standard. Eighteen different types of complex {PQ} issues are considered and studied which include second, third, and fourth order disturbances. These are obtained by combining the single stage {PQ} events such as sag \& swell in voltage, momentary interruption ({MI}), spike, flicker, harmonics, notch, impulsive transient ({IT}), and oscillatory transient ({OT}). The {ST} supported frequency contour and proposed plots such as amplitude, summing absolute values, phase and frequency-amplitude obtained by multi-resolution analysis ({MRA}) of signals are used to identify the complex {PQ} events. The statistical features such as sum factor, Skewness, amplitude factor, and Kurtosis extracted from these plots are utilized to classify the complex {PQ} events using rule-based {DT}. This is established that proposed approach effectively identifies a number of complex nature {PQ} events with accuracy above 98\%. Performance of the proposed method is tested successfully even with noise level of 20 {dB} signal to noise ratio ({SNR}). Effectiveness of the proposed algorithm is established by comparing it with the methods reported in literature such as fuzzy c-means clustering ({FCM}) \& adaptive particle swarm optimization ({APSO}), Wavelet transform ({WT}) \& neural network ({NN}), spline {WT} \& {ST}, {ST} \& {NN}, and {ST} \& fuzzy expert system ({FES}). Results of simulations are validated by comparing them with real time results computed by Real Time Digital Simulator ({RTDS}). Different stages for design of complex {PQ} monitoring device using the proposed approach are also described. It is verified that the proposed approach can effectively be employed for design of the online complex {PQ} monitoring devices.},
	pages = {173530--173547},
	journaltitle = {{IEEE} Access},
	author = {Mahela, Om Prakash and Shaik, Abdul Gafoor and Khan, Baseem and Mahla, Rajendra and Alhelou, Hassan Haes},
	date = {2020},
	keywords = {Feature extraction, Decision trees, Adaptive particle swarm optimizations, Alarm systems, Complex networks, Electric fault currents, Electric power supplies to apparatus, Expert systems, Fuzzy C means clustering, Higher order statistics, Momentary Interruptions, Oscillatory transients, Outages, Particle swarm optimization ({PSO}), Power quality, Power quality disturbances, Real time digital simulator, Signal to noise ratio, Statistical features, Trees (mathematics), Uninterrupted power supply, Uninterruptible power systems, Wavelet transforms, Monitoring, Transforms, power quality, Complex nature {PQ} event, ruled decision tree, statistical feature, Stockwell's transform, Time-frequency analysis, xno}
}

@inproceedings{rothe_comparison_2016,
	title = {Comparison of different information fusion methods using ensemble selection considering Benchmark data},
	abstract = {The overall system reliability of complex or safety critical systems is of increasing importance in a lot of application fields. To ensure a high accuracy of the decisions evaluating situations or conditions, the assignments from different classifiers can be fused to one final decision. The kind of selection as well as the individual properties of suitable classifiers to be fused are crucial for the overall accuracy. In this contribution a detailed comparison of three different fusion methods (Weighted Voting, Bayesian Combination Rule, and Dempster-Shafer Combination) using different ensemble selection strategies (Static Classifier Ensemble and Dynamic Classifier Ensemble) is given. Using a set of Benchmark data the results are numerically analyzed concerning the number and quality of classifiers to be combined. Based on this (general) example questions about a suitable combination of ensemble selection and fusion method can be answered.},
	pages = {73--78},
	booktitle = {2016 19th International Conference on Information Fusion ({FUSION})},
	author = {Rothe, Sandra and Söffker, Dirk},
	date = {2016-07},
	keywords = {Genetic algorithms, Fault diagnosis, Vegetation, Benchmark testing, Bayes methods, xno}
}

@inproceedings{singh_recognition_2020,
	title = {Recognition of Disturbances in Hybrid Grid Using Discrete Wavelet Transform and Stockwell Transform Based Algorithm},
	doi = {10.1109/ICMICA48462.2020.9242867},
	abstract = {An algorithm supported by features of both discrete wavelet transform ({DWT}) and Stockwell transform ({ST}) has been proposed for recognition of the disturbances on the grid. Voltage signals recorded on the point of common coupling ({PCC}) of hybrid grid test system is decomposed with the help of {DWT} and a proposed {DWT}-index is extracted. Similarly, same voltage signal is decomposed and a proposed S-index is extracted. A proposed disturbance index ({DI}) is calculated element by element multiplication of {DWT}-index and S-index. Relative values of this index indicate the level of disturbances in the test network. Classification of disturbances due to the operational events and faulty events is performed using the decision tree based on rules. It is anticipated that the proposed hybrid power grid will be effective to achieve the objective of retiring the thermal power plants in India by 2029-30 as decided by the Central electricity authority ({CEA}) of the India and proposed algorithm will effectively recognize the various disturbances in the utility grid. Proposed study is carried out using {MATLAB} software.},
	pages = {1--6},
	booktitle = {2020 First {IEEE} International Conference on Measurement, Instrumentation, Control and Automation ({ICMICA})},
	author = {Singh, Aditi and Khan, Ahmad Hasan and Kumar Sharma, Ankit and Mahela, Om Prakash},
	date = {2020-06},
	keywords = {Software, Decision trees, Software algorithms, Indexes, Power grids, Stockwell transform, Transforms, Direct current, discrete wavelet transform, Discrete wavelet transforms, hybrid grid, xno}
}

@inproceedings{tiwari_study_2020,
	title = {Study of Combined Time Current Grading Protection Scheme for Distribution System},
	doi = {10.1109/PARC49193.2020.236651},
	abstract = {The paper paraphrases the power system protection principle using overcurrent relaying, distribution network configurations and relay coordination to enhance reliability of power supply. The distribution network topology and its protection schemes are described in detail. The most simple, cheapest \& high speed scheme of protections i.e. overcurrent and their limitations are reported. Rules for setting of relays operating time and pickup current setting under time graded, current graded and in combined time \& current graded protection schemes for various network configurations are examined. A {PSCAD}/{EMTDC} software environment is used to simulate the radial distribution network and {MATLAB} software for analysis. The variation in fault current and fault {MVA} with change in fault position are simulated. Result indicates the time and current setting for relays under combined time and current protection scheme. Instant of fault clearance and relay trip signal reflects coordination of primary and back protection. by proper selecting the faulty zone in case of combine setting of both time and current of relays. In addition the classification of overcurrent relay as per {ANSI}/{IEEE}, {IEC} standards and the protection zones for radial, ring main and parallel network are inferred \& the study engrossed on a small portion of distribution network consisting four subsection with load on each.},
	pages = {443--448},
	booktitle = {2020 International Conference on Power Electronics {IoT} Applications in Renewable Energy and its Control ({PARC})},
	author = {Tiwari, Ravi Shankar and Hari Gupta, Om},
	date = {2020-02},
	keywords = {{MATLAB}, Internet of things, Distribution network configuration, Distribution network topology, Electric power system protection, Grading, Network configuration, Overcurrent protection, Pickup current settings (Ip), Power electronics, Power system protection, Radial distribution networks, Reliability of power supply, Software environments, Current grading, Distribution system, {EMTDC}, {IDMT}, Ring main, Time graded, Time grading, xno}
}

@inproceedings{ferreira_da_silva_self-healing_2012,
	title = {Self-Healing of Operational Workflow Incidents on Distributed Computing Infrastructures},
	doi = {10.1109/CCGrid.2012.24},
	abstract = {Distributed computing infrastructures are commonly used through scientific gateways, but operating these gateways requires important human intervention to handle operational incidents. This paper presents a self-healing process that quantifies incident degrees of workflow activities from metrics measuring long-tail effect, application efficiency, data transfer issues, and site-specific problems. These metrics are simple enough to be computed online and they make little assumptions on the application or resource characteristics. Incidents are classified in levels and associated to sets of healing actions that are selected based on association rules modeling correlations between incident levels. The healing process is parametrized on real application traces acquired in production on the European Grid Infrastructure. Implementation and experimental results obtained in the Virtual Imaging Platform show that the proposed method speeds up execution up to a factor of 4 and properly detects unrecoverable errors.},
	pages = {318--325},
	booktitle = {2012 12th {IEEE}/{ACM} International Symposium on Cluster, Cloud and Grid Computing (ccgrid 2012)},
	author = {Ferreira da Silva, Rafael and Glatard, Tristan and Desprez, Frédéric},
	date = {2012-05},
	keywords = {Association rules, Measurement, Monitoring, Logic gates, Computational modeling, Error detection and handling, Production distributed systems, Wheels, Workflow execution, xno}
}

@inproceedings{ji_visual_2011,
	title = {Visual Recognition Using Density Adaptive Clustering},
	doi = {10.1109/MUE.2011.23},
	abstract = {Visual codebook based texture analysis and image recognition is popular for its robustness to affine transformation and illumination variation. It is based on the affine invariable descriptors of local patches extracted by region detector, and then represents the image by histogram of the codebook constructed by the feature vector quantization. The most commonly used vector quantization method is k-means. But due to the limitations of predefined number of clusters and local minimum update rule, we show that k-means would fail to code the most discriminable descriptors. Another defect of k-means is that the computational complexity is extremely high. In this paper, we proposed a nonparametric vector quantization method based on mean shift, and use locality-sensitive hashing ({LSH}) to reduce the cost of the nearest neighborhood query in the mean-shift iterations. The performance of proposed method is demonstrated in several image classification tasks. We also show that the Information Gain or Mutual Information based feature selection based on our codebook further improves the performance.},
	pages = {67--72},
	booktitle = {2011 Fifth {FTRA} International Conference on Multimedia and Ubiquitous Engineering},
	author = {Ji, Lei and Qin, Zheng and Chen, Kai and Li, Huan},
	date = {2011-06},
	keywords = {Feature extraction, Accuracy, Kernel, affine invariance, bag of features, Detectors, Histograms, image classification, Lighting, local features, mean shift, Support vector machine classification, xno}
}

@inproceedings{lin_predicting_2020,
	title = {Predicting Remediations for Hardware Failures in Large-Scale Datacenters},
	doi = {10.1109/DSN-S50200.2020.00016},
	abstract = {Large-scale service environments rely on autonomous systems for remediating hardware failures efficiently. In production, the autonomous system diagnoses hardware failures based on the rules that the subject matter experts put in the system. This process is increasingly complex given new types of failures and the increasing complexity in the hardware and software configurations. In this paper, we present a machine learning framework that predicts the required remediations for undiagnosed failures, based on the similar repair tickets closed in the past. We explain the methodology in detail for setting up a machine learning model, deploying it in a production environment, and monitoring its performance with the necessary metrics. We also demonstrate the prediction performance on some of the repair actions.},
	pages = {13--16},
	booktitle = {2020 50th Annual {IEEE}-{IFIP} International Conference on Dependable Systems and Networks-Supplemental Volume ({DSN}-S)},
	author = {Lin, Fred and Davoli, Antonio and Akbar, Imran and Kalmanje, Sukumar and Silva, Leandro and Stamford, John and Golany, Yanai and Piazza, Jim and Sankar, Sriram},
	date = {2020-06},
	keywords = {Predictive models, Machine learning, Hardware, Data models, Maintenance engineering, Servers, Production, n/a, xno}
}

@inproceedings{mutlu_end--end_2018,
	title = {End-to-End Hierarchical Fuzzy Inference Solution},
	doi = {10.1109/FUZZ-IEEE.2018.8491481},
	abstract = {Hierarchical Fuzzy System ({HFS}) is a popular approach for handling curse of dimensionality problem occurred in complex fuzzy rule-based systems with various and numerous inputs. However, the processes of modeling and reasoning of {HFS} have some critical issues to be considered. In this study, the effect of these issues on the accuracy and stability of the resulting system has been investigated, and an end-to-end {HFS} framework has been proposed. The proposed framework has three main steps such as single system modeling, rule partitioning and {HFS} reasoning. It is fully automated, generic, almost independent from data, and applicable for any kind of inference problem. In addition, the proposed framework preserves accuracy and stability during the {HFS} reasoning. These judgments have been ensured by a number of experimental studies on several datasets about software faulty prediction ({SFP}) problem with a large feature space. The main contributions of this paper are as follows: (i) it provides the entire {HFS} implementation from problem definition to calculation of final output, (ii) it increases the accuracy of recently proposed rule generation scheme in the literature, (iii) it presents the only possible fuzzy system solution for {SFP} problem containing a large feature space with reasonable accuracy.},
	pages = {1--9},
	booktitle = {2018 {IEEE} International Conference on Fuzzy Systems ({FUZZ}-{IEEE})},
	author = {Mutlu, Begum and Sezer, Ebru A. and Ali Akcayol, M.},
	date = {2018-07},
	keywords = {Fuzzy inference, Fuzzy systems, Fuzzy sets, Decision making, Critical issues, Curse of dimensionality, Fully automated, Hierarchical fuzzy, Hierarchical fuzzy systems, Hierarchical systems, Inference problem, Problem definition, Reasonable accuracy, Cognition, Production, Hafnium, Stability analysis, xyes}
}

@inproceedings{havryliuk_detecting_2020,
	title = {Detecting of Signal Distortions in Cab Signalling System Using {ANFIS} and {WPESE}},
	doi = {10.1109/IEPS51250.2020.9263165},
	abstract = {The problem considered in the work is concerned to detecting of signal distortions occurred in the railway {ALSN} cab signaling system. The {ALSN} system is designed to transmit track status information into the train cab and uses rails as a continuous communication channel between track and train. The amplitude and duration of the pulses in the {ALSN} code combinations are changed over time due to deterioration of the track transmitters and other devices in the signal transmission channel, as well as due to electromagnetic influence of the traction current, rails magnetization, and other sources of electromagnetic interference. Due to distortions of {ALSN} signals, their decoding becomes unstable, which leads to intermittent failures in the form of temporary incorrect indication at the cab traffic light or to complete failure of the {ALSN} system. Diagnostic of the {ALSN} system and the revealing of signals with distortions is carried out by analyzing the signal current, recorded using the railway car-laboratory. However, the use for this purpose of the classifiers with sharp boundaries for input diagnostic parameters and strict rules for signal selection does not allow us to reveal incipient defects that arise in the {ALSN} system. The work investigates the effectiveness of using adaptive neuro-fuzzy inference system ({ANFIS}) and wavelet packet energy Shannon entropy ({WPESE}) for timely detecting of signal distortions in the {ALSN} system. The obtained results confirmed the efficiency of {ALSN} signal processing using {ANFIS} and {WPESE} for detecting of railway sections with unstable or faulty {ALSN} system.},
	pages = {231--236},
	booktitle = {2020 {IEEE} 4th International Conference on Intelligent Energy and Power Systems ({IEPS})},
	author = {Havryliuk, Volodymyr},
	date = {2020-09},
	keywords = {fault detection, Rail transportation, adaptive neuro-fuzzy inference system, cab signalling system, Decoding, Distortion, Rails, signal disturbances, Transmitters, Wavelet analysis, Wavelet packets, wavelet transform, xno}
}

@inproceedings{mijumbi_mayor_2019,
	title = {{MAYOR}: Machine Learning and Analytics for Automated Operations and Recovery},
	doi = {10.1109/ICCCN.2019.8847092},
	abstract = {Communications systems continuously generate a big number of alarms. Such alarms are usually monitored by network operations centers ({NOCs}) from where steps to resolve the causes are launched either automatically or through a ticketing system. In order to respond to a practical number of alarms in real-time, automation is a must. This problem is more so in virtualized infrastructure since the number of alarm generating entities in such networks is significantly increased because their monitoring has to be performed for both physical as well as virtual functions. In this paper, we propose {MAYOR}: a suite of machine learning and analytics algorithms for automated operations and recovery. {MAYOR} is made up of a model generation entity which uses long term historic data to determine alarm persistence times, clusters, and patterns. To this, we model alarm persistence time as a normal distribution, and use the resulting cumulative distribution function to determine the time with an appropriate confidence. Moreover, we use sequential pattern mining and linear correlation to create alarm clusters. Finally, decision trees are used to create patterns between alarms as association rules. In addition, the system also has an adaptation entity that uses realtime alarms to perform short term adaptations. {MAYOR} has been implemented and evaluated using real telecommunications network alarm data as well as {NOC} settings. Evaluations show that the proposed persistence times can reduce 20\% of static ones by atleast 80\%, and that at least 23\% of alarms can be predicted 1 hour before they appear with an accuracy of at least 80\%.},
	pages = {1--9},
	booktitle = {2019 28th International Conference on Computer Communication and Networks ({ICCCN})},
	author = {Mijumbi, Rashid and Asthana, Abhaya and Bernal, Carlos and Castejon, Manuel},
	date = {2019-07},
	note = {{ISSN}: 2637-9430},
	keywords = {Clustering algorithms, Data mining, Monitoring, Circuit faults, Databases, Correlation, Proposals, xno}
}

@inproceedings{pacher_using_2014,
	title = {Using a Machine Learning Algorithm to Control an Artificial Hormone System},
	doi = {10.1109/ISORC.2014.25},
	abstract = {The Artificial Hormone System ({AHS}) is a decentralized software which can be used to allocate tasks in a system of heterogeneous processing elements ({PEs}). Tasks are allocated according to their suitability for the heterogeneous {PEs}, the current {PE} load and task relationships. The {AHS} also provides properties like self-configuration, self-optimization and self-healing in the context of task allocation. In addition, it is able to guarantee real-time bounds for such self-X-properties. Our contribution in this paper is a machine learning approach for gradually learning the hormone values of different tasks. This is a major advance because expert knowledge is needed to configure the {AHS} up to now. We present an Observer-/Controller architecture monitoring and controlling the behaviour of the {AHS}. The user has to provide a simple set of initial rules and the Observer-/Controller is able to generate new rules if needed. The evaluation of our approach is very promising and we show and discuss our evaluation.},
	pages = {317--325},
	booktitle = {2014 {IEEE} 17th International Symposium on Object/Component/Service-Oriented Real-Time Distributed Computing},
	author = {Pacher, Mathias},
	date = {2014-06},
	note = {{ISSN}: 2375-5261},
	keywords = {Computer architecture, Monitoring, Silicon, Real-time systems, Artificial Hormone System, Biochemistry, Learning Classifier Systems, learning of hormone parameters, Observer-/Controller architecture, Observers, Radiation detectors, xno}
}

@inproceedings{rojas_early_2014,
	title = {Early failure characterization of cantilever snap assemblies using the {PA}-{RCBHT}},
	doi = {10.1109/ICRA.2014.6907344},
	abstract = {Failure detection and correction is essential in robust systems. In robotics, failure detection has focused on traditional parts assembly, tool breakage, and threaded fastener assembly. However, not much work has focused on sub-mode failure classification. This is an important step in order to provide accurate failure recovery. Our work implemented a novel failure characterization scheme for cantilever snap assemblies. The approach identified exemplars that characterized salient features for specific deviations from a nominal trajectory. Then, a rule based approach with statistical measures was used to identify failure and classify failure sub-modes. Failure sub-mode classification was evaluated by using a reliability measure. Our work classified failure deviations with 88\% accuracy. Varying success was experienced in correlating failure deviation modes. Cases with only 1-deviation had 86\% accuracy, cases with 2-deviations had 67\% accuracy, and cases with 3 deviations had 55\% accuracy. Our work is an important step in failure characterization of complex geometrical parts and serves as a stepping stone to enact failure recovery.},
	pages = {3370--3377},
	booktitle = {2014 {IEEE} International Conference on Robotics and Automation ({ICRA})},
	author = {Rojas, Juan and Harada, Kensuke and Onda, Hiromu and Yamanobe, Natsuki and Yoshida, Eiichi and Nagata, Kazuyuki},
	date = {2014-05},
	note = {{ISSN}: 1050-4729},
	keywords = {Testing, Training, Accuracy, Assembly, Force, Robots, Trajectory, xno}
}

@inproceedings{bose_discovering_2013,
	title = {Discovering signature patterns from event logs},
	doi = {10.1109/CIDM.2013.6597225},
	abstract = {More and more information about processes is recorded in the form of so-called “event logs”. High-tech systems such as X-ray machines and high-end copiers provide their manufacturers and services organizations with detailed event data. Larger organizations record relevant business events for process improvement, auditing, and fraud detection. Traces in such event logs can be classified as desirable or undesirable (e.g., faulty or fraudulent behavior). In this paper, we present a comprehensive framework for discovering signatures that can be used to explain or predict the class of seen or unseen traces. These signatures are characteristic patterns that can be used to discriminate between desirable and undesirable behavior. As shown, these patterns can, for example, be used to predict remotely whether a particular component in an X-ray machine is broken or not. Moreover, the signatures also help to improve systems and organizational processes. Our framework for signature discovery is fully implemented in {ProM} and supports class labeling, feature extraction and selection, pattern discovery, pattern evaluation and cross-validation, reporting, and visualization. A real-life case study is used to demonstrate the applicability and scalability of the approach.},
	pages = {111--118},
	booktitle = {2013 {IEEE} Symposium on Computational Intelligence and Data Mining ({CIDM})},
	author = {Bose, R.P. Jagadeesh Chandra and van der Aalst, Wil M.P.},
	date = {2013-04},
	keywords = {Feature extraction, Association rules, Support vector machines, Measurement, Computer aided software engineering, Discriminatory Patterns, Event Log, Labeling, Process Mining, Signature Patterns, xno}
}

@inproceedings{zhou_control_2016,
	title = {Control of autonomic parallelism adaptation on software transactional memory},
	doi = {10.1109/HPCSim.2016.7568333},
	abstract = {Parallel programs need to manage the trade-off between the time spent in synchronization and computation. A high parallelism may decrease computing time while increase synchronization cost among threads. A way to improve program performance is to adjust parallelism to balance conflicts among threads. However, there is no universal rule to decide the best parallelism for a program from an offline view. Furthermore, an offline tuning is error-prone. Hence, it becomes necessary to adopt a dynamic tuning-configuration strategy to better manage a {STM} system. Software Transactional Memory ({STM}) has emerged as a promising technique, which bypasses locks, to address synchronization issues through transactions. Autonomic computing offers designers a framework of methods and techniques to build automated systems with well-mastered behaviours. Its key idea is to implement feedback control loops to design safe, efficient and predictable controllers, which enable monitoring and adjusting controlled systems dynamically while keeping overhead low. We propose to design feedback control loops to automate the choice of parallelism level at runtime to diminish program execution time.},
	pages = {180--187},
	booktitle = {2016 International Conference on High Performance Computing Simulation ({HPCS})},
	author = {Zhou, Naweiluo and Delaval, Gwenaël and Robu, Bogdan and Rutten, Éric and Méhaut, Jean-François},
	date = {2016-07},
	keywords = {Throughput, Adaptation models, autonomic, feedback control, Feedback control, Instruction sets, Parallel processing, parallelism adaptation, synchronization, Synchronization, transactional memory, xno}
}

@inproceedings{zhao_flowwatcher_2018,
	title = {{FlowWatcher}: Adaptive Flow Counting for Source Routing Over Protocol Independent {SDN} Networks},
	doi = {10.1109/ICEIEC.2018.8473501},
	abstract = {With the rapid development and increasing scale of Data Center Network ({DCN}), traditional routing mechanisms cannot adapt to the growing network requirements. Source Routing ({SR}) is a feasible simple routing method and has been explored in {DCN} recently to build an efficient stateless forwarding plane. Protocol-Oblivious Forwarding ({POF}) is a breakthrough technology that supports arbitrary protocols and enables the forwarding elements to evolve independently from specific network protocols. In this paper, we propose {POF}-based source routing mechanism that eliminates existing application defects in source routing and it has been proved to work well in {DCN}. We develop {FlowWatcher}, which not only monitors elephant flows with end-host-based detection method, but also measures other abnormally changing flows dynamically, using weighted moving average ({WMA}) to predict future values and identify anomalies. To make an overall balance on each switch, we propose two types of rule placement algorithms separately for unicast and multicast. Finally, we take some experiments to demonstrate the effectiveness of {FlowWatcher} and resource saving of the rule placement algorithm.},
	pages = {237--242},
	booktitle = {2018 8th International Conference on Electronics Information and Emergency Communication ({ICEIEC})},
	author = {Zhao, Min and Li, Mingzheng and Mei, Lei and Tian, Ye},
	date = {2018-06},
	note = {{ISSN}: 2377-844X},
	keywords = {Optical fibers, Monitoring, Computer networks, Probes, Protocols, Routing, Switches, xno}
}

@inproceedings{khlif_graph_2014,
	title = {A Graph Transformation-Based Approach for the Validation of Checkpointing Algorithms in Distributed Systems},
	doi = {10.1109/WETICE.2014.23},
	abstract = {Autonomic Computing Systems are oriented to prevent the human intervention and to enable distributed systems to manage themselves. One of their challenges is the efficient monitoring at runtime oriented to collect information from which the system can automatically repair itself in case of failure. Quasi-Synchronous Check pointing is a well-known technique, which allows processes to recover in spite of failures. Based on this technique, several check pointing algorithms have been developed. According to the checkpoint properties detected and ensured, they are classified into: Strictly Z-Path Free ({SZPF}), Z-Path Free ({ZPF}) and Z-Cycle Free ({ZCF}). In the literature, the simulation has been the method adopted for the performance evaluation of check pointing algorithms. However, few works have been designed to validate their correctness. In this paper, we propose a validation approach based on graph transformation oriented to automatically detect the previous mentioned check pointing properties. To achieve this, we take the vector clocks resulting from the algorithm execution, and we model it into a causal graph. Then, we design and use transformation rules oriented to verify if in such a causal graph, the algorithm is exempt from non desirable patterns, such as Z-paths or Z-cycles, according to the case.},
	pages = {80--85},
	booktitle = {2014 {IEEE} 23rd International {WETICE} Conference},
	author = {Khlif, Houda and Kacem, Hatem Hadj and Pomares Hernandez, Saul E. and Eichler, Cedric and Kacem, Ahmed Hadj and Simon, Alberto Calixto},
	date = {2014-06},
	note = {{ISSN}: 1524-4547},
	keywords = {Monitoring, Algorithm design and analysis, Educational institutions, Computational modeling, Runtime, Autonomic Computing, Checkpointing, checkpointing algorithms, Distributed Systems, Graph Transformation, Happened Before Relation, Laboratories, Z-cycles, Z-paths, xno}
}

@inproceedings{basuki_online_2018,
	title = {Online Dissolved Gas Analysis of Power Transformers Based on Decision Tree Model},
	doi = {10.1109/ICPERE.2018.8739761},
	abstract = {This paper presents the possibility of using one of machine learning model, decision tree with C4.5 algorithm for gas interpretation in online condition monitoring and diagnostic application of power transformers. Decision tree selection is based on the best learning outcomes of machine learning software ({WEKA} and Orange) compared to naïve Bayes, neural network, nearest neighbour and support vector machine models. The decision tree was built from 715 data, 7 attributes of gas and 9 types of fault which were cleaned by interquartile range method become 471 data. Evaluation result based on correction prediction are 95.54\% using data training, 88.32\% using cross validation and 87.23\% using 10\% random data from data training. The decision tree rule was implemented in online condition monitoring and diagnostic of power transformer which is integrated into {SCADA} system. This implementation result can predict transformers fault from gas values by online better than conventional {DGA} methods.},
	pages = {1--6},
	booktitle = {2018 Conference on Power Engineering and Renewable Energy ({ICPERE})},
	author = {Basuki, Arief and {Suwarno}},
	date = {2018-10},
	keywords = {Decision trees, Machine learning, Condition monitoring, Machine Learning, Machine learning algorithms, Discharges (electric), C4.5 Algorithm, Decision Tree, Dissolved Gas Analysis, Oil insulation, Power transformers, {SCADA}, xno}
}

@inproceedings{jafri_outlier_2018,
	title = {Outlier Detection in {WSN}},
	doi = {10.1109/ICICT43934.2018.9034286},
	abstract = {In Wireless Sensor Network ({WSN}), anomaly detection is a consequential challenge for tasks like fault detection, intrusion detection and supervising applications. The sensor nodes with constrained resources execute freely for collaborating and managing the network of wireless via which the amassment and transferring of raw data is considered towards the decision makers or the terminus users. This network could be utilized in challenging applications like home automation, health monitoring system, fire detection system and enemy target monitoring and so on in which there is a dependency on {WSN}. These types of applications have precise and reliable data. {WSN} could be incognizant to the anomalies that occur due to less costly hardware and software and non-operative Operating system ({OS}) that may affect the communication of network. The hybrid algorithms have been developed for anomaly detection that considers the intrinsic limits of wireless sensor networks in their development so that the energy consumption for the wireless sensor nodes is minimized and the throughput of the {WSN} is maximized during the simulation. Hamamoto et.al has also utilized Genetic Algorithm for signature generation and fuzzy logic to generate rule sets for the anomaly detection and the problems are identified in Hamamoto et al. research work. Generation of signature in each iteration may consume a lot of time. For large networks, the number of rules will be very high. The problem of this work is the removal of the problems occurred in Hamamoto using hybrid algorithm predicated on Genetic Algorithm and Artificial Neural Network. According to the fitness function, we will optimize the property of each node those are involved in the simulation of {WSN}. So, we design a novel fitness function for Genetic Algorithm ({GA}) and using Artificial Neural Network ({ANN}) as a classifier to detect the anomaly nodes. The simulation would be executed in {MATLAB} and for the authentication of the work; various performance parameters like Accuracy, Error Rate, True positive rate and False positive rate, Throughput and Energy Consumption will be calculated.},
	pages = {236--241},
	booktitle = {2018 3rd International Conference on Inventive Computation Technologies ({ICICT})},
	author = {Jafri, Rana and Kumar, Rakesh},
	date = {2018-11},
	keywords = {Genetic algorithms, Iterative methods, {MATLAB}, Hardware and software, Fault detection, Support vector machines, False positive rates, Health, Fuzzy logic, Neural networks, Anomaly detection, Constrained resources, Decision making, Energy utilization, Fire detection systems, Fire detectors, Gallium, Health monitoring system, Intrusion detection, Monitoring, Performance parameters, Sensor nodes, Signature generation, Wireless sensor node, Wireless sensor networks, {ANN}, Anomaly Detection, Artificial neural networks, {GA}, Neurons, Telecommunication traffic, {WSN}, xno}
}

@article{yu_self-propagation_2021,
	title = {Self-propagation Graph Neural Network for Recommendation},
	issn = {1558-2191},
	doi = {10.1109/TKDE.2021.3076772},
	abstract = {In recommendation tasks, we model user preferences by learning node preference representations (i.e., user and item embeddings) based on an observed user-item bipartite graph. {GNN} is widely used to refine the representations by exploring the topology of the graph: embeddings of connected nodes are propagated to the current node to assist the construction of its embeddings. However, the propagation strategy in {GNN} is empirical and defective: 1) A substantial proportion of links are missed in the sparse observed graph, which causes inefficient and biased propagation in {GNN}; and 2) The propagation weights are determined by a coarse pre-defined rule, which only takes the degree of nodes into consideration. In this paper, we propose a dense and data-driven propagation mechanism for {GNN}. Considering the graph we use to propagate embeddings in recommendation tasks is extremely sparse, we complement it and use the predicted graph as the new propagation tool. We learn the propagation matrix from the observed data, and propose a Self-propagation {GNN} ({SGNN}). In {SGNN}, the embeddings can be propagated to potential yet unobserved neighbors, and the propagation weights are learned based on the connection strength. Comprehensive experiments on two real-world datasets show that {SGNN} outperforms recent state-of-the-art {GNNs} significantly.},
	pages = {1--1},
	journaltitle = {{IEEE} Transactions on Knowledge and Data Engineering},
	author = {Yu, Wenhui and Lin, Xiao and Liu, Jinfei and Ge, Junfeng and Ou, Wenwu and Qin, Zheng},
	date = {2021},
	keywords = {Feature extraction, Tools, Bipartite graph, Collaboration, collaborative filtering, Graph neural network, Graph neural networks, item recommendation, Recommender systems, self propagation, Task analysis, xno}
}

@inproceedings{han_distributed_2019,
	title = {A distributed autonomic logistics system with parallel-computing diagnostic algorithm for aircrafts},
	doi = {10.1109/AUTEST.2019.8878477},
	abstract = {The autonomic logistic system ({ALS}), first used by the U.S. military {JSF}, is a new conceptional system which supports prognostic and health management system of aircrafts, including such as real-time failure monitoring, remaining useful life prediction and maintenance decisions-making. However, the development of {ALS} faces some challenges. Firstly, current {ALS} is mainly based on client/server architecture, which is very complex in a large-scale aircraft control center and software is required to be reconfigured for every accessed node, which will increase the cost and decrease the expandability of deployment for large scale aircraft control centers. Secondly, interpretation of telemetry parameters from the aircraft is a tough task considering various real-time flight conditions, including instructions from controllers, work statements of single machines or machine groups, and intrinsic physical meaning of telemetry parameters. It is troublesome to meet the expectation of full representing the relationship between faults and tests without a standard model. Finally, typical diagnostic algorithms based on dependency matrix are inefficient, especially the temporal waste when dealing with thousands of test points and fault modes, for the reason that the time complexity will increase exponentially as dependency matrix expansion. Under this situation, this paper proposed a distributed {ALS} under complex operating conditions, which has the following contributions 1) Introducing a distributed system based on browser/server architecture, which is divided overall system into primary control system and diagnostic and health assessment platform; 2) Designing a novel interface for modelling the interpretation rules of telemetry parameters and the relationship between faults and tests in consideration of multiple elements of aircraft conditions; 3) Proposing a promoted diagnostic algorithm under parallel computing in order to decrease the computing time complexity. what's more, this paper develops a construction with 3D viewer of aircraft for user to locate fault points and presents repairment instructions for maintenance personnels based on Interactive Electronic Technical Manual, which supports both online and offline. A practice in a certain aircraft demonstrated the efficiency of improved diagnostic algorithm and proposed {ALS}.},
	pages = {1--8},
	booktitle = {2019 {IEEE} {AUTOTESTCON}},
	author = {Han, Danyang and Yu, Jinsong and Song, Yue and Tang, Diyin and Dai, Jing},
	date = {2019-08},
	note = {{ISSN}: 1558-4550},
	keywords = {Aircraft, Aircraft control, Autonomic logistics, Browser/server architectures, Client server computer systems, Client/server architecture, Failure analysis, Flight control systems, Health, Maintenance decisions, Maintenance personnel, Matrix algebra, Prognostic and health management, Prognostics and health managements, Remaining useful life predictions, Telemetering equipment, fault diagnosis, autonomic logistic, prognostics and health management, xno},
	file = {Full Text:C\:\\Users\\michalm\\Zotero\\storage\\6N8QRTJB\\Han et al. - 2019 - A distributed autonomic logistics system with para.pdf:application/pdf}
}