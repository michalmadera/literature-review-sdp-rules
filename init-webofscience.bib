
@article{bhushan_classifying_2021,
	title = {Classifying and resolving software product line redundancies using an ontological first-order logic rule based method},
	volume = {168},
	issn = {09574174},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099231625&doi=10.1016%2fj.eswa.2020.114167&partnerID=40&md5=d3d7a804f51c4eeecd20365e7e7d4ad4},
	doi = {10.1016/j.eswa.2020.114167},
	abstract = {Software product line engineering improves software quality and diminishes development cost and time by efficiently developing software products. Its success lies in identifying the commonalities and variabilities of a set of software products which are generally modeled using feature models. The success of software product lines heavily relies upon the quality of feature models to derive high quality products. However, there are various defects that reduce profits of software product line. One of such defect is redundancy. While the majority of research work focuses on the identification of redundancies, their causes and corrections have been poorly explored. Causes and corrections must be as accurate and comprehensible as possible in order to support the developer in resolving the cause of a redundancy. This research work classified redundancies in the form of a typology. An ontological first-order logic rule based method is proposed to deal with redundancies. A two-step process is presented for mapping model to ontology based on predicate logic. First-order logic based rules are developed and applied to the generated ontology for identifying redundancies, their causes and corrections to resolve redundancies. The proposed method is illustrated using a case study from software product lines online tools repository. The results of experiments performed on 35 models with varied sizes of real world models as well as automatically-generated models from the Software Product Line Online Tools repository and models created via {FeatureIDE} tool conclude that the method is accurate, efficient and scalable with {FM} up to 30,000 features. Thus, enables deriving redundancy free end products from the product line and ultimately, improves its quality. © 2020 Elsevier Ltd},
	journaltitle = {Expert Systems with Applications},
	author = {Bhushan, M. and Ángel Galindo Duarte, J. and Samant, P. and Kumar, A. and Negi, A.},
	date = {2021},
	note = {Publisher: Elsevier Ltd},
	keywords = {Automatically generated, Computer circuits, Computer software selection and evaluation, Cost engineering, Defects, Development costs, Engineering research, First order logic, Formal logic, High-quality products, Ontology, Redundancy, Software design, Software Product Line, Software product line engineerings, Software products, Software quality, Two-step process, xyes}
}

@article{malhotra_predicting_2021,
	title = {Predicting Software Defects for Object-Oriented Software Using Search-based Techniques},
	volume = {31},
	issn = {02181940},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101961196&doi=10.1142%2fS0218194021500054&partnerID=40&md5=81449e0a65f756bf7d3cb03fc70e1e95},
	doi = {10.1142/S0218194021500054},
	abstract = {Development without any defect is unsubstantial. Timely detection of software defects favors the proper resource utilization saving time, effort and money. With the increasing size and complexity of software, demand for accurate and efficient prediction models is increasing. Recently, search-based techniques ({SBTs}) have fascinated many researchers for Software Defect Prediction ({SDP}). The goal of this study is to conduct an empirical evaluation to assess the applicability of {SBTs} for predicting software defects in object-oriented ({OO}) softwares. In this study, 16 {SBTs} are exploited to build defect prediction models for 13 {OO} software projects. Stable performance measures-{GMean}, Balance and Receiver Operating Characteristic-Area Under Curve ({ROC}-{AUC}) are employed to probe into the predictive capability of developed models, taking into consideration the imbalanced nature of software datasets. Proper measures are taken to handle the stochastic behavior of {SBTs}. The significance of results is statistically validated using the Friedman test complied with Wilcoxon post hoc analysis. The results confirm that software defects can be detected in the early phases of software development with help of {SBTs}. This paper identifies the effective subset of {SBTs} that will aid software practitioners to timely detect the probable software defects, therefore, saving resources and bringing up good quality softwares. Eight {SBTs}-{sUpervised} Classification System ({UCS}), Bioinformatics-oriented hierarchical evolutionary learning ({BIOHEL}), {CHC}, Genetic Algorithm-based Classifier System with Adaptive Discretization Intervals ({GA}-{ADI}), Genetic Algorithm-based Classifier System with Intervalar Rule ({GA}-{INT}), Memetic Pittsburgh Learning Classifier System ({MPLCS}), Population-Based Incremental Learning ({PBIL}) and Steady-State Genetic Algorithm for Instance Selection ({SGA}) are found to be statistically good defect predictors. © 2021 World Scientific Publishing Company.},
	pages = {193--215},
	number = {2},
	journaltitle = {International Journal of Software Engineering and Knowledge Engineering},
	author = {Malhotra, R. and Jain, J.},
	date = {2021},
	note = {Publisher: World Scientific},
	keywords = {Defects, Software design, Software defect prediction, Adaptive systems, Computer aided diagnosis, Defect prediction models, Forecasting, Genetic algorithms, Learning classifier system, Learning systems, Object oriented programming, Object oriented software, Population based incremental learning, Predictive analytics, Receiver operating characteristics, Steady-state genetic algorithms, Stochastic systems, Supervised classification, xno}
}

@article{mahmood_mining_2021,
	title = {Mining software repository for cleaning bugs using data mining technique},
	volume = {69},
	issn = {15462218},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107792741&doi=10.32604%2fcmc.2021.016614&partnerID=40&md5=5f9bc74622b43439b6c231bbc7001e10},
	doi = {10.32604/cmc.2021.016614},
	abstract = {Despite advances in technological complexity and efforts, software repository maintenance requires reusing the data to reduce the effort and complexity. However, increasing ambiguity, irrelevance, and bugs while extracting similar data during software development generate a large amount of data from those data that reside in repositories. Thus, there is a need for a repository mining technique for relevant and bug-free data prediction. This paper proposes a fault prediction approach using a data-mining technique to find good predictors for high-quality software. To predict errors in mining data, the Apriori algorithm was used to discover association rules by fixing confidence at more than 40\% and support at least 30\%. The pruning strategy was adopted based on evaluation measures. Next, the rules were extracted from three projects of different domains; the extracted rules were then combined to obtain the most popular rules based on the evaluation measure values. To evaluate the proposed approach, we conducted an experimental study to compare the proposed rules with existing ones using four different industrial projects. The evaluation showed that the results of our proposal are promising. Practitioners and developers can utilize these rules for defect prediction during early software development. © 2021 Tech Science Press. All rights reserved.},
	pages = {873--893},
	number = {1},
	journaltitle = {Computers, Materials and Continua},
	author = {Mahmood, N. and Hafeez, Y. and Iqbal, K. and Hussain, S. and Aqib, M. and Jamal, M. and Song, O.-Y.},
	date = {2021},
	note = {Publisher: Tech Science Press},
	keywords = {Data mining, Software design, Forecasting, Apriori algorithms, Different domains, Evaluation measures, High-quality software, Industrial projects, Mining software repositories, Program debugging, Software repositories, Technological complexity, xyes}
}

@article{wu_limcr_2020,
	title = {{LIMCR}: Less-informative majorities cleaning rule based on naïve bayes for imbalance learning in software defect prediction},
	volume = {10},
	issn = {20763417},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096559362&doi=10.3390%2fapp10238324&partnerID=40&md5=0b46eefed4e930bc94d9faf859b5c9c4},
	doi = {10.3390/app10238324},
	abstract = {Software defect prediction ({SDP}) is an effective technique to lower software module testing costs. However, the imbalanced distribution almost exists in all {SDP} datasets and restricts the accuracy of defect prediction. In order to balance the data distribution reasonably, we propose a novel resampling method {LIMCR} on the basis of Naïve Bayes to optimize and improve the {SDP} performance. The main idea of {LIMCR} is to remove less-informative majorities for rebalancing the data distribution after evaluating the degree of being informative for every sample from the majority class. We employ 29 {SDP} datasets from the {PROMISE} and {NASA} dataset and divide them into two parts, the small sample size (the amount of data is smaller than 1100) and the large sample size (larger than 1100). Then we conduct experiments by comparing the matching of classifiers and imbalance learning methods on small datasets and large datasets, respectively. The results show the effectiveness of {LIMCR}, and {LIMCR}+{GNB} performs better than other methods on small datasets while not brilliant on large datasets. © 2020 by the authors. Licensee {MDPI}, Basel, Switzerland.},
	pages = {1--24},
	number = {23},
	journaltitle = {Applied Sciences (Switzerland)},
	author = {Wu, Y. and Yao, J. and Chang, S. and Liu, B.},
	date = {2020},
	note = {Publisher: {MDPI} {AG}},
	keywords = {xno},
	file = {Full Text:C\:\\Users\\michalm\\Zotero\\storage\\4Q3KYT3P\\Wu et al. - 2020 - LIMCR Less-Informative Majorities Cleaning Rule B.pdf:application/pdf;Full Text:C\:\\Users\\michalm\\Zotero\\storage\\CBEM3TC5\\Wu et al. - 2020 - LIMCR Less-Informative Majorities Cleaning Rule B.pdf:application/pdf}
}

@article{zhou_multi-agent_2020,
	title = {A Multi-Agent Simulation Method of Urban Land Layout Structure Based on {FPGA}},
	volume = {25},
	issn = {1383469X},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072178973&doi=10.1007%2fs11036-019-01361-0&partnerID=40&md5=d25d050e3bacfd8c023f7d0370775f20},
	doi = {10.1007/s11036-019-01361-0},
	abstract = {The unavoidable birth defects for current simulation method make the final simulation results cannot truly reflect the evolution rules of urban land layout. In this way, a multi-agent simulation method based on {FPGA} for urban land layout is proposed in this paper. The evolution rule of urban eco-land is explored by combination of cellular automata, dynamic reconfiguration and multi-agent methods. Relying on platform of {MATLAB} software and dynamic reconfiguration of {FPGA} logic resources, a regional {ANN}-{CA}-Agent model for evolution and prediction model of urban land layout is established. {MATLAB}, {FPGA}, and {ArcGIS} are interoperable by programming, and foreground is displayed by the repast tool. The input layer contains 18 data layers, and the output layer contains 6 data layers. A multi-agent model is established for studying the evolution of urban land layout structures. Finally, in order to make calculation results of the model more in line with actual situation and reflect uncertainty of urban system, random factors are added. Example analysis results show that simulation accuracy of the proposed land layout structure reaches 92.4\%, which is a high simulation accuracy. Moreover, conclusion shows that speed of urban expansion in the study area from 2007 to 2029 has gradually slowed down, and urban land-use pattern has changed from epitaxial expansion to intensive land-use. © 2019, Springer Science+Business Media, {LLC}, part of Springer Nature.},
	pages = {1572--1581},
	number = {4},
	journaltitle = {Mobile Networks and Applications},
	author = {Zhou, X. and Fu, W.},
	date = {2020},
	note = {Publisher: Springer},
	keywords = {Calculation results, Dynamic models, Dynamic re-configuration, Field programmable gate arrays ({FPGA}), Integrated circuit layout, Intensive land use, Land use, Layout structure, {MATLAB}, Multi agent, Multi agent simulation, Multi agent systems, Simulation, Simulation accuracy, Software agents, xno}
}

@article{abaei_fuzzy_2020,
	title = {A fuzzy logic expert system to predict module fault proneness using unlabeled data},
	volume = {32},
	issn = {13191578},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053032476&doi=10.1016%2fj.jksuci.2018.08.003&partnerID=40&md5=dc1ca5a715bad9a6287aa6e72f63f14d},
	doi = {10.1016/j.jksuci.2018.08.003},
	abstract = {Several techniques have been proposed to predict the fault proneness of software modules in the absence of fault data. However, the application of these techniques requires an expert assistant and is based on fixed thresholds and rules, which potentially prevents obtaining optimal prediction results. In this study, the development of a fuzzy logic expert system for predicting the fault proneness of software modules is demonstrated in the absence of fault data. The problem of strong dependability with the prediction model for expert assistance as well as deciding on the module fault proneness based on fixed thresholds and fixed rules have been solved in this study. In fact, involvement of experts is more relaxed or provides more support now. Two methods have been proposed and implemented using the fuzzy logic system. In the first method, the Takagi and Sugeno-based fuzzy logic system is developed manually. In the second method, the rule-base and data-base of the fuzzy logic system are adjusted using a genetic algorithm. The second method can determine the optimal values of the thresholds while recommending the most appropriate rules to guide the testing of activities by prioritizing the module's defects to improve the quality of software testing with a limited budget and limited time. Two datasets from {NASA} and the Turkish white-goods manufacturer that develops embedded controller software are used for evaluation. The results based on the second method show improvement in the false negative rate, f-measure, and overall error rate. To obtain optimal prediction results, developers and practitioners are recommended to apply the proposed fuzzy logic expert system for predicting the fault proneness of software modules in the absence of fault data. © 2018 The Authors},
	pages = {684--699},
	number = {6},
	journaltitle = {Journal of King Saud University - Computer and Information Sciences},
	author = {Abaei, G. and Selamat, A. and Al Dallal, J.},
	date = {2020},
	note = {Publisher: King Saud bin Abdulaziz University},
	keywords = {xyes}
}

@article{shao_software_2020,
	title = {Software defect prediction based on correlation weighted class association rule mining},
	volume = {196},
	issn = {09507051},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082463500&doi=10.1016%2fj.knosys.2020.105742&partnerID=40&md5=aafe1f60cfd93765b177edb265fe468d},
	doi = {10.1016/j.knosys.2020.105742},
	abstract = {Software defect prediction based on supervised learning plays a crucial role in guiding software testing for resource allocation. In particular, it is worth noticing that using associative classification with high accuracy and comprehensibility can predict defects. But owing to the imbalance data distribution inherent, it is easy to generate a large number of non-defective class association rules, but the defective class association rules are easily ignored. Furthermore, classical associative classification algorithms mainly measure the interestingness of rules by the occurrence frequency, such as support and confidence, without considering the importance of features, resulting in combinations of the insignificant frequent itemset. This promotes the generation of weighted associative classification. However, the feature weighting based on domain knowledge is subjective and unsuitable for a high dimensional dataset. Hence, we present a novel software defect prediction model based on correlation weighted class association rule mining ({CWCAR}). It leverages a multi-weighted supports-based framework rather than the traditional support-confidence approach to handle class imbalance and utilizes the correlation-based heuristic approach to assign feature weight. Besides, we also optimize the ranking, pruning and prediction stages based on weighted support. Results show that {CWCAR} is significantly superior to state-of-the-art classifiers in terms of Balance, {MCC}, and Gmean. © 2020 Elsevier B.V.},
	journaltitle = {Knowledge-Based Systems},
	author = {Shao, Y. and Liu, B. and Wang, S. and Li, G.},
	date = {2020},
	note = {Publisher: Elsevier B.V.},
	keywords = {Data mining, Defects, Software defect prediction, Forecasting, Apriori, Association rules, Associative classification, Attribute weighting, Class imbalance, Heuristic methods, Software testing, Association rule, xyes}
}

@article{chen_modeling_2020,
	title = {Modeling and reasoning of {IoT} architecture in semantic ontology dimension},
	volume = {153},
	issn = {01403664},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079849963&doi=10.1016%2fj.comcom.2020.02.006&partnerID=40&md5=691e8881ffcbb75bad3e5a62acdfd69a},
	doi = {10.1016/j.comcom.2020.02.006},
	abstract = {The architecture for {IoT} is the primary foundation for designing and implementing the System of Internet of things. This paper discusses the theory, method, tools and practice of modeling and reasoning the architecture of the Internet of Things system from the dimension of semantic ontology. This paper breaks the way of static ontology modeling, and proposes an implementation framework for real-time and dynamic ontology modeling of the {IoT} system from the running system. According to the actual needs of the health cabin {IoT} system and the combination of theory and practice, the system architecture model of the semantic ontology dimension of {IoT} is built. Then, based on the reasoning rules of the ontology model, the model is reasoned by Pellet reasoning engine which injects the atom of the custom reasoning built-ins into the source code. In this way we have realized the automatic classification and attribute improvement of resources and behaviors of the {IoT} system, the real-time working state detection and fault diagnosis of the {IoT} system, and the automatic control of the {IoT} system and resources. © 2020 The Authors},
	pages = {580--594},
	journaltitle = {Computer Communications},
	author = {Chen, G. and Jiang, T. and Wang, M. and Tang, X. and Ji, W.},
	date = {2020},
	note = {Publisher: Elsevier B.V.},
	keywords = {Ontology, Computer aided diagnosis, Semantics, Fault detection, Architecture, Automatic classification, Automation, Computer architecture, Dynamic ontologies, Internet of things, Iot architectures, Models, Reasoning, Semantic ontology, System architectures, Theory and practice, Tools and practices, xno}
}

@article{barbez_machine-learning_2020,
	title = {A machine-learning based ensemble method for anti-patterns detection},
	volume = {161},
	issn = {01641212},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076559059&doi=10.1016%2fj.jss.2019.110486&partnerID=40&md5=ec8a0ff172eff58af69f033014cacec9},
	doi = {10.1016/j.jss.2019.110486},
	abstract = {Anti-patterns are poor solutions to recurring design problems. Several empirical studies have highlighted their negative impact on program comprehension, maintainability, as well as fault-proneness. A variety of detection approaches have been proposed to identify their occurrences in source code. However, these approaches can identify only a subset of the occurrences and report large numbers of false positives and misses. Furthermore, a low agreement is generally observed among different approaches. Recent studies have shown the potential of machine-learning models to improve this situation. However, such algorithms require large sets of manually-produced training-data, which often limits their application in practice. In this paper, we present {SMAD} ({SMart} Aggregation of Anti-patterns Detectors), a machine-learning based ensemble method to aggregate various anti-patterns detection approaches on the basis of their internal detection rules. Thus, our method uses several detection tools to produce an improved prediction from a reasonable number of training examples. We implemented {SMAD} for the detection of two well known anti-patterns: God Class and Feature Envy. With the results of our experiments conducted on eight java projects, we show that: (1) Our method clearly improves the so aggregated tools; (2) {SMAD} significantly outperforms other ensemble methods. © 2019},
	journaltitle = {Journal of Systems and Software},
	author = {Barbez, A. and Khomh, F. and Guéhéneuc, Y.-G.},
	date = {2020},
	note = {Publisher: Elsevier Inc.},
	keywords = {Computer software selection and evaluation, Software quality, Empirical studies, Learning systems, Machine learning, Anti-patterns, Detection approach, Ensemble methods, Machine learning models, Pattern recognition, Program comprehension, Software Quality, Training example, xno},
	file = {Submitted Version:C\:\\Users\\michalm\\Zotero\\storage\\8J6XU45L\\Barbez et al. - 2020 - A machine-learning based ensemble method for anti-.pdf:application/pdf}
}

@article{liu_programming_2020,
	title = {Programming logic modeling and cross-program defect detection method for object-oriented code},
	volume = {64},
	issn = {15462218},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090883454&doi=10.32604%2fCMC.2020.09659&partnerID=40&md5=4fa681e6f6c363ecc2deda4e85575a01},
	doi = {10.32604/CMC.2020.09659},
	abstract = {Code defects can lead to software vulnerability and even produce vulnerability risks. Existing research shows that the code detection technology with text analysis can judge whether object-oriented code files are defective to some extent. However, these detection techniques are mainly based on text features and have weak detection capabilities across programs. Compared with the uncertainty of the code and text caused by the developer’s personalization, the programming language has a stricter logical specification, which reflects the rules and requirements of the language itself and the developer’s potential way of thinking. This article replaces text analysis with programming logic modeling, breaks through the limitation of code text analysis solely relying on the probability of sentence/word occurrence in the code, and proposes an object-oriented language programming logic construction method based on method constraint relationships, selecting features through hypothesis testing ideas, and construct support vector machine classifier to detect class files with defects and reduce the impact of personalized programming on detection methods. In the experiment, some representative Android applications were selected to test and compare the proposed methods. In terms of the accuracy of code defect detection, through cross validation, the proposed method and the existing leading methods all reach an average of more than 90\%. In the aspect of cross program detection, the method proposed in this paper is superior to the other two leading methods in accuracy, recall and F1 value. © 2020 Tech Science Press. All rights reserved.},
	pages = {273--295},
	number = {1},
	journaltitle = {Computers, Materials and Continua},
	author = {Liu, Y. and Fang, W. and Wei, Q. and Zhao, Y. and Wang, L.},
	date = {2020},
	note = {Publisher: Tech Science Press},
	keywords = {Feature extraction, Computer circuits, Defects, Object oriented programming, Android applications, Codes (symbols), Construction method, Defect detection method, Detection capability, Logical specifications, Machine oriented languages, Modeling languages, Object detection, Object-oriented code, Software vulnerabilities, Support vector machine classifiers, Support vector machines, Text mining, xno}
}

@article{ochodek_recognizing_2020,
	title = {Recognizing lines of code violating company-specific coding guidelines using machine learning: A Method and Its Evaluation},
	volume = {25},
	issn = {13823256},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075204546&doi=10.1007%2fs10664-019-09769-8&partnerID=40&md5=8cf3824cf1e4c7096ea29752f6a12917},
	doi = {10.1007/s10664-019-09769-8},
	abstract = {Software developers in big and medium-size companies are working with millions of lines of code in their codebases. Assuring the quality of this code has shifted from simple defect management to proactive assurance of internal code quality. Although static code analysis and code reviews have been at the forefront of research and practice in this area, code reviews are still an effort-intensive and interpretation-prone activity. The aim of this research is to support code reviews by automatically recognizing company-specific code guidelines violations in large-scale, industrial source code. In our action research project, we constructed a machine-learning-based tool for code analysis where software developers and architects in big and medium-sized companies can use a few examples of source code lines violating code/design guidelines (up to 700 lines of code) to train decision-tree classifiers to find similar violations in their codebases (up to 3 million lines of code). Our action research project consisted of (i) understanding the challenges of two large software development companies, (ii) applying the machine-learning-based tool to detect violations of Sun’s and Google’s coding conventions in the code of three large open source projects implemented in Java, (iii) evaluating the tool on evolving industrial codebase, and (iv) finding the best learning strategies to reduce the cost of training the classifiers. We were able to achieve the average accuracy of over 99\% and the average F-score of 0.80 for open source projects when using ca. 40K lines for training the tool. We obtained a similar average F-score of 0.78 for the industrial code but this time using only up to 700 lines of code as a training dataset. Finally, we observed the tool performed visibly better for the rules requiring to understand a single line of code or the context of a few lines (often allowing to reach the F-score of 0.90 or higher). Based on these results, we could observe that this approach can provide modern software development companies with the ability to use examples to teach an algorithm to recognize violations of code/design guidelines and thus increase the number of reviews conducted before the product release. This, in turn, leads to the increased quality of the final software. © 2019, The Author(s).},
	pages = {220--265},
	number = {1},
	journaltitle = {Empirical Software Engineering},
	author = {Ochodek, M. and Hebig, R. and Meding, W. and Frost, G. and Staron, M.},
	date = {2020},
	note = {Publisher: Springer},
	keywords = {Software design, Open source software, Learning systems, Decision trees, Industrial research, Open source projects, Machine learning, Open systems, Software developer, Codes (symbols), Action research, Code review, Decision tree classifiers, Industrial sources, Measurement, Medium-sized companies, Static code analysis, xno},
	file = {Full Text:C\:\\Users\\michalm\\Zotero\\storage\\G8YVFBJ4\\Ochodek et al. - 2020 - Recognizing lines of code violating company-specif.pdf:application/pdf;Full Text:C\:\\Users\\michalm\\Zotero\\storage\\YDFJT9HT\\Ochodek et al. - 2020 - Recognizing lines of code violating company-specif.pdf:application/pdf}
}

@article{shukla_formal_2019,
	title = {Formal modeling and verification of software-defined networks: A survey},
	volume = {29},
	issn = {10557148},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071243945&doi=10.1002%2fnem.2082&partnerID=40&md5=309aab14a5e3e12ab6b07b457090f14a},
	doi = {10.1002/nem.2082},
	abstract = {Unlike traditional networking devices, control and management plane are decoupled from data plane in software-defined networks ({SDN}). The logically centralized control and management plane facilitate dynamic orchestration of network resources, services, and policies by writing software programs. This provides much needed flexibility and programmability where networking rules and policies can be modified dynamically depending upon the application context. As the operation of network services entirely depends on a program, a small fault may induce several issues which can adversely affect the expected behavior of the network. Formal modeling and verification help in catching inconsistencies and existence of errors prior to the deployment of the programs that control the behavior of a network. In this paper, we provide a comprehensive survey of tools and techniques available in the literature for formal modeling and verification of {SDN}. These tools and techniques are classified based on their types, the components of {SDN} where they can be applied, and the design and development phase when they are utilized. In particular, their respective benefits and limitations are discussed in terms of ease of use, interfaces, and the ability to capture and verify intended network properties. © 2019 John Wiley \& Sons, Ltd.},
	number = {5},
	journaltitle = {International Journal of Network Management},
	author = {Shukla, N. and Pandey, M. and Srivastava, S.},
	date = {2019},
	note = {Publisher: John Wiley and Sons Ltd},
	keywords = {Surveys, Application contexts, Centralized control, Control and management, Design and Development, Formal modeling and verification, Network properties, Networking devices, Software defined networking, Tools and techniques, Verification, xno}
}

@article{khuat_binary_2019,
	title = {Binary teaching–learning-based optimization algorithm with a new update mechanism for sample subset optimization in software defect prediction},
	volume = {23},
	issn = {14327643},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053884634&doi=10.1007%2fs00500-018-3546-6&partnerID=40&md5=63c72345eda9a46d0b9730ad78e9765a},
	doi = {10.1007/s00500-018-3546-6},
	abstract = {Software defect prediction has gained considerable attention in recent years. A broad range of computational methods has been developed for accurate prediction of faulty modules based on code and design metrics. One of the challenges in training classifiers is the highly imbalanced class distribution in available datasets, leading to an undesirable bias in the prediction performance for the minority class. Data sampling is a widespread technique to tackle this problem. However, traditional sampling methods, which depend mainly on random resampling from a given dataset, do not take advantage of useful information available in training sets, such as sample quality and representative instances. To cope with this limitation, evolutionary undersampling methods are usually used for identifying an optimal sample subset for the training dataset. This paper proposes a binary teaching–learning- based optimization algorithm employing a distribution-based solution update rule, namely {BTLBOd}, to generate a balanced subset of highly valuable examples. This subset is then applied to train a classifier for reliable prediction of potentially defective modules in a software system. Each individual in {BTLBOd} includes two vectors: a real-valued vector generated by the distribution-based update mechanism, and a binary vector produced from the corresponding real vector by a proposed mapping function. Empirical results showed that the optimal sample subset produced by {BTLBOd} might ameliorate the classification accuracy of the predictor on highly imbalanced software defect data. Obtained results also demonstrated the superior performance of the proposed sampling method compared to other popular sampling techniques. © 2018, Springer-Verlag {GmbH} Germany, part of Springer Nature.},
	pages = {9919--9935},
	number = {20},
	journaltitle = {Soft Computing},
	author = {Khuat, T.T. and Le, M.H.},
	date = {2019},
	note = {Publisher: Springer Verlag},
	keywords = {Software defect prediction, Forecasting, Classification (of information), Accurate prediction, Classification accuracy, Distribution-based update, Imbalanced Learning, Learning algorithms, Optimization, Optimization algorithms, Prediction performance, Sampling technique, Set theory, Vectors, xno}
}

@article{chatterjee_fuzzy_2019,
	title = {A fuzzy rule-based generation algorithm in interval type-2 fuzzy logic system for fault prediction in the early phase of software development},
	volume = {31},
	issn = {0952813X},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058179117&doi=10.1080%2f0952813X.2018.1552315&partnerID=40&md5=39c1d24ec8744497e3521e77af9da471},
	doi = {10.1080/0952813X.2018.1552315},
	abstract = {Reliability, a measure of software, deals in total number of faults count up to a certain period of time. The present study aims at estimating the total number of software faults during the early phase of software life cycle. Such estimation helps in producing more reliable software as there may be a scope to take necessary corrective actions for improving the reliability within optimum time and cost by the software developers. The proposed interval type-2 fuzzy logic-based model considers reliability-relevant software metric and earlier project data as model inputs. Type-2 fuzzy sets have been used to reduce uncertainties in the vague linguistic values of the software metrics. A rule formation algorithm has been developed to overcome inconsistency in the consequent parts of large number of rules. Twenty-six software project data help to validate the model, and a comparison has been provided to analyse the proposed model’s performance. © 2018, © 2018 Informa {UK} Limited, trading as Taylor \& Francis Group.},
	pages = {369--391},
	number = {3},
	journaltitle = {Journal of Experimental and Theoretical Artificial Intelligence},
	author = {Chatterjee, S. and Maji, B. and Pham, H.},
	date = {2019},
	note = {Publisher: Taylor and Francis Ltd.},
	keywords = {Computer circuits, Software design, Software developer, Software reliability, Fuzzy inference, Fuzzy rules, Corrective actions, Early fault, Fuzzy logic, Fuzzy rule base, Generation algorithm, Interval type-2 fuzzy logic, Interval type-2 fuzzy logic systems, Life cycle, Software life cycles, xyes}
}

@article{chen_probabilistic_2019,
	title = {Probabilistic timing analysis of time-randomised caches with fault detection mechanisms},
	volume = {13},
	issn = {17518601},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065673504&doi=10.1049%2fiet-cdt.2018.5043&partnerID=40&md5=92250ec7af01af75cf71cdd217f336c4},
	doi = {10.1049/iet-cdt.2018.5043},
	abstract = {In the real-time systems domain, time-randomised caches have been proposed as a way to simplify software timing analysis, i.e. the process of estimating the probabilistic worst case execution time ({pWCET}) of an application. However, the technology scaling of the cache memory manufacturing process is rendering transient and permanent faults more and more likely. These faults, in turn, affect a system's timing behaviour and the complexity of its analysis. In this study, the authors propose a static probabilistic timing analysis approach for time-randomised caches that is able to account for the presence of faults- A nd their detection mechanisms-using a state-space modelling technique. Their experiments show that the proposed methodology is capable of providing tight {pWCET} estimates. In their analysis, the effects on the estimation of safe {pWCET} bounds of two online mechanisms for the detection and classification of faults, i.e. a rule-based system and dynamic hidden Markov models (D-{HMMs}), are compared. The experimental results show that different mechanisms can greatly affect safe {pWCET} margins and that, by using D-{HMMs}, the {pWCET} of the system can be improved with respect to rule-based detection. © 2019 Institution of Engineering and Technology. All rights reserved.},
	pages = {262--272},
	number = {3},
	journaltitle = {{IET} Computers and Digital Techniques},
	author = {Chen, C. and Panerati, J. and Li, M. and Beltrame, G.},
	date = {2019},
	note = {Publisher: Institution of Engineering and Technology},
	keywords = {Fault detection, Application programs, Cache memory, Different mechanisms, Fault-detection mechanisms, Hidden Markov models, Interactive computer systems, Manufacturing process, Real time systems, Rule based detection, Software timing analysis, State-space modelling, Timing circuits, Transient and permanent fault, Worst-case execution time, xno}
}

@article{mori_balancing_2019,
	title = {Balancing the trade-off between accuracy and interpretability in software defect prediction},
	volume = {24},
	issn = {13823256},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050694331&doi=10.1007%2fs10664-018-9638-1&partnerID=40&md5=24406dcf95de7405b06a8ce16df8182c},
	doi = {10.1007/s10664-018-9638-1},
	abstract = {Context: Classification techniques of supervised machine learning have been successfully applied to various domains of practice. When building a predictive model, there are two important criteria: predictive accuracy and interpretability, which generally have a trade-off relationship. In particular, interpretability should be accorded greater emphasis in the domains where the incorporation of expert knowledge into a predictive model is required. Objective: The aim of this research is to propose a new classification model, called superposed naive Bayes ({SNB}), which transforms a naive Bayes ensemble into a simple naive Bayes model by linear approximation. Method: In order to evaluate the predictive accuracy and interpretability of the proposed method, we conducted a comparative study using well-known classification techniques such as rule-based learners, decision trees, regression models, support vector machines, neural networks, Bayesian learners, and ensemble learners, over 13 real-world public datasets. Results: A trade-off analysis between the accuracy and interpretability of different classification techniques was performed with a scatter plot comparing relative ranks of accuracy with those of interpretability. The experiment results show that the proposed method ({SNB}) can produce a balanced output that satisfies both accuracy and interpretability criteria. Conclusions: {SNB} offers a comprehensible predictive model based on a simple and transparent model structure, which can provide an effective way for balancing the trade-off between accuracy and interpretability. © 2018, Springer Science+Business Media, {LLC}, part of Springer Nature.},
	pages = {779--825},
	number = {2},
	journaltitle = {Empirical Software Engineering},
	author = {Mori, T. and Uchihira, N.},
	date = {2019},
	note = {Publisher: Springer New York {LLC}},
	keywords = {Defects, Software defect prediction, Classification (of information), Decision trees, Economic and social effects, Bayesian networks, Classifiers, Ensemble learning, Interpretability, Mathematical transformations, Model approximations, Naive Bayes classifiers, Predictive accuracy, Regression analysis, Supervised learning, Trade-off analysis, Weights of evidences, xyes},
	file = {Mori and Uchihira - 2019 - Balancing the trade-off between accuracy and inter.pdf:C\:\\Users\\michalm\\Zotero\\storage\\MJIVB7Y2\\Mori and Uchihira - 2019 - Balancing the trade-off between accuracy and inter.pdf:application/pdf}
}

@article{juneja_fuzzy-filtered_2019,
	title = {A fuzzy-filtered neuro-fuzzy framework for software fault prediction for inter-version and inter-project evaluation},
	volume = {77},
	issn = {15684946},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061619023&doi=10.1016%2fj.asoc.2019.02.008&partnerID=40&md5=95a51edee31eacae45d0bea5ffbf7d64},
	doi = {10.1016/j.asoc.2019.02.008},
	abstract = {Fault Prediction is the most required measure to estimate the software quality and reliability. Several methods, measures, aspects and testing methodologies are available to evaluate the software fault. In this paper, a fuzzy-filtered neuro-fuzzy framework is introduced to predict the software faults for internal and external software projects. The suggested framework is split into three primary phases. At the earlier phase, the effective metrics or measures are identified, which can derive the accurate decision on prediction of software fault. In this phase, the composite analytical observation of each software attribute is calculated using Information Gain and Gain Ratio measures. In the second phase, these fuzzy rules are applied on these measures for selection of effective and high-impact features. In the last phase, the Neuro-fuzzy classifier is applied on fuzzy-filtered training and testing sets. The proposed framework is applied to identify the software faults based on inter-version and inter-project evaluation. In this framework, the earlier projects or project-versions are considered as training sets and the new projects or versions are taken as testing sets. The experimentation is conducted on nine open source projects taken from {PROMISE} repository as well as on {PDE} and {JDT} projects. The approximation is applied on internal version-specific fault prediction and external software projects evaluation. The comparative analysis is performed against Decision Tree, Random Tree, Random Forest, Naive Bayes and Multilevel Perceptron classifiers. This prediction result signifies that the proposed framework has gained the higher accuracy, lesser error rate and significant {AUC} and {GM} for inter-project and inter-version evaluations. © 2019 Elsevier B.V.},
	pages = {696--713},
	journaltitle = {Applied Soft Computing Journal},
	author = {Juneja, K.},
	date = {2019},
	note = {Publisher: Elsevier Ltd},
	keywords = {Computer software selection and evaluation, Open source software, Forecasting, Classification (of information), Decision trees, Software testing, Project management, Software reliability, Software fault prediction, Defect prediction, Fuzzy inference, Fuzzy systems, Comparative analysis, Fuzzy, Fuzzy sets, Inter project, Intra project, Multi-level perceptron, Neural networks, Neuro fuzzy classifier, xyes}
}

@article{baarah_machine_2019,
	title = {Machine learning approaches for predicting the severity level of software bug reports in closed source projects},
	volume = {10},
	issn = {2158107X},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072269511&doi=10.14569%2fijacsa.2019.0100836&partnerID=40&md5=e2a0bcd27384aa873caffd025fa64dab},
	doi = {10.14569/ijacsa.2019.0100836},
	abstract = {In Software Development Life Cycle, fixing defect bugs is one of the essential activities of the software maintenance phase. Bug severity indicates how major or minor the bug impacts on the execution of the system and how rapidly the developer should fix it. Triaging a vast amount of new bugs submitted to the software bug repositories is a cumbersome and time-consuming process. Manual triage might lead to a mistake in assigning the appropriate severity level for each bug. As a consequence, a delay for fixing severe software bugs will take place. However, the whole process of assigning the severity level for bug reports should be automated. In this paper, we aim to build prediction models that will be utilized to determine the class of the severity (severe or non-severe) of the reported bug. To validate our approach, we have constructed a dataset from historical bug reports stored in {JIRA} bug tracking system. These bug reports are related to different closed-source projects developed by {INTIX} Company located in Amman, Jordan. We compare eight popular machine learning algorithms, namely Naive Bayes, Naive Bayes Multinomial, Support Vector Machine, Decision Tree (J48), Random Forest, Logistic Model Trees, Decision Rules ({JRip}) and K-Nearest Neighbor in terms of accuracy, F-measure and Area Under the Curve ({AUC}). According to the experimental results, a Decision Tree algorithm called Logistic Model Trees achieved better performance compared to other machine learning algorithms in terms of Accuracy, {AUC} and F-measure with values of 86.31, 0.90 and 0.91, respectively. © 2018 The Science and Information ({SAI}) Organization Limited.},
	pages = {285--294},
	number = {8},
	journaltitle = {International Journal of Advanced Computer Science and Applications},
	author = {Baarah, A. and Aloqaily, A. and Salah, Z. and Zamzeer, M. and Sallam, M.},
	date = {2019},
	note = {Publisher: Science and Information Organization},
	keywords = {xno},
	file = {Full Text:C\:\\Users\\michalm\\Zotero\\storage\\GN8MK3X5\\Baarah et al. - 2019 - Machine Learning Approaches for Predicting the Sev.pdf:application/pdf}
}

@article{iqbal_performance_2019,
	title = {Performance analysis of machine learning techniques on software defect prediction using {NASA} datasets},
	volume = {10},
	issn = {2158107X},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066739131&doi=10.14569%2fijacsa.2019.0100538&partnerID=40&md5=039cbe35e0f11a12f4eb5be407c9b4b2},
	doi = {10.14569/ijacsa.2019.0100538},
	abstract = {Defect prediction at early stages of software development life cycle is a crucial activity of quality assurance process and has been broadly studied in the last two decades. The early prediction of defective modules in developing software can help the development team to utilize the available resources efficiently and effectively to deliver high quality software product in limited time. Until now, many researchers have developed defect prediction models by using machine learning and statistical techniques. Machine learning approach is an effective way to identify the defective modules, which works by extracting the hidden patterns among software attributes. In this study, several machine learning classification techniques are used to predict the software defects in twelve widely used {NASA} datasets. The classification techniques include: Naïve Bayes ({NB}), Multi-Layer Perceptron ({MLP}). Radial Basis Function ({RBF}), Support Vector Machine ({SVM}), K Nearest Neighbor ({KNN}), {kStar} (K*), One Rule ({OneR}), {PART}, Decision Tree ({DT}), and Random Forest ({RF}). Performance of used classification techniques is evaluated by using various measures such as: Precision, Recall, F-Measure, Accuracy, {MCC}, and {ROC} Area. The detailed results in this research can be used as a baseline for other researches so that any claim regarding the improvement in prediction through any new technique, model or framework can be compared and verified. © 2018 The Science and Information ({SAI}) Organization Limited.},
	pages = {300--308},
	number = {5},
	journaltitle = {International Journal of Advanced Computer Science and Applications},
	author = {Iqbal, A. and Aftab, S. and Ali, U. and Nawaz, Z. and Sana, L. and Ahmad, M. and Husen, A.},
	date = {2019},
	note = {Publisher: Science and Information Organization},
	keywords = {xno},
	file = {Full Text:C\:\\Users\\michalm\\Zotero\\storage\\KJC5BWR2\\Iqbal et al. - 2019 - Performance Analysis of Machine Learning Technique.pdf:application/pdf}
}

@inproceedings{amasaki_cross-version_2018,
	title = {Cross-version defect prediction using cross-project defect prediction approaches: Does it work?},
	isbn = {978-1-4503-6593-2},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056731345&doi=10.1145%2f3273934.3273938&partnerID=40&md5=4e9a55dd5411a2ac5b51ea8893ac4538},
	doi = {10.1145/3273934.3273938},
	abstract = {Background: Specifying and removing defects before release deserve extra cost for the success of software projects. Long-running projects experience multiple releases, and it is a natural choice to adopt cross-version defect prediction ({CVDP}) that uses information from older versions. A past study shows that feeding multi older versions data may have a positive influence on the performance. The study also suggests that cross-project defect prediction ({CPDP}) may fit the situation but one {CPDP} approach was only examined. Aims: To investigate whether feeding multiple older versions data is effective for {CVDP} using {CPDP} approaches. The investigation also involves performance comparisons of the {CPDP} approaches under {CVDP} situation. Method: We chose a style of replication of the comparative study on {CPDP} approaches by Herbold et al. under {CVDP} situation. Results: Feeding multiple older versions had a positive effect for more than a half {CPDP} approaches. However, almost all of the {CPDP} approaches did not perform significantly better than a simple rule-based prediction. Although the best {CPDP} approach could work better than it and with-in project defect prediction, we found no effect of feeding multiple older versions for it. Conclusions: Feeding multiple older versions could improve {CPDP} approaches under {CVDP} situation. However, it did not work for the best {CPDP} approach in the study. © 2018 Association for Computing Machinery.},
	pages = {32--41},
	booktitle = {{ACM} International Conference Proceeding Series},
	publisher = {Association for Computing Machinery},
	author = {Amasaki, S.},
	date = {2018},
	keywords = {Defects, Forecasting, Predictive analytics, Software engineering, Defect prediction, Comparative studies, Feeding, Multiple release, Performance comparison, Rule based, Software project, xyes}
}

@article{watanabe_cross-validation-based_2018,
	title = {Cross-validation-based association rule prioritization metric for software defect characterization},
	volume = {E101D},
	issn = {09168532},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053838288&doi=10.1587%2ftransinf.2018EDP7020&partnerID=40&md5=9ec0a3efa0bc78a0699afdc104ea6de9},
	doi = {10.1587/transinf.2018EDP7020},
	abstract = {Association rule mining discovers relationships among variables in a data set, representing them as rules. These are expected to often have predictive abilities, that is, to be able to predict future events, but commonly used rule interestingness measures, such as support and confidence, do not directly assess their predictive power. This paper proposes a cross-validation-based metric that quantifies the predictive power of such rules for characterizing software defects. The results of evaluation this metric experimentally using four open-source data sets (Mylyn, {NetBeans}, Apache Ant and {jEdit}) show that it can improve rule prioritization performance over conventional metrics (support, confidence and odds ratio) by 72.8\%for Mylyn, 15.0\%for {NetBeans}, 10.5\%for Apache Ant and 0 for {jEdit} in terms of {SumNormPre}(100) precision criterion. This suggests that the proposed metric can provide better rule prioritization performance than conventional metrics and can at least provide similar performance even in the worst case. © 2018 The Institute of Electronics, Information and Communication Engineers.},
	pages = {2269--2278},
	number = {9},
	journaltitle = {{IEICE} Transactions on Information and Systems},
	author = {Watanabe, T. and Monden, A. and Yücel, Z. and Kamei, Y. and Morisaki, S.},
	date = {2018},
	note = {Publisher: Institute of Electronics, Information and Communication, Engineers, {IEICE}},
	keywords = {Data mining, Computer software selection and evaluation, Defects, Open source software, Association rules, Software Quality, Cross validation, Defect prediction, Precision criteria, Predictive abilities, Rule interestingness, Rule prioritization, Support and confidence, xno},
	file = {Full Text:C\:\\Users\\michalm\\Zotero\\storage\\C785N42S\\Watanabe et al. - 2018 - Cross-Validation-Based Association Rule Prioritiza.pdf:application/pdf}
}

@article{chatterjee_mahalanobis_2018,
	title = {A Mahalanobis distance based algorithm for assigning rank to the predicted fault prone software modules},
	volume = {70},
	issn = {15684946},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049096980&doi=10.1016%2fj.asoc.2018.06.032&partnerID=40&md5=ed49f211663087ac70467c0e841e7f11},
	doi = {10.1016/j.asoc.2018.06.032},
	abstract = {This article proposes a methodology based on Artificial Neural Network({ANN}) and type-2 fuzzy logic system ({FLS}) for detecting the fault prone software modules at early development phase. The present research concentrates on software metrics from requirement analysis and design phase of software life cycle. A new approach has been developed to sort out degree of fault proneness ({DFP}) of the software modules through type-2 {FLS}. {ANN} is used to prepare the rule base for inference engine. Furthermore, the proposed model has induced an order relation among the fault prone modules ({FPMs}) with the help of Mahalanobis distance ({MD}) metric. During software development process, a project manager needs to recognize the fault prone software modules with their {DFP}. Hence, the present study is of great importance to the project personnel to develop more cost-effective and reliable software. {KC}2 dataset of {NASA} has been applied for validating the model. Performance analysis clearly indicates the better prediction capability of the proposed model compared to some existing similar models. © 2018 Elsevier B.V.},
	pages = {764--772},
	journaltitle = {Applied Soft Computing Journal},
	author = {Chatterjee, S. and Maji, B.},
	date = {2018},
	note = {Publisher: Elsevier Ltd},
	keywords = {Software metrics, Computer circuits, Software design, {NASA}, Software modules, Fuzzy inference, Fuzzy logic, Interval type-2 fuzzy logic systems, Life cycle, Neural networks, Cost effectiveness, Mahalanobis distances, Performance analysis, Prediction capability, Software development process, Type-2 fuzzy logic system, Artificial neural network, Fault prone software module, Interval type-2 fuzzy logic system, Mahalanobis distance, xno}
}

@inproceedings{szczypinski_qmazda_2017,
	title = {{QMaZda} - Software tools for image analysis and pattern recognition},
	volume = {2017-September},
	isbn = {978-83-62065-30-1},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041504873&doi=10.23919%2fSPA.2017.8166867&partnerID=40&md5=09992260f6b97c545049c6f6442bd115},
	doi = {10.23919/SPA.2017.8166867},
	abstract = {Qmazda is a package of software tools for digital image analysis. They compute shape, color and texture attributes in arbitrary regions of interest, implement selected algorithms of discriminant analysis and machine learning, and enable texture based image segmentation. The algorithms generalize a concept of texture to three-dimensional data to enable analysis of volumetric images from magnetic resonance imaging or computed tomography scanners. The tools support a complete workflow - from image examples as an input to classification rules as an output. The extracted knowledge can be further used in custom made image analysis systems. Here we also present an application of {QMaZda} to identify defective barley kernels. The cereal seeds variability is high, therefore, characterization and discriminant analysis of such the biological objects is challenging and non-trivial. The software is available free of charge and open source, with executables for Windows, Linux and {OS} X platforms. © 2017 Division of Signal Processing and Electronic Systems, Poznan University of Technology.},
	pages = {217--221},
	booktitle = {Signal Processing - Algorithms, Architectures, Arrangements, and Applications Conference Proceedings, {SPA}},
	publisher = {{IEEE} Computer Society},
	author = {Szczypinski, P.M. and Klepaczko, A. and Kociolek, M.},
	date = {2017},
	note = {{ISSN}: 23260262},
	keywords = {Artificial intelligence, Biological objects, cereal grains, Cereal grains, Classification rules, Computed tomography scanners, Computer architecture, Computer operating systems, Computer software, Computerized tomography, {DH}-{HEMTs}, Digital image analysis, Discriminant analysis, feature extraction, Feature extraction, High definition video, Image analysis, Image analysis systems, Image processing, Image segmentation, Image texture, Integrated circuits, Learning systems, machine learning, Magnetic resonance imaging, Open source software, Open systems, Pattern recognition, Regions of interest, Signal processing, Signal processing algorithms, Three-dimensional data, xno}
}

@article{chatterjee_software_2017,
	title = {Software fault prediction using neuro-fuzzy network and evolutionary learning approach},
	volume = {28},
	issn = {09410643},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976253647&doi=10.1007%2fs00521-016-2437-y&partnerID=40&md5=cbd4f26d3720adad0b1ca215e84179e8},
	doi = {10.1007/s00521-016-2437-y},
	abstract = {In the real world, a great deal of information is provided by human experts that normally do not conform to the rules of physics, but describe the complicated systems by a set of incomplete or vague statements. The need of conducting uncertainty analysis in software reliability for the large and complex system is demanding. For large complex systems made up of many components, the uncertainty of each individual parameter amplifies the uncertainty of the total system reliability. In this paper, to overcome with the problem of uncertainty in software development process and environment, a neuro-fuzzy modeling has been proposed for software fault prediction. The training of the proposed neuro-fuzzy model has been done with genetic algorithm and back-propagation learning algorithm. The proposed model has been validated using some real software failure data. The efficiency of the two learning algorithms has been compared with various fuzzy and statistical time series-based forecasting algorithms on the basis of their prediction ability. © 2016, The Natural Computing Applications Forum.},
	pages = {1221--1231},
	journaltitle = {Neural Computing and Applications},
	author = {Chatterjee, S. and Nigam, S. and Roy, A.},
	date = {2017},
	note = {Publisher: Springer London},
	keywords = {Software design, Forecasting, Genetic algorithms, Software engineering, Reliability analysis, Software reliability, Complex networks, Software fault prediction, Learning algorithms, Computer software, Fuzzy inference, Fuzzy logic, Reliability, Software development process, Algorithms, Backpropagation, Backpropagation algorithms, Backpropagation learning algorithm, Evolutionary Learning, Faulting, Forecasting algorithm, Fuzzy neural networks, Large complex systems, Large scale systems, Neuro-fuzzy network, Total system reliability, Uncertainty analysis, xno}
}

@article{li_what_2017,
	title = {What Are They Talking About? Analyzing Code Reviews in Pull-Based Development Model},
	volume = {32},
	issn = {10009000},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037378589&doi=10.1007%2fs11390-017-1783-2&partnerID=40&md5=27a059207de0baa870b8a5232ac49601},
	doi = {10.1007/s11390-017-1783-2},
	abstract = {Code reviews in pull-based model are open to community users on {GitHub}. Various participants are taking part in the review discussions and the review topics are not only about the improvement of code contributions but also about project evolution and social interaction. A comprehensive understanding of the review topics in pull-based model would be useful to better organize the code review process and optimize review tasks such as reviewer recommendation and pull-request prioritization. In this paper, we first conduct a qualitative study on three popular open-source software projects hosted on {GitHub} and construct a fine-grained two-level taxonomy covering four level-1 categories (code correctness, pull-request decision-making, project management, and social interaction) and 11 level-2 subcategories (e.g., defect detecting, reviewer assigning, contribution encouraging). Second, we conduct preliminary quantitative analysis on a large set of review comments that were labeled by {TSHC} (a two-stage hybrid classification algorithm), which is able to automatically classify review comments by combining rule-based and machine-learning techniques. Through the quantitative study, we explore the typical review patterns. We find that the three projects present similar comments distribution on each subcategory. Pull-requests submitted by inexperienced contributors tend to contain potential issues even though they have passed the tests. Furthermore, external contributors are more likely to break project conventions in their early contributions. © 2017, Springer Science+Business Media, {LLC} \& Science Press, China.},
	pages = {1060--1075},
	number = {6},
	journaltitle = {Journal of Computer Science and Technology},
	author = {Li, Z.-X. and Yu, Y. and Yin, G. and Wang, T. and Wang, H.-M.},
	date = {2017},
	note = {Publisher: Springer New York {LLC}},
	keywords = {Open source software, Learning systems, Machine learning techniques, Open systems, Software engineering, Project management, Codes (symbols), Code review, Hybrid classification, Decision making, Open source software projects, pull-request, Qualitative study, Quantitative study, Social interactions, Social sciences, xno}
}

@article{rathore_towards_2017,
	title = {Towards an ensemble based system for predicting the number of software faults},
	volume = {82},
	issn = {09574174},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018510580&doi=10.1016%2fj.eswa.2017.04.014&partnerID=40&md5=fff8aa3402a4ce72c295edb2460e4e42},
	doi = {10.1016/j.eswa.2017.04.014},
	abstract = {Software fault prediction using different techniques has been done by various researchers previously. It is observed that the performance of these techniques varied from dataset to dataset, which make them inconsistent for fault prediction in the unknown software project. On the other hand, use of ensemble method for software fault prediction can be very effective, as it takes the advantage of different techniques for the given dataset to come up with better prediction results compared to individual technique. Many works are available on binary class software fault prediction (faulty or non-faulty prediction) using ensemble methods, but the use of ensemble methods for the prediction of number of faults has not been explored so far. The objective of this work is to present a system using the ensemble of various learning techniques for predicting the number of faults in given software modules. We present a heterogeneous ensemble method for the prediction of number of faults and use a linear combination rule and a non-linear combination rule based approaches for the ensemble. The study is designed and conducted for different software fault datasets accumulated from the publicly available data repositories. The results indicate that the presented system predicted number of faults with higher accuracy. The results are consistent across all the datasets. We also use prediction at level l (Pred(l)), and measure of completeness to evaluate the results. Pred(l) shows the number of modules in a dataset for which average relative error value is less than or equal to a threshold value l. The results of prediction at level l analysis and measure of completeness analysis have also confirmed the effectiveness of the presented system for the prediction of number of faults. Compared to the single fault prediction technique, ensemble methods produced improved performance for the prediction of number of software faults. Main impact of this work is to allow better utilization of testing resources helping in early and quick identification of most of the faults in the software system. © 2017 Elsevier Ltd},
	pages = {357--382},
	journaltitle = {Expert Systems with Applications},
	author = {Rathore, S.S. and Kumar, S.},
	date = {2017},
	note = {Publisher: Elsevier Ltd},
	keywords = {Empirical studies, Forecasting, Genetic algorithms, Software testing, Software fault prediction, Computer software, Average relative error, Ensemble-based systems, Genetic programming, Gradient boosting, Heterogeneous ensembles, Linear combination rules, Linear regression, Promise repository, xno}
}

@article{goyal_fuzzy_2017,
	title = {Fuzzy inferencing to identify degree of interaction in the development of fault prediction models},
	volume = {29},
	issn = {13191578},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006791416&doi=10.1016%2fj.jksuci.2014.12.008&partnerID=40&md5=3e0f7210b23221ef38cf0b4799ed0bdf},
	doi = {10.1016/j.jksuci.2014.12.008},
	abstract = {The software fault prediction models, based on different modeling techniques have been extensively researched to improve software quality for the last three decades. Out of the analytical techniques used by the researchers, fuzzy modeling and its variants are bringing out a major share of the attention of research communities. In this work, we demonstrate the models developed through data driven fuzzy inference system. A comprehensive set of rules induced by such an inference system, followed by a simplification process provides deeper insight into the linguistically identified level of interaction. This work makes use of a publicly available data repository for four software modules, advocating the consideration of compound effects in the model development, especially in the area of software measurement. One related objective is the identification of influential metrics in the development of fault prediction models. A fuzzy rule intrinsically represents a form of interaction between fuzzified inputs. Analysis of these rules establishes that Low and {NOT} (High) level of inheritance based metrics significantly contributes to the F-measure estimate of the model. Further, the Lack of Cohesion of Methods ({LCOM}) metric was found insignificant in this empirical study. © 2015 The Authors},
	pages = {93--102},
	number = {1},
	journaltitle = {Journal of King Saud University - Computer and Information Sciences},
	author = {Goyal, R. and Chandra, P. and Singh, Y.},
	date = {2017},
	note = {Publisher: King Saud bin Abdulaziz University},
	keywords = {Software fault prediction, Object oriented metrics, Fuzzy inference system, Influential metrics, xyes}
}

@article{abdi_hybrid_2015,
	title = {A hybrid one-class rule learning approach based on swarm intelligence for software fault prediction},
	volume = {11},
	issn = {16145046},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945477440&doi=10.1007%2fs11334-015-0258-2&partnerID=40&md5=55244e135378604d70e7ef15669e8675},
	doi = {10.1007/s11334-015-0258-2},
	abstract = {Software testing is a fundamental activity in the software development process aimed to determine the quality of software. To reduce the effort and cost of this process, defect prediction methods can be used to determine fault-prone software modules through software metrics to focus testing activities on them. Because of model interpretation and easily used by programmers and testers some recent studies presented classification rules to make prediction models. This study presents a rule-based prediction approach based on kernel k-means clustering algorithm and Distance based Multi-objective Particle Swarm Optimization ({DSMOPSO}). Because of discrete search space, we modified this algorithm and named it {DSMOPSO}-D. We prevent best global rules to dominate local rules by dividing the search space with kernel k-means algorithm and by taking different approaches for imbalanced and balanced clusters, we solved imbalanced data set problem. The presented model performance was evaluated by four publicly available data sets from the {PROMISE} repository and compared with other machine learning and rule learning algorithms. The obtained results demonstrate that our model presents very good performance, especially in large data sets. © 2015, Springer-Verlag London.},
	pages = {289--301},
	number = {4},
	journaltitle = {Innovations in Systems and Software Engineering},
	author = {Abdi, Y. and Parsa, S. and Seyfari, Y.},
	date = {2015},
	note = {Publisher: Springer-Verlag London Ltd},
	keywords = {Clustering algorithms, Software design, Forecasting, Learning systems, Classification (of information), Software engineering, Software testing, Fault prediction, Particle swarm optimization ({PSO}), Artificial intelligence, Learning algorithms, Classification rules, Algorithms, Imbalanced Data-sets, Kernel k-means, Multi objective particle swarm optimization, Multiobjective optimization, xno}
}

@article{czibula_detecting_2015,
	title = {Detecting software design defects using relational association rule mining},
	volume = {42},
	issn = {02191377},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891894965&doi=10.1007%2fs10115-013-0721-z&partnerID=40&md5=b53495712645fce717363a3a8717fdb7},
	doi = {10.1007/s10115-013-0721-z},
	abstract = {In this paper, we are approaching, from a machine learning perspective, the problem of automatically detecting defective software entities (classes and methods) in existing software systems, a problem of major importance during software maintenance and evolution. In order to improve the internal quality of a software system, identifying faulty entities such as classes, modules, methods is essential for software developers. As defective software entities are hard to identify, machine learning-based classification models are still developed to approach the problem of detecting software design defects. We are proposing a novel method based on relational association rule mining for detecting faulty entities in existing software systems. Relational association rules are a particular type of association rules and describe numerical orderings between attributes that commonly occur over a dataset. Our method is based on the discovery of relational association rules for identifying design defects in software. Experiments on open source software are conducted in order to detect defective classes in object-oriented software systems, and a comparison of our approach with similar existing approaches is provided. The obtained results show that our method is effective for software design defect detection and confirms the potential of our proposal. © 2014, Springer-Verlag London.},
	pages = {545--577},
	number = {3},
	journaltitle = {Knowledge and Information Systems},
	author = {Czibula, G. and Marian, Z. and Czibula, I.G.},
	date = {2015},
	note = {Publisher: Springer London},
	keywords = {Data mining, Defects, Software design, Open source software, Turing machines, Learning systems, Object oriented programming, Machine learning, Open systems, Association rules, Software developer, Object detection, Software systems, Software entities, Classification models, Defect detection, Internal quality, Object-oriented software systems, Software maintenance and evolution, xyes}
}

@article{tiwari_design_2015,
	title = {Design and implementation of rough set co-processor on {FPGA}},
	volume = {11},
	issn = {13494198},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924651054&partnerID=40&md5=11ae313b9cae6f239288f503551080d8},
	abstract = {Rough set theory is a mathematical approach to process and interpret in¬complete information system. Several researchers have dealt with the problem of finding reduct from the set of attributes, cores, and rules from databases using different soft¬ware deployed on multiprocessor system. Recently researchers have started using Field Programmable Gate Array ({FPGA}) implementation as an alternate option. Software approach is versatile but slow as compared to hardware implementation. The goal of this work is to design an exemplary rough set co-processor based on rough set theory and map it on {FPGA}. This paper gives an insight of a rough set co-processor’s modules. The theory of dealing with large databases is studied. With the usage of dual port {RAM} and pipelining in design, a considerable time is saved thus making it suitable for real time applications. The application for rough set co-processor is explained with the case study of a typical fault dictionary of a Very Large Scale Integrated ({VLSI}) chip. It can be used as a Built-in-Self-Test controller for testing {VLSI} chip. Simulation results show that proposed hardware is significantly faster than algorithms running on general-purpose processor. The rough set co-processor can also be used as hardware classifier unit in per¬sonal computer. © 2015 {ISSN} 1349-4198.},
	pages = {641--656},
	number = {2},
	journaltitle = {International Journal of Innovative Computing, Information and Control},
	author = {Tiwari, K.S. and Kothari, A.G.},
	date = {2015},
	note = {Publisher: {IJICIC} Editorial Office},
	keywords = {Classification (of information), Field programmable gate arrays ({FPGA}), Set theory, Computer hardware, Hardware, Rough set theory, Algorithms, Built-in self test, Co-processors, Computer hardware description languages, Design, Discernibility matrix, General purpose computers, Hardware accelerators, {HDL}, Integrated circuit testing, Reduct, Rules, Testability, xno}
}

@article{moeyersoms_comprehensible_2015,
	title = {Comprehensible software fault and effort prediction: A data mining approach},
	volume = {100},
	issn = {01641212},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919361139&doi=10.1016%2fj.jss.2014.10.032&partnerID=40&md5=2eab3043a0a68f06cf9b0418d024f57f},
	doi = {10.1016/j.jss.2014.10.032},
	abstract = {Software fault and effort prediction are important tasks to minimize costs of a software project. In software effort prediction the aim is to forecast the effort needed to complete a software project, whereas software fault prediction tries to identify fault-prone modules. In this research both tasks are considered, thereby using different data mining techniques. The predictive models not only need to be accurate but also comprehensible, demanding that the user can understand the motivation behind the model's prediction. Unfortunately, to obtain predictive performance, comprehensibility is often sacrificed and vice versa. To overcome this problem, we extract trees from well performing Random Forests ({RFs}) and Support Vector Machines for regression ({SVRs}) making use of a rule extraction algorithm {ALPA}. This method builds trees (using C4.5 and {REPTree}) that mimic the black-box model ({RF}, {SVR}) as closely as possible. The proposed methodology is applied to publicly available datasets, complemented with new datasets that we have put together based on the Android repository. Surprisingly, the trees extracted from the black-box models by {ALPA} are not only comprehensible and explain how the black-box model makes (most of) its predictions, but are also more accurate than the trees obtained by working directly on the data. © 2014 Elsevier Inc. All rights reserved.},
	pages = {80--90},
	journaltitle = {Journal of Systems and Software},
	author = {Moeyersoms, J. and Junqué De Fortuny, E. and Dejaeger, K. and Baesens, B. and Martens, D.},
	date = {2015},
	note = {Publisher: Elsevier Inc.},
	keywords = {Data mining, Forecasting, Decision trees, Forestry, Effort prediction, Extraction, Software fault prediction, Computer software, Rule extraction, Comprehensibility, Computer Programs, Data Processing, Fault-prone modules, Predictive performance, Rule extraction algorithms, Software effort prediction, Trees, Software fault and effort prediction, xyes},
	file = {Accepted Version:C\:\\Users\\michalm\\Zotero\\storage\\VYDKZISE\\Moeyersoms et al. - 2015 - Comprehensible software fault and effort predictio.pdf:application/pdf}
}

@article{ma_investigating_2014,
	title = {Investigating associative classification for software fault prediction: An experimental perspective},
	volume = {24},
	issn = {02181940},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902316977&doi=10.1142%2fS021819401450003X&partnerID=40&md5=24b92bd4a35a5c9e854ed011c73d58ec},
	doi = {10.1142/S021819401450003X},
	abstract = {It is a recurrent finding that software development is often troubled by considerable delays as well as budget overruns and several solutions have been proposed in answer to this observation, software fault prediction being a prime example. Drawing upon machine learning techniques, software fault prediction tries to identify upfront software modules that are most likely to contain faults, thereby streamlining testing efforts and improving overall software quality. When deploying fault prediction models in a production environment, both prediction performance and model comprehensibility are typically taken into consideration, although the latter is commonly overlooked in the academic literature. Many classification methods have been suggested to conduct fault prediction; yet associative classification methods remain uninvestigated in this context. This paper proposes an associative classification ({AC})-based fault prediction method, building upon the {CBA}2 algorithm. In an empirical comparison on 12 real-world datasets, the {AC}-based classifier is shown to achieve a predictive performance competitive to those of models induced by five other tree/rule-based classification techniques. In addition, our findings also highlight the comprehensibility of the {AC}-based models, while achieving similar prediction performance. Furthermore, the possibilities of cross project prediction are investigated, strengthening earlier findings on the feasibility of such approach when insufficient data on the target project is available. © 2014 World Scientific Publishing Company.},
	pages = {61--90},
	number = {1},
	journaltitle = {International Journal of Software Engineering and Knowledge Engineering},
	author = {Ma, B. and Zhang, H. and Chen, G. and Zhao, Y. and Baesens, B.},
	date = {2014},
	note = {Publisher: World Scientific Publishing Co. Pte Ltd},
	keywords = {Computer software selection and evaluation, Forecasting, Learning systems, Classification (of information), Associative classification, Software testing, Budget control, Artificial intelligence, Software fault prediction, Prediction performance, comprehensibility, cross project validation, xno}
}

@article{rafael_lenz_linking_2013,
	title = {Linking software testing results with a machine learning approach},
	volume = {26},
	issn = {09521976},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876919615&doi=10.1016%2fj.engappai.2013.01.008&partnerID=40&md5=d736723192e999d5f815a975d64fb628},
	doi = {10.1016/j.engappai.2013.01.008},
	abstract = {Software testing techniques and criteria are considered complementary since they can reveal different kinds of faults and test distinct aspects of the program. The functional criteria, such as Category Partition, are difficult to be automated and are usually manually applied. Structural and fault-based criteria generally provide measures to evaluate test sets. The existing supporting tools produce a lot of information including: input and produced output, structural coverage, mutation score, faults revealed, etc. However, such information is not linked to functional aspects of the software. In this work, we present an approach based on machine learning techniques to link test results from the application of different testing techniques. The approach groups test data into similar functional clusters. After this, according to the tester's goals, it generates classifiers (rules) that have different uses, including selection and prioritization of test cases. The paper also presents results from experimental evaluations and illustrates such uses. © 2013 Elsevier Ltd.},
	pages = {1631--1640},
	number = {5},
	journaltitle = {Engineering Applications of Artificial Intelligence},
	author = {Rafael Lenz, A. and Pozo, A. and Regina Vergilio, S.},
	date = {2013},
	keywords = {Testing, Learning systems, Software testing, Machine learning approaches, Category partition, Experimental evaluation, Functional aspects, Software testing techniques, Supporting tool, Test coverage criteria, Testing technique, xno}
}

@article{chang_integrating_2013,
	title = {Integrating action-based defect prediction to provide recommendations for defect action correction},
	volume = {23},
	issn = {02181940},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879385903&doi=10.1142%2fS0218194013500022&partnerID=40&md5=e888fc5bc9722ab6fbb1a4fb3591f3c1},
	doi = {10.1142/S0218194013500022},
	abstract = {Reducing software defects is an essential activity for Software Process Improvement. The Action-Based Defect Prediction ({ABDP}) approach fragments the software process into actions, and builds software defect prediction models using data collected from the execution of actions and reported defects. Though the {ABDP} approach can be applied to predict possible defects in subsequent actions, the efficiency of corrections is dependent on the skill and knowledge of the stakeholders. To address this problem, this study proposes the Action Correction Recommendation ({ACR}) model to provide recommendations for action correction, using the Negative Association Rule mining technique. In addition to applying the association rule mining technique to build a High Defect Prediction Model ({HDPM}) to identify high defect action, the {ACR} builds a Low Defect Prediction Model ({LDPM}). For a submitted action, each {HDPM} rule used to predict the action as a high defect action and the {LDPM} rules are analyzed using negative association rule mining to spot the rule items with different characteristics in {HDPM} and {LDPM} rules. This information not only identifies the attributes required for corrections, but also provides a range (or a value) to facilitate the high defect action corrections. This study applies the {ACR} approach to a business software project to validate the efficiency of the proposed approach. The results show that the recommendations obtained can be applied to decrease software defect removal efforts. © 2013 World Scientific Publishing Company.},
	pages = {147--172},
	number = {2},
	journaltitle = {International Journal of Software Engineering and Knowledge Engineering},
	author = {Chang, C.-P.},
	date = {2013},
	keywords = {Defects, Software defect prediction, Defect prediction models, Forecasting, Defect prediction, Mathematical models, action correction recommendation, clustering, Negative association rules, Software process, Software Process Improvement, xno}
}

@inproceedings{woodley_multisource_2011,
	title = {Multisource information fusion for logistics},
	volume = {8064},
	isbn = {978-0-8194-8638-7},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960107118&doi=10.1117%2f12.883498&partnerID=40&md5=c5a4cca5b2260cf032c26a5b3a190517},
	doi = {10.1117/12.883498},
	abstract = {Current Army logistical systems and databases contain massive amounts of data that need an effective method to extract actionable information. The databases do not contain root cause and case-based analysis needed to diagnose or predict breakdowns. A system is needed to find data from as many sources as possible, process it in an integrated fashion, and disseminate information products on the readiness of the fleet vehicles. 21st Century Systems, Inc. introduces the Agent- Enabled Logistics Enterprise Intelligence System ({AELEIS}) tool, designed to assist logistics analysts with assessing the availability and prognostics of assets in the logistics pipeline. {AELEIS} extracts data from multiple, heterogeneous data sets. This data is then aggregated and mined for data trends. Finally, data reasoning tools and prognostics tools evaluate the data for relevance and potential issues. Multiple types of data mining tools may be employed to extract the data and an information reasoning capability determines what tools are needed to apply them to extract information. This can be visualized as a push-pull system where data trends fire a reasoning engine to search for corroborating evidence and then integrate the data into actionable information. The architecture decides on what reasoning engine to use (i.e., it may start with a rule-based method, but, if needed, go to condition based reasoning, and even a model-based reasoning engine for certain types of equipment). Initial results show that {AELEIS} is able to indicate to the user of potential fault conditions and root-cause information mined from a database. © 2011 {SPIE}.},
	booktitle = {Proceedings of {SPIE} - The International Society for Optical Engineering},
	author = {Woodley, R. and Petrov, P. and Noll, W.},
	date = {2011},
	note = {{ISSN}: 0277786X},
	keywords = {Data mining, Software agents, Expert systems, Database systems, Search engines, Algorithms, Potential faults, Data-mining tools, Logistics, Root cause, Data reasoning, Data trend, Equipment, Fleet operations, Heterogeneous data, Information dissemination, Information fusion, Information products, Integrated fashion, Logistical systems, Logistics enterprise, Model-based Reasoning, Multi-source information fusion, Reasoning capabilities, Reasoning engine, Rule-based method, Vehicle health monitoring, xno}
}

@article{peng_ensemble_2011,
	title = {Ensemble of software defect predictors: An {AHP}-based evaluation method},
	volume = {10},
	issn = {02196220},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-78751522102&doi=10.1142%2fS0219622011004282&partnerID=40&md5=d535f5e25a57ca903f2c57a8efface15},
	doi = {10.1142/S0219622011004282},
	abstract = {Classification algorithms that help to identify software defects or faults play a crucial role in software risk management. Experimental results have shown that ensemble of classifiers are often more accurate and robust to the effects of noisy data, and achieve lower average error rate than any of the constituent classifiers. However, inconsistencies exist in different studies and the performances of learning algorithms may vary using different performance measures and under different circumstances. Therefore, more research is needed to evaluate the performance of ensemble algorithms in software defect prediction. The goal of this paper is to assess the quality of ensemble methods in software defect prediction with the analytic hierarchy process ({AHP}), which is a multicriteria decision-making approach that prioritizes decision alternatives based on pairwise comparisons. Through the application of the {AHP}, this study compares experimentally the performance of several popular ensemble methods using 13 different performance metrics over 10 public-domain software defect datasets from the {NASA} Metrics Data Program ({MDP}) repository. The results indicate that ensemble methods can improve the classification results of software defect prediction in general and {AdaBoost} gives the best results. In addition, tree and rule based classifiers perform better in software defect prediction than other types of classifiers included in the experiment. In terms of single classifier, K-nearest-neighbor, C4.5, and Naïve Bayes tree ranked higher than other classifiers. © 2011 World Scientific Publishing Company.},
	pages = {187--206},
	number = {1},
	journaltitle = {International Journal of Information Technology and Decision Making},
	author = {Peng, Y. and Kou, G. and Wang, G. and Wu, W. and Shi, Y.},
	date = {2011},
	keywords = {xno},
	file = {Submitted Version:C\:\\Users\\michalm\\Zotero\\storage\\T8PAZY9S\\Peng et al. - 2011 - ENSEMBLE OF SOFTWARE DEFECT PREDICTORS AN AHP-BAS.pdf:application/pdf}
}

@article{zheng_cost-sensitive_2010,
	title = {Cost-sensitive boosting neural networks for software defect prediction},
	volume = {37},
	issn = {09574174},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-77249108251&doi=10.1016%2fj.eswa.2009.12.056&partnerID=40&md5=c5ace00567884beaa8404f785eb1702b},
	doi = {10.1016/j.eswa.2009.12.056},
	abstract = {Software defect predictors which classify the software modules into defect-prone and not-defect-prone classes are effective tools to maintain the high quality of software products. The early prediction of defect-proneness of the modules can allow software developers to allocate the limited resources on those defect-prone modules such that high quality software can be produced on time and within budget. In the process of software defect prediction, the misclassification of defect-prone modules generally incurs much higher cost than the misclassification of not-defect-prone ones. Most of the previously developed predication models do not consider this cost issue. In this paper, three cost-sensitive boosting algorithms are studied to boost neural networks for software defect prediction. The first algorithm based on threshold-moving tries to move the classification threshold towards the not-fault-prone modules such that more fault-prone modules can be classified correctly. The other two weight-updating based algorithms incorporate the misclassification costs into the weight-update rule of boosting procedure such that the algorithms boost more weights on the samples associated with misclassified defect-prone modules. The performances of the three algorithms are evaluated by using four datasets from {NASA} projects in terms of a singular measure, the Normalized Expected Cost of Misclassification ({NECM}). The experimental results suggest that threshold-moving is the best choice to build cost-sensitive software defect prediction models with boosted neural networks among the three algorithms studied, especially for the datasets from projects developed by object-oriented language. © 2009 Elsevier Ltd. All rights reserved.},
	pages = {4537--4543},
	number = {6},
	journaltitle = {Expert Systems with Applications},
	author = {Zheng, J.},
	date = {2010},
	keywords = {Computer software selection and evaluation, Defects, {NASA}, Software defect prediction, Software defects, Object oriented programming, High-quality software, Software developer, Software modules, Early prediction, Adaptive boosting, Neural networks, Algorithms, Costs, Fault-prone modules, Mathematical models, Data sets, {AdaBoost}, Best choice, Boosting algorithm, Cost-sensitive, Effective tool, Expected cost of misclassification, High quality, Misclassification costs, Misclassifications, {NASA} projects, Object-oriented languages, {ON} time, Predication model, Singular measures, xno}
}

@article{rodriguez-echeverria_suggesting_2021,
	title = {Suggesting model transformation repairs for rule-based languages using a contract-based testing approach},
	issn = {1619-1366},
	doi = {10.1007/s10270-021-00891-0},
	abstract = {Model transformations play an essential role in most model-driven software projects. As the size and complexity of model transformations increase, their reuse, evolution and maintenance become a challenge. This work further details the Model Transformation {TEst} Specification ({MoTES}) approach, which leverages contract-based model testing techniques to assist engineers in model transformation evolution and repairing. The main novelty of our approach is to use contract-based model transformation testing as a foundation to derive suggestions of concrete adaptation actions. {MoTES} uses contracts to specify the expected behaviour of the model transformation under test. These contracts are transformed into model transformations which act as oracles on input-output model pairs, previously generated by executing the transformation under test on provided input models. By further processing, the oracles' output model, precision and recall metrics are calculated for every output pattern (testing results). These metrics are then categorised to increase the user's ability to interpret and act on them. The {MoTES} approach defines 8 cases for precision and recall values classification (test result cases). As traceability information is retained from transformation rules to each output pattern, it is possible to classify each transformation rule involved according to its impact on the metrics, e.g. the number of true positives generated. The {MoTES} approach defines 37 cases for these classifications, with each one linked to a particular (abstract) action suggested on a rule, such as relaxation of the rules. A comprehensive evaluation of this approach is also presented, consisting of three case studies. Two previous case studies performed over two model transformations ({UML}2ER and E2M) are replicated to contrast {MoTES} with an existing model transformation fault localisation approach. An additional case study presents how {MoTES} helps with the evolution of an existing model transformation in the context of a reverse engineering project. Main evaluation results show that our approach can not only detect the errors introduced in the transformations but also localise the faulty rule and suggest the proper repair actions, which significantly reduce testers' effort. From a quantitative perspective, in the third case study, {MoTES} was able to indicate one faulty rule from 19 possibilities for each result case and suggest one or two repair actions from 6 possibilities for each faulty rule.},
	journaltitle = {{SOFTWARE} {AND} {SYSTEMS} {MODELING}},
	author = {Rodriguez-Echeverria, Roberto and Macias, Fernando and Rutle, Adrian and Conejero, Jose M.},
	date = {2021},
	keywords = {Testing, Precision and recall, Classification (of information), Comprehensive evaluation, Evaluation results, Model transformation, Repair, Reverse engineering, Rule-based language, Test specifications, Traceability information, Transformation rules, xno}
}

@article{shankar_optimization_2021,
	title = {Optimization of association rules using hybrid data mining technique},
	issn = {1614-5046},
	doi = {10.1007/s11334-021-00387-6},
	abstract = {Software quality has been the important area of interest for decades now in the {IT} sector and software firms. Defect prediction gives the tester the pointers as to where the bugs will most likely be hidden in the software product. Identifying and reporting the defect probe areas is the main job of software defect prediction techniques. Early detection of software defects during Software Development Life Cycle could lead to a reduction in cost of development, time involved in further testing activities and rework effort post-production and maintenance phase, thus resulting in more reliable software. Software metrics can be used for developing the defect prediction models. Several data mining techniques can be applied on the available open-source software datasets. These datasets are extracted from software programs. Such datasets made publicly available by National Aeronautics and Space Administration for their various softwares have been extensively used in software engineering-related research activities. These datasets contain information on associated Software Metrics at module level. The proposed idea is a novel hybrid data mining technique consisting of Clustering and Modified Apriori Algorithm that results in improved efficiency and reliability of Software Defect Prediction. This technique works by reducing the number of association rules generated. The results are achieved by using interestingness measure called spread. The paper also does a comparative analysis of the results obtained from the novel technique with the existing hybrid technique of Clustering and Apriori.},
	journaltitle = {{INNOVATIONS} {IN} {SYSTEMS} {AND} {SOFTWARE} {ENGINEERING}},
	author = {Shankar, Sahana P. and Naresh, E. and Agrawal, Harshit},
	date = {2021},
	keywords = {xno}
}

@article{stahl_online_2021,
	title = {Online Verification Enabling Approval of Driving Functions-Implementation for a Planner of an Autonomous Race Vehicle},
	volume = {2},
	doi = {10.1109/OJITS.2021.3078121},
	abstract = {Safety guarantees and regulatory approval for autonomous vehicles remain an ongoing challenge. In particular, software that is frequently adapted or contains complex, non-transparent components, such as artificial intelligence, is exceeding the limits of safety standards. This paper presents a detailed implementation of an online verification module - the Supervisor - that copes with these challenges. The presented implementation focuses on autonomous race vehicles without loss of generality. Following an identified holistic list of safety-relevant requirements for a trajectory, metrics are developed to monitor whether the trajectory can safely be executed. To evaluate safety with respect to dynamic objects in a semi-structured and highly dynamic racing environment, rule-based reachable sets are presented. As a result, the pure reachable set is further constrained by applicable regulations. Real-time capability and effectiveness are demonstrated in fault-injected scenario-based tests and on real-world run data. The implemented Supervisor will be publicly available on {GitHub}.},
	pages = {97--110},
	journaltitle = {{IEEE} {OPEN} {JOURNAL} {OF} {IN}℡{LIGENT} {TRANSPORTATION} {SYSTEMS}},
	author = {Stahl, Tim and Diermeyer, Frank},
	date = {2021},
	keywords = {xno}
}

@article{draz_code_2021,
	title = {Code Smell Detection Using Whale Optimization Algorithm},
	volume = {68},
	issn = {1546-2218},
	doi = {10.32604/cmc.2021.015586},
	abstract = {Software systems have been employed in many fields as a means to reduce human efforts; consequently, stakeholders are interested in more updates of their capabilities. Code smells arise as one of the obstacles in the software industry. They are characteristics of software source code that indicate a deeper problem in design. These smells appear not only in the design but also in software implementation. Code smells introduce bugs, affect software maintainability, and lead to higher maintenance costs. Uncovering code smells can be formulated as an optimization problem of finding the best detection rules. Although researchers have recommended different techniques to improve the accuracy of code smell detection, these methods are still unstable and need to be improved. Previous research has sought only to discover a few at a time (three or five types) and did not set rules for detecting their types. Our research improves code smell detection by applying a search-based technique; we use the Whale Optimization Algorithm as a classifier to find ideal detection rules. Applying this algorithm, the Fisher criterion is utilized as a fitness function to maximize the between-class distance over the within class variance. The proposed framework adopts if-then detection rules during the software development life cycle. Those rules identify the types for both medium and large projects. Experiments are conducted on five open-source software projects to discover nine smell types that mostly appear in codes. The proposed detection framework has an average of 94.24\% precision and 93.4\% recall. These accurate values are better than other search-based algorithms of the same field. The proposed framework improves code smell detection, which increases software quality while minimizing maintenance effort, time, and cost. Additionally, the resulting classification rules are analyzed to find the software metrics that differentiate the nine code smells.},
	pages = {1919--1935},
	number = {2},
	journaltitle = {{CMC}-{COMPUTERS} {MATERIALS} \& {CONTINUA}},
	author = {Draz, Moatasem M. and Farhan, Marwa S. and Abdulkader, Sarah N. and Gafar, M. G.},
	date = {2021},
	keywords = {xyes}
}

@article{zhang_adoption_2020,
	title = {Adoption of Photovoltaic Array Modeling and Data Mining Technology in Its Fault Detection},
	volume = {15},
	issn = {1555-130X},
	doi = {10.1166/jno.2020.2872},
	abstract = {Online monitoring and fault diagnosis of the operation status of the photovoltaic power generation system are of great significance to ensure the safety and stability of the photovoltaic power generation system. The power generation characteristics of photovoltaic arrays are analyzed and the simulation model of distributed photovoltaic power generation units is constructed via the {MatLab}/Simulink software. The operating status of photovoltaic arrays and the system are simulated under different environmental conditions, so as to analyze the influence of changing irradiance and temperature on the photovoltaic power generation system. Then the causes of photovoltaic array fault are explained and the types of photovoltaic fault are summarized. The real-time operating data (photovoltaic array current, output power, positive photovoltaic panel working temperature, ambient temperature collected by environmental testing, irradiance, and micro-inverter fault indication data, etc.) are used to establish a data mining decision tree model. The modeling steps include variable initialization, determination of split points, pruning, and rule set reasoning, then the fault branching situation of the photovoltaic array is judged based on the model. Based on the {SPSS} and {MatLab}, in the decision tree photovoltaic array fault detection experiment, the C5.0 algorithm and {CART} algorithm are adopted respectively so as to obtain the fault decision tree of the photovoltaic array orresponding to the algorithm, among which the accuracy of the fault prediction results basd Delivered on the by decision tree corresponding to the C5.0 algorithm is relatively high, and it is also better than the latter compared with the {CART} algorithm.},
	pages = {1225--1233},
	number = {10},
	journaltitle = {{JOURNAL} {OF} {NANOELECTRONICS} {AND} {OPTOELECTRONICS}},
	author = {Zhang, Xiaohong},
	date = {2020-10},
	keywords = {xno}
}

@article{sabbaghi_fcci_2020,
	title = {{FCCI}: A fuzzy expert system for identifying coincidental correct test cases},
	volume = {168},
	issn = {0164-1212},
	doi = {10.1016/j.jss.2020.110635},
	abstract = {Spectrum-based fault localization ({SBFL}) is a promising approach to reduce the cost of program debugging and there has been a large body of research on introducing effective {SBFL} techniques. However, performance of these techniques can be adversely affected by the existence of coincidental correct ({CC}) test cases in the test suites. Such test cases execute the faulty statement but do not cause failures. Given that coincidental correctness is prevalent, it is necessary to precisely identify {CC} test cases and eliminate their effects from test suites. To do so, in this paper, we propose several important factors to identify {CC} test cases and model the {CC} identification process as a decision making system by constructing a fuzzy expert system and proposing a novel fuzzy {CC} identification method, namely {FCCI}. {FCCI} estimates the {CC} likelihood of passed test cases using the designed fuzzy rules, which effectively correlate the proposed {CC} identification factors. We evaluated {FCCI} by conducting extensive experiments on 17 popular and open source subject programs ranging from small- to large-scale containing both artificial and real faults. The experimental results indicate that {FCCI} successfully improves the accuracy of the {CC} identification as well as the accuracy of the representative {SBFL} techniques. (C) 2020 Elsevier Inc. All rights reserved.},
	journaltitle = {{JOURNAL} {OF} {SYSTEMS} {AND} {SOFTWARE}},
	author = {Sabbaghi, Arash and Keyvanpour, Mohammad Reza and Parsa, Saeed},
	date = {2020-10},
	keywords = {Software debugging, Spectrum-based fault localization, Fuzzy expert system, Coincidentally correct test cases, xno}
}

@article{morariu_machine_2020,
	title = {Machine learning for predictive scheduling and resource allocation in large scale manufacturing systems},
	volume = {120},
	issn = {0166-3615},
	doi = {10.1016/j.compind.2020.103244},
	abstract = {The digitalization processes in manufacturing enterprises and the integration of increasingly smart shop floor devices and software control systems caused an explosion in the data points available in Manufacturing Execution Systems. The degree in which enterprises can capture value from big data processing and extract useful insights represents a differentiating factor in developing controls that optimize production and protect resources. Machine learning and Big Data technologies have gained increased traction being adopted in some critical areas of planning and control. Cloud manufacturing allows using these technologies in real time, lowering the cost of implementing and deployment. In this context, the paper offers a machine learning approach for reality awareness and optimization in cloud. Specifically, the paper focuses on predictive production planning (operation scheduling, resource allocation) and predictive maintenance. The main contribution of this research consists in developing a hybrid control solution that uses Big Data techniques and machine learning algorithms to process in real time information streams in large scale manufacturing systems, focusing on energy consumptions that are aggregated at various layers. The control architecture is distributed at the edge of the shop floor for data collecting and format transformation, and then centralized at the cloud computing platform for data aggregation, machine learning and intelligent decisions. The information is aggregated in logical streams and consolidated based on relevant metadata; a neural network is trained and used to determine possible anomalies or variations relative to the normal patterns of energy consumption at each layer. This novel approach allows for accurate forecasting of energy consumption patterns during production by using Long Short-term Memory neural networks and deep learning in real time to re-assign resources (for batch cost optimization) and detect anomalies (for robustness) based on predicted enerdgy data. (C) 2020 Elsevier B.V. All rights reserved.},
	journaltitle = {{COMPUTERS} {IN} {INDUSTRY}},
	author = {Morariu, Cristina and Morariu, Octavian and Raileanu, Silviu and Borangiu, Theodor},
	date = {2020-09},
	keywords = {xno}
}

@article{ceron-figueroa_stochastic_2020,
	title = {Stochastic gradient boosting for predicting the maintenance effort of software-intensive systems},
	volume = {14},
	issn = {1751-8806},
	doi = {10.1049/iet-sen.2018.5332},
	abstract = {The maintenance of software-intensive systems ({SISs}) must be undertaken to correct faults, improve the design, implement enhancements, adapt programmes such that different hardware, software, system features, and telecommunications facilities can be used, as well as to migrate legacy software. A lack of planning has been identified as one explanation for late and over budget software projects. An activity of planning is effort prediction. The goal of this study is to propose the application of a stochastic gradient boosting ({SGB}) model for predicting the {SIS} maintenance effort. We compare the {SGB} prediction accuracy with those obtained with statistical regression, neural network, support vector regression, decision trees, and association rules. We trained and tested the models with five {SIS} data sets selected from the International Software Benchmarking Standards Group Release 11. The {SGB} prediction accuracy was statistically better than the mentioned five models in the two larger data sets. We can conclude that a {SGB} can be applied to predict the maintenance effort of {SISs} coded in languages of the third generation and developed on either mainframes or multi-platform. The predicted effort corresponds to the aggregate of efforts obtained from the project team, project management, and project administration.},
	pages = {82--87},
	number = {2},
	journaltitle = {{IET} {SOFTWARE}},
	author = {Ceron-Figueroa, Sergio and Lopez-Martin, Cuauhtemoc and Yanez-Marquez, Cornelio},
	date = {2020-04},
	note = {Publisher: {IEEE}; Centro Investigacion Matematicas A C},
	keywords = {Forecasting, Stochastic systems, Decision trees, Software testing, Budget control, Computer software maintenance, Effort prediction, Human resource management, International Software Benchmarking Standards Group, Legacy systems, Maintenance efforts, Prediction accuracy, Project management, Software intensive systems, Statistical regression, Stochastic gradient boosting, Stochastic models, Support vector regression, Third generation, xno},
	file = {Full Text:C\:\\Users\\michalm\\Zotero\\storage\\UMH7U5YA\\Cerón-Figueroa et al. - 2020 - Stochastic gradient boosting for predicting the ma.pdf:application/pdf}
}

@article{malik_empirical_2019,
	title = {Empirical Role Rule Classification Model for Software Fault Forecast with Vector Machine Analysis},
	volume = {19},
	issn = {1738-7906},
	abstract = {Our research aims to be analyses the software fault forecast with the help of machine learning and data mining tools. The analysis depends upon defected and non-defected datasets models. The datasets model we have used here are {NASA} datasets models. Our research proposed methodology is rule classification classifier with the help of vector machine. We have illustrated results in tp-rate, f- measure, area under curve ({ROC}) and correctly classified instances. Basically, these are measure efficiency unit which are used for measuring the accuracy and improvement of software fault forecast we have used here for analysis the proposed methodology vector machine with rule classification classifiers and without using of vector machine analysis. We observed that M5rule classifier is worst classifier in all over rule classification because it decreased his efficiency in all scenario case during the use of vector machine. But without using proposed solution methodology we can use it for analysis and can compare their results with other classifiers. {ONER} and {PART} classifiers are very good in all scenario cases because they have enhanced the efficiency and also improved the correctly classified instance c.c.i \% ratio.},
	pages = {195--201},
	number = {9},
	journaltitle = {{INTERNATIONAL} {JOURNAL} {OF} {COMPUTER} {SCIENCE} {AND} {NETWORK} {SECURITY}},
	author = {Malik, Maaz Rasheed and Yining, Liu and Shaikh, Salahuddin},
	date = {2019-09-30},
	keywords = {xyes}
}

@article{czibula_novel_2019,
	title = {A novel concurrent relational association rule mining approach},
	volume = {125},
	issn = {0957-4174},
	doi = {10.1016/j.eswa.2019.01.082},
	abstract = {Data mining techniques are intensively used to uncover relevant patterns in large volumes of complex data which are continuously extended with newly arrived data instances. Relational association rules ({RARs}), a data analysis and mining concept, have been introduced as an extension of classical association rules ({ARs}) for capturing various relationships between the attributes characterizing the data. Due to its {NP}-completeness, the problem of mining all the interesting {RARs} within a data set is computationally difficult. As the dimensionality of the data set to be mined increases, the classical algorithm Discovery of Relational Association Rules ({DRAR}) for {RARs} mining fails in providing the set of rules in reasonable time. This paper introduces a new approach named {CRAR} (Concurrent Relational Association Rule mining) which uses concurrency for the {RARs} discovery process and thus significantly reduces the mining time. The effectiveness of {CRAR} is empirically validated on nine open source data sets. The reduction in mining time when using {CRAR} against {DRAR} emphasizes that it can be successfully applied in various practical data mining scenarios. (C) 2019 Elsevier Ltd. All rights reserved.},
	pages = {142--156},
	journaltitle = {{EXPERT} {SYSTEMS} {WITH} {APPLICATIONS}},
	author = {Czibula, Gabriela and Czibula, Istvan Gergely and Miholca, Diana-Lucia and Crivei, Liana Maria},
	date = {2019-07-01},
	keywords = {xno}
}

@article{zhang_implementation_2019,
	title = {Implementation of the Tresca yield criterion in finite element analysis of burst capacity of pipelines},
	volume = {172},
	issn = {0308-0161},
	doi = {10.1016/j.ijpvp.2019.03.037},
	abstract = {The Tresca and von Mises criteria are two classic yield criteria for metals. While the von Mises criterion has been widely implemented in commercial finite element analysis ({FEA}) software package, the Tresca yield criterion in the context of the associated-plastic flow rule is not available in any well-known {FEA} packages such as {ABAQUS} and {ANSYS}. In this study, the Tresca yield criterion as well as the associated plastic flow and hardening rules are implemented in a user-defined material model ({UMAT}) in commercial {FEA} package {ABAQUS} based on the return-mapping technique. The consistent tangent operator for the Tresca criterion is also implemented in the {UMAT} model to facilitate the computation of the tangent stiffness matrix during the Newton-Raphson iteration for solving the nonlinear global equilibrium equations in the plastic domain. The constitutive integration algorithm for the Tresca {UMAT} is written in the principal stress space, which markedly simplifies the implementation. The implemented Tresca {UMAT} model is validated by theoretical solution and full-scale burst tests of both pristine and defected pipes. Furthermore, based on the Tresca-based and von Mises-based {FEA} predictions, a new formula is proposed to accurately predict burst pressure of thin-walled corroded pipes.},
	pages = {180--187},
	journaltitle = {{INTERNATIONAL} {JOURNAL} {OF} {PRESSURE} {VESSELS} {AND} {PIPING}},
	author = {Zhang, Shulong and Wang, Qingguo and Zhou, Wenxing},
	date = {2019-05},
	keywords = {xno}
}

@article{niedermayr_too_2019,
	title = {Too trivial to test? An inverse view on defect prediction to identify methods with low fault risk},
	issn = {2376-5992},
	doi = {10.7717/peerj-cs.187},
	abstract = {Background: Test resources are usually limited and therefore it is often not possible to completely test an application before a release. To cope with the problem of scarce resources, development teams can apply defect prediction to identify fault-prone code regions. However, defect prediction tends to low precision in cross-project prediction scenarios. Aims: We take an inverse view on defect prediction and aim to identify methods that can be deferred when testing because they contain hardly any faults due to their code being “trivial”. We expect that characteristics of such methods might be project-independent, so that our approach could improve cross-project predictions. Method: We compute code metrics and apply association rule mining to create rules for identifying methods with low fault risk ({LFR}). We conduct an empirical study to assess our approach with six Java open-source projects containing precise fault data at the method level. Results: Our results show that inverse defect prediction can identify approx. 32-44\% of the methods of a project to have a {LFR}; on average, they are about six times less likely to contain a fault than other methods. In cross-project predictions with larger, more diversified training sets, identified methods are even 11 times less likely to contain a fault. Conclusions: Inverse defect prediction supports the efficient allocation of test resources by identifying methods that can be treated with less priority in testing activities and is well applicable in cross-project prediction scenarios.},
	journaltitle = {{PEERJ} {COMPUTER} {SCIENCE}},
	author = {Niedermayr, Rainer and Roehm, Tobias and Wagner, Stefan},
	date = {2019-04-15},
	keywords = {Testing, Defects, Empirical studies, Open source software, Forecasting, Open source projects, Codes (symbols), Defect prediction, Development teams, Efficient allocations, Fault-prone codes, Identifying methods, Inverse problems, Low-fault-risk methods, Well testing, xno},
	file = {Full Text:C\:\\Users\\michalm\\Zotero\\storage\\9G2E4NKN\\Niedermayr et al. - 2019 - Too trivial to test An inverse view on defect pre.pdf:application/pdf}
}

@article{vakilian_fuzzy-based_2018,
	title = {A fuzzy-based decision making software for enzymatic electrochemical nitrate biosensors},
	volume = {177},
	issn = {0169-7439},
	doi = {10.1016/j.chemolab.2018.04.016},
	abstract = {So far, many analytical biosensors have been introduced to determine the concentration of a wide variety of analytes in environmental and agricultural samples. However, a major part of these biosensors has not been yet developed in commercial portable devices due to the fact that implementation of these methods requires an analyst to interpret the biosensor's response for analyte concentration determination. In this study, a mobile application for {iOS} platform is developed to predict samples' nitrate concentration in a mediated enzyme-based three-electrode biosensor. The introduced application uses fuzzy inference system ({FIS}) for nitrate concentration determination. The limiting cathodic current from the cyclic voltammetry, along with the sample' {pH} and mediator concentration were considered as the input variables of the {FIS}, whilst nitrate concentration in the sample was considered as the output variable. In order to design the {FIS}, fuzzy rules were defined by an expert considering the nature of the problem. Furthermore, the values of the membership function parameters were optimized using a genetic algorithm-based optimization method. The performance of the fuzzy system was acceptable for nitrate concentration prediction since the normalized R-2 and {MSE} of the prediction in test patterns were 0.95 and 0.005, respectively. Although the {FIS} model has been used in an intelligent nitrate biosensor in this study, the proposed model can be used in a wide range of environmental, agricultural and food biosensors. An open source version of the software in {MATLAB} programming environment is available at www.plba.ir/ publications.html.},
	pages = {55--63},
	journaltitle = {{CHEMOMETRICS} {AND} {IN}℡{LIGENT} {LABORATORY} {SYSTEMS}},
	author = {Vakilian, Keyvan Asefpour and Massah, Jafar},
	date = {2018-06-15},
	keywords = {xno}
}

@article{gosain_investigating_2018,
	title = {Investigating structural metrics for understandability prediction of data warehouse multidimensional schemas using machine learning techniques},
	volume = {14},
	issn = {1614-5046},
	doi = {10.1007/s11334-017-0308-z},
	abstract = {Data warehouse ({DW}) quality metrics help in evaluating quality attributes and building classification models for predicting multidimensional ({MD}) schemas as understandable/non-understandable, thereby assisting in {DW} maintenance. To evaluate {DW} {MD} schema quality, we have earlier proposed a set of metrics based on some important aspects of dimension hierarchies and its sharing (like sharing of few hierarchy levels within a dimension; sharing of few hierarchy levels between dimensions, within and across facts) which may lead to structural complexity of {MD} schemas, thereby affecting its quality. The preliminary empirical validation of these metrics using classical statistical techniques (correlation and linear regression) indicated some of them as possible understandability indicators. However, machine learning ({ML}) techniques can model the complex associations between {DW} structural metrics and their quality attributes in a better way. Therefore, this work employs five {ML} classifiers [J48, partial decision trees ({PART}), Naive Bayes, support vector machines ({SVM}) and logistic regression] to empirically investigate whether accurate prediction models can be built, based on our structural metrics, to be used as understandability predictors. The obtained results reveal that four of our metrics are good predictors of understandability of {DW} {MD} schemas. The experimentation further involved comparing the classifiers using mainly five performance measures: accuracy, precision, sensitivity, specificity and area under the receiver operating characteristic curve. The study confirmed the predictive capability of {ML} techniques for understandability prediction of {DW} {MD} schemas. The results also suggest that the {SVM} and Na {\textless} ve Bayes classifiers perform better than other classifiers included in the study. Further, the typically used logistic regression technique gave results that were reasonably competitive with the more sophisticated techniques. However, the tree-based (J48) and rule-based ({PART}) techniques performed significantly worse than the best performing techniques.},
	pages = {59--80},
	number = {1},
	journaltitle = {{INNOVATIONS} {IN} {SYSTEMS} {AND} {SOFTWARE} {ENGINEERING}},
	author = {Gosain, Anjana and Singh, Jaspreeti},
	date = {2018-03},
	keywords = {xno}
}

@article{aydilek_analyzing_2018,
	title = {Analyzing and improving information gain of metrics used in software defect prediction in decision trees},
	volume = {24},
	issn = {1300-7009},
	doi = {10.5505/pajes.2018.93584},
	abstract = {{McCabe} and Halstead method-level metrics are among the well-known and widely used quantitative software metrics are used to measure software quality in a concrete way. Software defect prediction can guess which or which of the sub-modules in the software to be developed may be more prone to defect Thus, loss of labor and time can be avoided. The datasets which are used for software defect prediction, usually have an unbalanced class distribution, since the number of records with defective class can be fewer than the number of records with not defective class and this situation adversely affect the results of the machine learning methods. Information gain is employed in decision trees and decision tree based rule classifier and attribute selection methods. In this study, software metrics that provide important information for software defect prediction have been investigated and {CM}1, {JM}1, {KC}1 and {PC}1 datasets of {NASA}'s {PROMISE} software repository have been balanced with the synthetic data over-sampling Smote algorithm and improved in terms of information gain. As a result, the software defect prediction datasets with higher classification success performance and the software metrics with increased information gain ratio are obtained in the decision trees.},
	pages = {906--914},
	number = {5},
	journaltitle = {{PAMUKKALE} {UNIVERSITY} {JOURNAL} {OF} {ENGINEERING} {SCIENCES}-{PAMUKKALE} {UNIVERSITESI} {MUHENDISLIK} {BILIMLERI} {DERGISI}},
	author = {Aydilek, Ibrahim Berkan},
	date = {2018},
	keywords = {xyes}
}

@article{kumar_fuzzy_2018,
	title = {Fuzzy Inference System Based Distance Estimation Approach for Multi Location and Transforming Phase to Ground Faults in Six Phase Transmission Line},
	volume = {11},
	issn = {1875-6891},
	abstract = {The faults occurring in different phases at multiple locations and different times are difficult to locate exact location using conventional techniques. This paper develops a fault location estimation approach using fuzzy inference system for multi location phase to ground faults and transforming phase to ground faults in six phase transmission ({SPT}) line. The six phase current data of {SPT} line are generated by {MATLAB} software and processed through butter worth filter, sampling and discrete fourier transform for distance locator. The proposed technique is dependent on single terminal data only. Mamdani fuzzy inference system is employed to make decision. Triangular membership functions are used to design input-output fuzzy sets. Fuzzy inference system has been deployed for the fault distance location using {IF}-{THEN} rules. The proposed technique is validated during different multi location and transforming phase to ground faults with wide variations in fault resistance and fault inception angle. Simulations and calculations with {MATLAB} prove that the fault location method is correct and accurate.},
	pages = {757--769},
	number = {1},
	journaltitle = {{INTERNATIONAL} {JOURNAL} {OF} {COMPUTATIONAL} {IN}℡{LIGENCE} {SYSTEMS}},
	author = {Kumar, A. Naresh and Chakravarthy, M.},
	date = {2018}
}

@article{bhushan_improving_2016,
	title = {Improving software product line using an ontological approach},
	volume = {41},
	issn = {0256-2499},
	doi = {10.1007/s12046-016-0571-y},
	abstract = {Software product line ({SPL}) is an emergent strategy for generating software products. The variability and commonality of {SPL} is illustrated by feature models ({FMs}). The quality of software products relies on the correctness of {SPL}. The overall benefits of software product line engineering ({SPLE}) are reduced by various kinds of defects such as dead features and false optional features in an {FM}. These defects can be inherited in the software products built from a defective product line model ({PLM}). In this paper, the problem of enhancing the quality of software products derived from {SPLE} is handled. An ontological based approach is proposed following first-order logic ({FOL}) rules to identify defects namely dead features and false optional features. The classification of cases for these defects in {FMs} that represent variability of {SPL} is defined. The presented approach has been explained with the help of an {FM} derived from the standard case in product line ({PL}) community. The initial empirical evaluation of the proposed approach analyses 35 {FMs} with different sizes. The results obtained exhibit that the proposed approach is accurate, effective, scalable up to 200 features and therefore improves {SPL}.},
	pages = {1381--1391},
	number = {12},
	journaltitle = {{SADHANA}-{ACADEMY} {PROCEEDINGS} {IN} {ENGINEERING} {SCIENCES}},
	author = {Bhushan, Megha and Goel, Shivani},
	date = {2016-12},
	keywords = {xno}
}

@inproceedings{alrajeh_logic-based_2016,
	title = {Logic-based Learning in Software Engineering},
	isbn = {978-1-4503-4205-6},
	doi = {10.1145/2889160.2891050},
	abstract = {In recent years, research efforts have been directed towards the use of Machine Learning ({ML}) techniques to support and automate activities such as program repair, specification mining and risk assessment. The focus has largely been on techniques for classification, clustering and regression. Although beneficial, these do not produce a declarative, interpretable representation of the learned information. Hence, they cannot readily be used to inform, revise and elaborate software models. On the other hand, recent advances in {ML} have witnessed the emergence of new logic-based learning approaches that differ from traditional {ML} in that their output is represented in a declarative, rule-based manner, making them well-suited for many software engineering tasks. In this technical briefing, we will introduce the audience to the latest advances in logic-based learning, give an overview of how logic-based learning systems can successfully provide automated support to a variety of software engineering tasks, demonstrate the application to two real case studies from the domain of requirements engineering and software design and highlight future challenges and directions.},
	pages = {892--893},
	booktitle = {2016 {IEEE}/{ACM} 38TH {INTERNATIONAL} {CONFERENCE} {ON} {SOFTWARE} {ENGINEERING} {COMPANION} ({ICSE}-C)},
	publisher = {{IEEE}; Assoc Comp Machinery; {IEEE} Comp Soc; {IEEE} Tech Council Software Engn; Special Interest Grp Software Engn},
	author = {Alrajeh, Dalal and Russo, Alessandra and Uchitel, Sebastian and Kramer, Jeff},
	date = {2016},
	keywords = {xyes}
}

@article{ma_exploration_2016,
	title = {Exploration of the Reliability of Automotive Electronic Power Steering System Using Device Junction Electrothermal Profile Cycle},
	volume = {4},
	issn = {2169-3536},
	abstract = {This paper focuses on the driving steering load cycle stress-strain, shake, and transient temperature profiles in relation to failure in an automotive electric power steering ({EPS}) system. A compact {EPS} control system model was applied to construct a lookup table for the device power loss calculations. The lookup table was used to obtain the full electrothermal profile of a vehicle {EPS} system converter. An accurate thermal model was introduced using {FLOTHERM} software and compared with the results from {EPS} device temperature testing. The transient junction temperature profiles of the device were studied based on the operational cycles of the vehicle {EPS} system. Finally, an efficient rainflow cycle counting method was introduced to obtain the statistic random transient thermal stress cycles. Miner's rule was also explored, and the relationship between the accumulated thermal stress damage and material fatigue was used to predict the converter's reliability.},
	pages = {7054--7062},
	journaltitle = {{IEEE} {ACCESS}},
	author = {Ma, Xinlei and Guo, Yongfeng and Wang, Li and Ji, Wenqiang},
	date = {2016},
	keywords = {xno}
}

@inproceedings{petric_jinx_2016,
	title = {The Jinx on the {NASA} Software Defect Data Sets},
	isbn = {978-1-4503-3691-8},
	doi = {10.1145/2915970.2916007},
	abstract = {Background: The {NASA} datasets have previously been used extensively in studies of software defects. In 2013 Shepperd et al. presented an essential set of rules for removing erroneous data from the {NASA} datasets making this data more reliable to use. Objective: We have now found additional rules necessary for removing problematic data which were not identified by Shepperd et al. Results: In this paper, we demonstrate the level of erroneous data still present even after cleaning using Shepperd et al.'s rules and apply our new rules to remove this erroneous data. Conclusion: Even after systematic data cleaning of the {NASA} {MDP} datasets, we found new erroneous data. Data quality should always be explicitly considered by researchers before use.},
	booktitle = {{PROCEEDINGS} {OF} {THE} 20TH {INTERNATIONAL} {CONFERENCE} {ON} {EVALUATION} {AND} {ASSESSMENT} {IN} {SOFTWARE} {ENGINEERING} 2016 ({EASE} `16)},
	author = {Petric, Jean and Bowes, David and Hall, Tracy and Christianson, Bruce and Baddoo, Nathan},
	date = {2016},
	keywords = {Defects, {NASA}, Software defect prediction, Software defects, Learning systems, Software engineering, Artificial intelligence, Aluminum, Data cleaning, Data quality, Erroneous datum, Set of rules, xno},
	file = {Accepted Version:C\:\\Users\\michalm\\Zotero\\storage\\WI7QTY7P\\Petrić et al. - 2016 - The jinx on the NASA software defect data sets.pdf:application/pdf}
}

@inproceedings{yadav_construction_2015,
	title = {Construction of Membership Function for Software Metrics},
	volume = {46},
	doi = {10.1016/j.procs.2015.01.002},
	series = {Procedia Computer Science},
	abstract = {A software developer has to deal with a lot of challenging requirements such as cost prediction, defect prediction, reliability prediction, testing effort prediction, safety prediction, and many more while developing quality software. However, it has been found that the most of the software development activity is performed by human beings. This may introduce various faults across the development, causing failures in near future. Therefore, prediction of software defect has been one of the major areas of concern. A number of the software defect prediction model using software metrics has been proposed in last two decades. However, predicting software defect by taking all the software metrics (traditional, object oriented and process) is computationally complex. Therefore, an intelligent selection of metrics plays a vital role in improving the software quality. In the early phases of the software development life cycle, software metrics are associated with uncertainty and can be assessed in linguistic terms. Construction of membership function is very important because the success of a method depends on the membership functions used. Therefore, in this paper, a methodology has been proposed to construct the membership functions of software metrics. (C) 2015 The Authors. Published by Elsevier B.V.},
	pages = {933--940},
	booktitle = {{PROCEEDINGS} {OF} {THE} {INTERNATIONAL} {CONFERENCE} {ON} {INFORMATION} {AND} {COMMUNICATION} {TECHNOLOGIES}, {ICICT} 2014},
	publisher = {Cochin Uni Sci \& Technol, Sch Engn; {TEQIP} Phase {II}},
	author = {Yadav, Harikesh Bahadur and Yadav, Dilip Kumar},
	editor = {{Samuel, P}},
	date = {2015},
	note = {{ISSN}: 1877-0509},
	keywords = {xno}
}

@article{sahin_code-smell_2014,
	title = {Code-Smell Detection as a Bilevel Problem},
	volume = {24},
	issn = {1049-331X},
	doi = {10.1145/2675067},
	abstract = {Code smells represent design situations that can affect the maintenance and evolution of software. They make the system difficult to evolve. Code smells are detected, in general, using quality metrics that represent some symptoms. However, the selection of suitable quality metrics is challenging due to the absence of consensus in identifying some code smells based on a set of symptoms and also the high calibration effort in determining manually the threshold value for each metric. In this article, we propose treating the generation of code-smell detection rules as a bilevel optimization problem. Bilevel optimization problems represent a class of challenging optimization problems, which contain two levels of optimization tasks. In these problems, only the optimal solutions to the lower-level problem become possible feasible candidates to the upper-level problem. In this sense, the code-smell detection problem can be treated as a bilevel optimization problem, but due to lack of suitable solution techniques, it has been attempted to be solved as a single-level optimization problem in the past. In our adaptation here, the upper-level problem generates a set of detection rules, a combination of quality metrics, which maximizes the coverage of the base of code-smell examples and artificial code smells generated by the lower level. The lower level maximizes the number of generated artificial code smells that cannot be detected by the rules produced by the upper level. The main advantage of our bilevel formulation is that the generation of detection rules is not limited to some code-smell examples identified manually by developers that are difficult to collect, but it allows the prediction of new code-smell behavior that is different from those of the base of examples. The statistical analysis of our experiments over 31 runs on nine open-source systems and one industrial project shows that seven types of code smells were detected with an average of more than 86\% in terms of precision and recall. The results confirm the outperformance of our bilevel proposal compared to state-of-art code-smell detection techniques. The evaluation performed by software engineers also confirms the relevance of detected code smells to improve the quality of software systems.},
	number = {1},
	journaltitle = {{ACM} {TRANSACTIONS} {ON} {SOFTWARE} {ENGINEERING} {AND} {METHODOLOGY}},
	author = {Sahin, Dilan and Kessentini, Marouane and Bechikh, Slim and Deb, Kalyanmoy},
	date = {2014-09},
	keywords = {xyes}
}

@article{huang_links_2014,
	title = {The links between human error diversity and software diversity: Implications for fault diversity seeking},
	volume = {89},
	issn = {0167-6423},
	doi = {10.1016/j.scico.2014.03.004},
	abstract = {Software diversity is known to improve fault tolerance in N-version software systems by independent development. As the leading cause of software faults, human error is considered an important factor in diversity seeking. However, there is little scientific research focusing on how to seek software fault diversity based on human error mechanisms. A literature review was conducted to extract factors that may differentiate people with respect to human error-proneness. In addition, we constructed a conceptual model of the links between human error diversity and software diversity. An experiment was designed to validate the hypotheses, in the form of a programming contest, accompanied by a survey of cognitive styles and personality traits. One hundred ninety-two programs were submitted for the identical problem, and 70 surveys were collected. Code inspection revealed 23 faults, of which 10 were coincident faults. The results show that personality traits seems not effective predictors for fault diversity as a whole model, whereas cognitive styles and program measurements moderately account for the variation of fault density. The results also show causal relations between performance levels and coincident faults; coincident faults are unlikely to occur at skill-based performance level; the coincident faults introduced in rule-based performances show a high probability of occurrence, and the coincident faults introduced in knowledge-based performances are shaped by the content and formats of the task itself. Based on these results, we have proposed a model to seek software diversity and prevent coincident faults. (C) 2014 Elsevier B.V. All rights reserved.},
	pages = {350--373},
	issue = {C},
	journaltitle = {{SCIENCE} {OF} {COMPUTER} {PROGRAMMING}},
	author = {Huang, Fuqun and Liu, Bin and Song, You and Keyal, Shreya},
	date = {2014-09-01},
	keywords = {Surveys, Computer software, Knowledge based systems, Cognitive styles, Errors, Human errors, N version programming, Personality traits, Software diversity, N-version programming, Cognitive style, Human error, Personality trait, xno}
}

@article{czibula_software_2014,
	title = {Software defect prediction using relational association rule mining},
	volume = {264},
	issn = {0020-0255},
	doi = {10.1016/j.ins.2013.12.031},
	abstract = {This paper focuses on the problem of defect prediction, a problem of major importance during software maintenance and evolution. It is essential for software developers to identify defective software modules in order to continuously improve the quality of a software system. As the conditions for a software module to have defects are hard to identify, machine learning based classification models are still developed to approach the problem of defect prediction. We propose a novel classification model based on relational association rules mining. Relational association rules are an extension of ordinal association rules, which are a particular type of association rules that describe numerical orderings between attributes that commonly occur over a dataset. Our classifier is based on the discovery of relational association rules for predicting whether a software module is or it is not defective. An experimental evaluation of the proposed model on the open source {NASA} datasets, as well as a comparison to similar existing approaches is provided. The obtained results show that our classifier overperforms, for most of the considered evaluation measures, the existing machine learning based techniques for defect prediction. This confirms the potential of our proposal. (C) 2014 Elsevier Inc. All rights reserved.},
	pages = {260--278},
	journaltitle = {{INFORMATION} {SCIENCES}},
	author = {Czibula, Gabriela and Marian, Zsuzsanna and Czibula, Istvan Gergely},
	date = {2014-04-20},
	keywords = {xyes}
}

@article{khoshgoftaar_software_2014,
	title = {Software quality assessment using a multi-strategy classifier},
	volume = {259},
	issn = {0020-0255},
	doi = {10.1016/j.ins.2010.11.028},
	abstract = {Classifying program Modules as fault-prone or not fault-prone is a valuable technique for guiding the software development process, so that resources can be allocated to components most likely to have faults. The rule-based classification and the case-based learning techniques are commonly used in software quality classification problems. However, studies show that these two techniques share some complementary strengths and weaknesses. Therefore, in this paper we propose a new multi-strategy classification model, {RB}2CBL, which integrates a rule-based ({RB}) model with two case-based learning ({CBL}) models. {RB}2CBL possesses the merits of both the {RB} model and {CBL} model and restrains their drawbacks. In the {RB}2CBL model, the parameter optimization of the {CBL} models is critical and an embedded genetic algorithm optimizer is used. Two case studies were carried out to validate the proposed method. The results show that, by suitably choosing the accuracy of the {RB} model, the {RB}2CBL model outperforms the {RB} model alone without overfitting. (C) 2010 Elsevier Inc. All rights reserved.},
	pages = {555--570},
	journaltitle = {{INFORMATION} {SCIENCES}},
	author = {Khoshgoftaar, Taghi M. and Xiao, Yudong and Gao, Kehan},
	date = {2014-02-20},
	keywords = {Computer software selection and evaluation, Genetic algorithms, Software engineering, Software Quality, Software development process, Classification models, Rule-based classification, Case based learning, Parameter optimization, Rule-based models, Software quality assessment, xyes}
}

@article{ogburn_finite_2014,
	title = {A finite difference construction of the spheroidal wave functions},
	volume = {185},
	issn = {0010-4655},
	doi = {10.1016/j.cpc.2013.07.024},
	abstract = {A fast and simple finite difference algorithm for computing the spheroidal wave functions is described. The resulting eigenvalues and eigenfunctions for real and complex spheroidal bandwidth parameter, c, agree with those in the literature from four to more than eleven significant figures. The validity of this algorithm in the extreme parameter regime, up to c(2) = 10(14), is demonstrated. Furthermore, the algorithm generates the spheroidal functions for complex order m. The coefficients of the differential equation can be simply modified so that the algorithm may solve any second-order differential equation in Sturm-Liouville form. The prolate spheroidal functions and the spectral concentration problem in relation to band-limited and time-limited signals is discussed. We review the properties of these eigenfunctions in the context of Sturm-Liouville theory and the implications for a finite difference algorithm. A number of new suggestions for data fitting using prolate spheroidal wave functions with a heuristic for optimally choosing the value of c and the number of basis functions are described. Program summary Program title: {SWF}\_8thOrder Catalogue identifier: {AEQE}\_v1\_0. Program summary {URL}: http://cpc.cs.qub.ac.uk/summaries/{AEQE}\_v1\_0.html Program obtainable from: {CPC} Program Library, Queen's University, Belfast, N. Ireland Licensing provisions: Standard {CPC} licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 1081 No. of bytes in distributed program, including test data, etc.: 160,312 Distribution format: tar.gz Programming language: Matlab R2009b. Computer: Designed to run on any computer capable of running Matlab 2009b with at least 2 {GB} of {RAM} in order to handle moderate grid sizes. With an appropriate change of syntax, the program may be easily adapted to Maple, Mathematica, Fortran, {IDL} and any other software capable of performing the diagonalization of large matrices. Operating system: Any operating system which will run Matlab, Mathematica, Fortran or any other language capable of performing largematrix diagonalizations. Has the code been vectorized or parallelized?: Tested with dual core and quad core systems. The algorithm will also work with single core systems. {RAM}: 1372 {MB} total used by Matlab for a grid with 4001 points, 41 {MB} used to store eigenfunctions, grid and spectrum arrays (4001 points). More {RAM} is used for larger grids. For example, with 8000 grid points, 162 {MB} of {RAM} is required to store the eigenfunction and eigenvalue arrays. Classification: 4.3. External routines: The program uses Matlab's internal `eig' routine. Nature of problem: The problem is to construct the angular eigenfunctions of the Laplacian in three dimensional, spheroidal coordinates. These are the prolate, oblate and generalized spheroidal wave functions and to compute the corresponding eigenvalues. Equivalently, the task can be seen as generating the angular functions which arise when solving the Helmholtz wave equation by separation of variables in three dimensional, spheroidal coordinates: [partial derivative(eta)(1 - eta(2))partial derivative eta + lambda(m)(1)(c) - c(2)eta(2) - m(2)/1 - eta(2)] s(1)(m) = 0. This task often arises in the solution of problems with axial symmetry although setting c = 0 restores spherical symmetry. More generally, the coefficient function handles, {CD}1E and {CD}2E, can be redefined by the user to match the coefficients of the 2nd and 1st derivatives of any second order Sturm-Liouville type equation, for example, the harmonics of a tri-axial ellipsoid. Hence this algorithm and manuscript provide a foundation for solving a range of problems that have application beyond the spheroidal problems considered here. Solution method: The method of solution is the `finite difference method'. The spatial grid is discretized into N points, N - 2 of which comprise the `interior grid' and 2 points are the boundary points. The spheroidal differential operator is discretized which arises from separation of variables of the Laplacian in three dimensional, spheroidal coordinates on the interior grid using 8th order finite difference formulas. The boundary conditions for the spheroidal wave functions are implemented implicitly via finite difference operators which relate the boundary points back to the interior points. This is done using sliding off-centered differences at 8th order. The discretization of the Laplace operator and implicit implementation of the boundary conditions leads to a discretized eigenvalue problem. The eigenvectors give the discretized eigenfunctions of the spheroidal differential operator and the eigenvalues give the spectrum of the differential operator. Points at the boundary are reconstructed using forward and backward difference operators. The eigenfunctions are numerically normalized using a 6th order “Boole's Rule” integration procedure. Restrictions: The current version of this algorithm implements the option of both Dirichlet and Neumann boundary conditions, which is chosen by the user by the “{BC}” switch. Mixed boundary conditions can also be implemented by the user, by modifying the boundary condition `{IF}' statements. When solving with a very large concentration parameter, id, vertical bar c vertical bar one must use a large number of grid points which would result in longer computational times and require more {RAM}. Unusual features: This program solves for the angular eigenfunctions of the spheroidal wave equation in three dimensions. Due to the finite difference approach, one is able to solve over non-standard domains such as spheroidal caps or spheroidal belts. The program is capable of solving for the generalized spheroidal wavefunctions with complex geometric parameter c. Given a sufficiently large number of grid points, the program can generate spheroidal eigenfunctions and eigenvalues for extreme parameter regimes, for example, for vertical bar c vertical bar similar to 10(8) with a grid of 20,000 points. Furthermore, the program can generate spheroidal wavefunctions for non-integer and complex order parameter, m, which may correspond to some analytic continuation of the spheroidal wave functions. In particular, for real, non-integer values of m, this corresponds to an axially symmetric ellipsoid with a defect angle where the 2 pi periodicity symmetry about the rotation axis becomes 27 pi alpha periodicity, where a is some defect factor. Additional comments: Main User-Input Parameters: The main input parameters are located at the top of the code. The parameter C2 sets the value of the square of the spheroidal concentration parameter, c(2). The parameter m is the order parameter which is typically an integer such as for the Legendre functions, but can also be non-integer and complex valued. The switch {BC}, sets the boundary conditions for the differential equation. A value {BC} = 0 gives Dirichlet boundary conditions and {BC} = 1 gives Neumann boundary conditions. The values of {MinTheta} and {MaxTheta} set the minimum and maximum values of the angular coordinate, theta, which specify the domain of the differential equation. The parameter Npts sets the number of grid points, with larger grids resulting in more accurate eigenfunctions and spectra. General comments: To obtain spheroidal harmonics the eigenfunctions need to be multiplied by a complex exponential factor e(im phi) or sinusoidal functions for real solutions. The eigenfunctions generated by this program may be normalized numerically. Since normalization conventions differ by application, normalization is left for the user to implement. For this purpose, we have encoded a plain, 6th order Boole rule unit normalization which is easy to modify. Finally, we note that if the user modifies the function handles, {CD}1E and {CD}2E then the program will solve any 2nd order ordinary differential equation with Dirichlet or Neumann boundary conditions. Non-homogeneous terms can be added by modifying the entries in the finite difference matrix where c(2) and m(2) appear. Running time: On a laptop with an Intel Core i3-2350M {CPU} (2.30 {GHz}), with 4.00 {GB} of {RAM}, the program takes 149 s for a grid with 4000 points. Running time increases for larger grid sizes. For example, increasing the grid size to 8000 points increased the run time to 1138 s on the same machine. Crown Copyright (C) 2013 Published by Elsevier B.V. All rights reserved.},
	pages = {244--253},
	number = {1},
	journaltitle = {{COMPUTER} {PHYSICS} {COMMUNICATIONS}},
	author = {Ogburn, Daniel X. and Waters, Colin L. and Sciffer, Murray D. and Hogan, Jeff A. and Abbott, Paul C.},
	date = {2014-01},
	keywords = {xno}
}

@inproceedings{xiao_titan_2014,
	title = {Titan: A Toolset That Connects Software Architecture with Quality Analysis},
	isbn = {978-1-4503-3056-5},
	doi = {10.1145/2635868.2661677},
	abstract = {In this tool demo, we will illustrate our tool-Titan-that supports a new architecture model: design rule spaces ({DR}-Spaces). We will show how Titan can capture both architecture and evolutionary structure and help to bridge the gap between architecture and defect prediction. We will demo how to use our toolset to capture hundreds of buggy files into just a few architecturally related groups, and to reveal architecture issues that contribute to the error-proneness and change-proneness of these groups. Our tool has been used to analyze dozens of large-scale industrial projects, and has demonstrated its ability to provide valuable direction on which parts of the architecture are problematic, and on why, when, and how to refactor. The video demo of Titan can be found at https://art.cs.drexel.edu/(similar to)lx52/titan.mp4},
	pages = {763--766},
	booktitle = {22ND {ACM} {SIGSOFT} {INTERNATIONAL} {SYMPOSIUM} {ON} {THE} {FOUNDATIONS} {OF} {SOFTWARE} {ENGINEERING} ({FSE} 2014)},
	publisher = {Assoc Comp Machinery Special Interest Grp Software Engn; {CVIC} {SE}; {NSF}; Microsoft Res; Huawei; Neusoft; Siemens; Yonyou; Hong Kong Univ Sci \& Technol; Google; Radica; Samsung Res Amer; {IBM} Res; {TCL}; {CCC}},
	author = {Xiao, Lu and Cai, Yuanfang and Kazman, Rick},
	date = {2014},
	keywords = {Computer software selection and evaluation, Industrial projects, Software engineering, Computer software maintenance, Software Quality, Defect prediction, Software architecture, Design rules, Architecture modeling, Change proneness, Error proneness, Toolsets, xno}
}

@article{park_design_2013,
	title = {The design of polynomial function-based neural network predictors for detection of software defects},
	volume = {229},
	issn = {0020-0255},
	doi = {10.1016/j.ins.2011.01.026},
	abstract = {In this study, we introduce a design methodology of polynomial function-based Neural Network (pf-{NN}) classifiers (predictors). The essential design components include Fuzzy C-Means ({FCM}) regarded as a generic clustering algorithm and polynomials providing all required nonlinear capabilities of the model. The learning method uses a weighted cost function (objective function) while to analyze the performance of the system we engage a standard receiver operating characteristics ({ROC}) analysis. The proposed networks are used to detect software defects. From the conceptual standpoint, the classifier of this form can be expressed as a collection of “if-then” rules. Fuzzy clustering (Fuzzy C-Means, {FCM}) is aimed at the development of premise layer of the rules while the corresponding consequences of the rules are formed by some local polynomials. A detailed learning algorithm for the pf-{NNs} is presented with particular provisions made for dealing with imbalanced classes encountered quite commonly in software quality problems. The use of simple measures such as accuracy of classification becomes questionable. In the assessment of quality of classifiers, we confine ourselves to the use of the area under curve ({AUC}) in the receiver operating characteristics ({ROCs}) analysis. {AUC} comes as a sound classifier metric capturing a tradeoff between the high true positive rate ({TP}) and the low false positive rate ({FP}). The performance of the proposed classifier is contrasted with the results produced by some “standard” Radial Basis Function ({RBF}) neural networks. (C) 2011 Elsevier Inc. All rights reserved.},
	pages = {40--57},
	journaltitle = {{INFORMATION} {SCIENCES}},
	author = {Park, Byoung-Jun and Oh, Sung-Kwun and Pedrycz, Witold},
	date = {2013-04-20},
	keywords = {Clustering algorithms, Computer software selection and evaluation, Defects, Software defects, Receiver operating characteristics, Pattern recognition, Software Quality, False positive rates, Learning algorithms, Imbalanced data, Fuzzy systems, Neural networks, Quality control, Design, Design Component, Design Methodology, Functions, Fuzzy C mean, Fuzzy C-means, Fuzzy clustering, Imbalanced class, Learning methods, Neural network predictor, Objective functions, Radial basis function networks, Radial basis function neural networks, Receiver operating characteristics analysis, True positive rates, Two-class discrimination, Weighted cost functions, xno}
}

@article{yu_experience_2012,
	title = {Experience in Predicting Fault-Prone Software Modules Using Complexity Metrics},
	volume = {9},
	issn = {1684-3703},
	doi = {10.1080/16843703.2012.11673302},
	abstract = {Complexity metrics have been intensively studied in predicting fault-prone software modules. However, little work is done in studying how to effectively use the complexity metrics and the prediction models under realistic conditions. In this paper, we present a study showing how to utilize the prediction models generated from existing projects to improve the fault detection on other projects. The binary logistic regression method is used in studying publicly available data of five commercial products. Our study shows (1) models generated using more datasets can improve the prediction accuracy but not the recall rate; (2) lowering the cut-off value can improve the recall rate, but the number of false positives will be increased, which will result in higher maintenance effort. We further suggest that in order to improve model prediction efficiency, the selection of source datasets and the determination of cut-Off values should be based on specific properties of a project. So far, there are no general rules that have been found and reported to follow},
	pages = {421--433},
	number = {4},
	journaltitle = {{QUALITY} {TECHNOLOGY} {AND} {QUANTITATIVE} {MANAGEMENT}},
	author = {Yu, Liguo and Mishra, Alok},
	date = {2012-12},
	keywords = {xyes}
}

@article{rodriguez_searching_2012,
	title = {Searching for rules to detect defective modules: A subgroup discovery approach},
	volume = {191},
	issn = {0020-0255},
	doi = {10.1016/j.ins.2011.01.039},
	abstract = {Data mining methods in software engineering are becoming increasingly important as they can support several aspects of the software development life-cycle such as quality. In this work, we present a data mining approach to induce rules extracted from static software metrics characterising fault-prone modules. Due to the special characteristics of the defect prediction data (imbalanced, inconsistency, redundancy) not all classification algorithms are capable of dealing with this task conveniently. To deal with these problems, Subgroup Discovery ({SD}) algorithms can be used to find groups of statistically different data given a property of interest. We propose {EDER}-{SD} (Evolutionary Decision Rules for Subgroup Discovery), a {SD} algorithm based on evolutionary computation that induces rules describing only fault-prone modules. The rules are a well-known model representation that can be easily understood and applied by project managers and quality engineers. Thus, rules can help them to develop software systems that can be justifiably trusted. Contrary to other approaches in {SD}, our algorithm has the advantage of working with continuous variables as the conditions of the rules are defined using intervals. We describe the rules obtained by applying our algorithm to seven publicly available datasets from the {PROMISE} repository showing that they are capable of characterising subgroups of fault-prone modules. We also compare our results with three other well known {SD} algorithms and the {EDER}-{SD} algorithm performs well in most cases. (C) 2011 Elsevier Inc. All rights reserved.},
	pages = {14--30},
	journaltitle = {{INFORMATION} {SCIENCES}},
	author = {Rodriguez, D. and Ruiz, R. and Riquelme, J. C. and Aguilar-Ruiz, J. S.},
	date = {2012-05-15},
	keywords = {Data mining, Software metrics, Defects, Software engineering, Project management, Defect prediction, Classification algorithm, Software systems, Evolutionary algorithms, Imbalanced Data-sets, Rules, Fault-prone modules, Subgroup discovery, Data sets, Continuous variables, Data mining methods, Decision rules, Model representation, Project managers, Quality engineers, Software development life cycle, xyes, xbest},
	file = {Submitted Version:C\:\\Users\\michalm\\Zotero\\storage\\BQZNCWEN\\Rodríguez et al. - 2012 - Searching for rules to detect defective modules A.pdf:application/pdf}
}

@inproceedings{si_research_2012,
	title = {Research on {ODC} based Software Defect Analysis Model},
	volume = {457-458},
	isbn = {978-3-03785-355-9},
	doi = {10.4028/www.scientific.net/AMR.457-458.488},
	series = {Advanced Materials Research},
	abstract = {In this paper we discuss the fundamental principle of defect analysis, introduce the basic framework of Orthogonal Defect Classification({ODC}), and summarize the principle and features of defect analysis based on {ODC}. Using software defect metric, defect baseline and inference rules to do software process analysis, we propose a novel defect analysis model based on {ODC}. Combined with practical projects, the principle of this model is discussed in details, and this model can improve the objectivity and effectiveness of defect analysis, which is favorable for the automated defect analysis.},
	pages = {488+},
	booktitle = {{ADVANCED} {MATERIALS} {AND} {ENGINEERING} {MATERIALS}, {PTS} 1 {AND} 2},
	author = {Si, Qianran and Yan, Guoying and Zhang, Huiying},
	editor = {{Gao, S}},
	date = {2012},
	note = {{ISSN}: 1022-6680},
	keywords = {xno}
}

@inproceedings{kuo_using_2011,
	title = {{USING} {CLUSTERING} {TECHNIQUES} {GOES} {WITH} {GENETIC} {ALGORITHM} {TO} {IMPROVE} {SOFTWARE} {DEFECT} {PREDICTION}},
	isbn = {978-0-7918-5979-7},
	abstract = {Software defect prevention is an important activity to improve software quality, in which the possible defects can be predicted and avoided in advance. Identifying the actions that may cause defect before execution is difficult. To settle this problem, this study proposes a defect prediction approach, in which the data of defects and actions are collected to construct the prediction models. The proposed approach is adapted from Association Rule based Defect Prediction ({ARDP}), and applies Density-Based clustering algorithm with Genetic algorithm ({CBDPGA}) on the collected data. The improvements of the proposed approach include decreasing the impact of outlier to defect prediction models, and increasing the performance of prediction model. To demonstrate the performance of this study, this approach is applied on the same dataset used in Association Rule based Defect Prediction ({ARDP}) approach. The results show that the proposed approach can be used to improve prediction performance.},
	pages = {165--170},
	booktitle = {{PROCEEDINGS} {OF} {THE} 2011 3RD {INTERNATIONAL} {CONFERENCE} {ON} {SOFTWARE} {TECHNOLOGY} {AND} {ENGINEERING} ({ICSTE} 2011)},
	publisher = {Univ Putra Malaysia; Polytechn Univ Puerto Rico; Chengdu Young Educ \& Consultancy; {IACSIT}},
	author = {Kuo, Chia-Hao and Chang, Ching-Pao and Lin, Yu-Shih and Chu, Chih-Ping},
	date = {2011}
}

@article{gay_automatically_2010,
	title = {Automatically finding the control variables for complex system behavior},
	volume = {17},
	issn = {0928-8910},
	doi = {10.1007/s10515-010-0072-x},
	abstract = {Testing large-scale systems is expensive in terms of both time and money. Running simulations early in the process is a proven method of finding the design faults likely to lead to critical system failures, but determining the exact cause of those errors is still time-consuming and requires access to a limited number of domain experts. It is desirable to find an automated method that explores the large number of combinations and is able to isolate likely fault points. Treatment learning is a subset of minimal contrast-set learning that, rather than classifying data into distinct categories, focuses on finding the unique factors that lead to a particular classification. That is, they find the smallest change to the data that causes the largest change in the class distribution. These treatments, when imposed, are able to identify the factors most likely to cause a mission-critical failure. The goal of this research is to comparatively assess treatment learning against state-of-the-art numerical optimization techniques. To achieve this, this paper benchmarks the {TAR}3 and {TAR}4.1 treatment learners against optimization techniques across three complex systems, including two projects from the Robust Software Engineering ({RSE}) group within the National Aeronautics and Space Administration ({NASA}) Ames Research Center. The results clearly show that treatment learning is both faster and more accurate than traditional optimization methods.},
	pages = {439--468},
	number = {4},
	journaltitle = {{AUTOMATED} {SOFTWARE} {ENGINEERING}},
	author = {Gay, Gregory and Menzies, Tim and Davies, Misty and Gundy-Burlet, Karen},
	date = {2010-12},
	keywords = {xno}
}

@article{aldemir_probabilistic_2010,
	title = {Probabilistic risk assessment modeling of digital instrumentation and control systems using two dynamic methodologies},
	volume = {95},
	issn = {0951-8320},
	doi = {10.1016/j.ress.2010.04.011},
	abstract = {The Markov/cell-to-cell mapping technique ({CCMT}) and the dynamic flowgraph methodology ({DFM}) are two system logic modeling methodologies that have been proposed to address the dynamic characteristics of digital instrumentation and control (I\&C) systems and provide risk-analytical capabilities that supplement those provided by traditional probabilistic risk assessment ({PRA}) techniques for nuclear power plants. Both methodologies utilize a discrete state, multi-valued logic representation of the digital I\&C system. For probabilistic quantification purposes, both techniques require the estimation of the probabilities of basic system failure modes, including digital I\&C software failure modes, that appear in the prime implicants identified as contributors to a given system event of interest. As in any other system modeling process, the accuracy and predictive value of the models produced by the two techniques, depend not only on the intrinsic features of the modeling paradigm, but also and to a considerable extent on information and knowledge available to the analyst, concerning the system behavior and operation rules under normal and off-nominal conditions, and the associated controlled/monitored process dynamics. The application of the two methodologies is illustrated using a digital feedwater control system ({DFWCS}) similar to that of an operating pressurized water reactor. This application was carried out to demonstrate how the use of either technique, or both, can facilitate the updating of an existing nuclear power plant {PRA} model following an upgrade of the instrumentation and control system from analog to digital. Because of scope limitations, the focus of the demonstration of the methodologies was intentionally limited to aspects of digital I\&C system behavior for which probabilistic data was on hand or could be generated within the existing project bounds of time and resources. The data used in the probabilistic quantification portion of the process were gathered partially from fault injection experiments with the {DFWCS}, separately conducted under conservative assumptions, partially from operating experience, and partially from generic data bases. The purpose of the quantification portion of the process was, purely to demonstrate the {PRA}-updating use and application of the methodologies, without making any particular claim regarding the specific validity and predictive value of the data utilized to illustrate the quantitative risk calculations produced from the qualitative information analytically generated by the models. A comparison of the results obtained from the Markov/{CCMT} and {DFM} regarding the event sequences leading to {DFWCS} failure modes show qualitative and quantitative consistency for the risk scenarios and sequences under consideration. The study also shows that: (a) the risk significance of the timing of system component failures may depend on factors that include the actual variability of initiating conditions of a dynamic transient, even within the nominal control range and (b) the range of dynamic outcomes may also be dependent on the choice of the assumed basic system-component failure modes included in the models, regardless of whether some of these would or would not be considered to have direct safety implications according to the traditional safety/non-safety equipment classifications. (C) 2010 Elsevier Ltd. All rights reserved.},
	pages = {1011--1039},
	number = {10},
	journaltitle = {{RELIABILITY} {ENGINEERING} \& {SYSTEM} {SAFETY}},
	author = {Aldemir, T. and Guarro, S. and Mandelli, D. and Kirschenbaum, J. and Mangan, L. A. and Bucci, P. and Yau, M. and Ekici, E. and Miller, D. W. and Sun, X. and Arndt, S. A.},
	date = {2010-10},
	keywords = {xno}
}

@article{de_carvalho_symbolic_2010,
	title = {A symbolic fault-prediction model based on multiobjective particle swarm optimization},
	volume = {83},
	issn = {0164-1212},
	doi = {10.1016/j.jss.2009.12.023},
	abstract = {In the literature the fault-proneness of classes or methods has been used to devise strategies for reducing testing costs and efforts. In general, fault-proneness is predicted through a set of design metrics and, most recently, by using Machine Learning ({ML}) techniques. However, some {ML} techniques cannot deal with unbalanced data, characteristic very common of the fault datasets and, their produced results are not easily interpreted by most programmers and testers. Considering these facts, this paper introduces a novel fault-prediction approach based on Multiobjective Particle Swarm Optimization ({MOPSO}). Exploring Pareto dominance concepts, the approach generates a model composed by rules with specific properties. These rules can be used as an unordered classifier, and because of this, they are more intuitive and comprehensible. Two experiments were accomplished, considering, respectively, fault-proneness of classes and methods. The results show interesting relationships between the studied metrics and fault prediction. In addition to this, the performance of the introduced {MOPSO} approach is compared with other {ML} algorithms by using several measures including the area under the {ROC} curve, which is a relevant criterion to deal with unbalanced data. (C) 2010 Elsevier Inc. All rights reserved.},
	pages = {868--882},
	number = {5},
	journaltitle = {{JOURNAL} {OF} {SYSTEMS} {AND} {SOFTWARE}},
	author = {de Carvalho, Andre B. and Pozo, Aurora and Vergilio, Silvia Regina},
	date = {2010-05},
	keywords = {xno}
}

@article{lessmann_customer-centric_2010,
	title = {Customer-Centric Decision Support A Benchmarking Study of Novel Versus Established Classification Models},
	volume = {2},
	issn = {2363-7005},
	doi = {10.1007/s12599-010-0094-8},
	abstract = {Classification analysis is an important tool to support decision making in customer-centric applications like, e.g., proactively identifying churners or selecting responsive customers for direct-marketing campaigns. Whereas the development of novel classification algorithms is a popular avenue for research, corresponding advancements are rarely adopted in corporate practice. This lack of diffusion may be explained by a high degree of uncertainty regarding the superiority of novel classifiers over well established counterparts in customer-centric settings. To overcome this obstacle, an empirical study is undertaken to assess the ability of several novel as well as traditional classifiers to form accurate predictions and effectively support decision making. The results provide strong evidence for the appropriateness of novel methods and indicate that they offer economic benefits under a variety of conditions. Therefore, an increase in use of respective procedures can be recommended.},
	pages = {79--93},
	number = {2},
	journaltitle = {{BUSINESS} \& {INFORMATION} {SYSTEMS} {ENGINEERING}},
	author = {Lessmann, Stefan and Voss, Stefan},
	date = {2010-04},
	keywords = {xno}
}

@inproceedings{mukherjee_complex_2010,
	title = {Complex Event Processing in Power Distribution Systems: A Case Study},
	isbn = {978-1-936338-02-3},
	abstract = {Complex Event Processing ({CEP}) is an emerging discipline. This paper focuses on the application of {CEP} for fault detection and classification in 11kV radial distribution system using data collected from a Phasor Measurement Unit ({PMU}). The analysis has been done by monitoring the electrical quantities in the 11kV radial distribution system simulated using Matlab Simulink. The {PMU} is placed at the substation and' transmits data to the Command and Control Center. The data is analyzed to identify the signatures of different types of faults and based on that rules have been designed to categorize them. An architecture stack has been designed based on a commercial {CEP} product (Tibco Business Events) to implement the fault detection. In this paper we present the architecture, the 11kV distribution system simulation and share our experience about fault detection. The paper outlines the categorization and analysis of various types of faults like Single Line to Ground ({SLG}) fault, Double Line to Ground ({DLG}) fault, and Three phase (30) fault using {CEP} software. Thus it shows a real life application of {CEP} software for fault classification with low computational time.},
	pages = {55--60},
	booktitle = {{IMETI} 2010: 3RD {INTERNATIONAL} {MULTI}-{CONFERENCE} {ON} {ENGINEERING} {AND} {TECHNOLOGICAL} {INNOVATION}, {VOL} I},
	publisher = {Int Inst Inform \& System},
	author = {Mukherjee, Debnath and Shakya, Deepti and Misra, Prateep},
	editor = {{Callaos, N and Chu, HW and Tremante, A and Zinn, CD}},
	date = {2010},
	keywords = {xno}
}

@article{kaya_rule-based_2010,
	title = {A {RULE}-{BASED} {DOMAIN} {SPECIFIC} {LANGUAGE} {FOR} {FAULT} {MANAGEMENT}},
	volume = {14},
	issn = {1092-0617},
	abstract = {In this article, we propose a domain specific language for the “fault management for mission critical systems” domain that also supports rule-based operation. Variability management for a software product line, hence configuration of specific products will be achieved by programming various nodes in a “Software Factory Automation” based architecture, through this language. The architecture suggests a distributed set of interconnected “Domain Specific Engines” that interpret specific languages. Those engines and their corresponding languages assume different fault management capabilities, therefore suggest the design of different languages for fault monitoring, detection, prevention, diagnosis, and repair. The overall fault management capabilities should be supported by the composition of those languages. A high-level definition of the language is presented. The objective of the study is to provide an infrastructure that is more predictable for the development success of fault management systems for families of complex embedded mission critical applications.},
	pages = {13--23},
	number = {3},
	journaltitle = {{JOURNAL} {OF} {INTEGRATED} {DESIGN} \& {PROCESS} {SCIENCE}},
	author = {Kaya, Ozgur and Togay, Cengiz and Dogru, Ali},
	date = {2010},
	keywords = {xno}
}

@inproceedings{gayatri_feature_2010,
	title = {Feature Selection Using Decision Tree Induction in Class level Metrics Dataset for Software Defect Predictions},
	isbn = {978-988-17012-0-6},
	series = {Lecture Notes in Engineering and Computer Science},
	abstract = {The importance of software testing for quality assurance cannot be over emphasized. The estimation of quality factors is important for minimizing the cost and improving the effectiveness of the software testing process. One of the quality factors is fault proneness, for which unfortunately there is no generalized technique available to effectively identify fault proneness. Many researchers have concentrated on how to select software metrics that are likely to indicate fault proneness. At the same time dimensionality reduction (feature selection of software metrics) also plays a vital role for the effectiveness of the model or best quality model. Feature selection is important for a variety of reasons such as generalization, performance, computational efficiency and feature interpretability. In this paper a new method for feature selection is proposed based on Decision Tree Induction. Relevant features are selected from the class level dataset based on decision tree classifiers used in the classification process. The attributes which form rules for the classifiers are taken as the relevant feature set or new feature set named Decision Tree Induction Rule based ({DTIRB}) feature set. Different classifiers are learned with this new data set obtained by decision tree induction process and achieved better performance. The performance of 18 classifiers is studied with the proposed method. Comparison is made with the Support Vector Machines ({SVM}) and {RELIEF} feature selection techniques. It is observed that the proposed method outperforms the other two for most of the classifiers considered. Overall improvement in classification process is also found with original feature set and reduced feature set. The proposed method has the advantage of easy interpretability and comprehensibility. Class level metrics dataset is used for evaluating the performance of the model. Receiver Operating Characteristics ({ROC}) and Mean Absolute Error ({MAE}) and Root Mean Squared Error ({RMSE}) error measures are used as the performance measures for checking effectiveness of the model.},
	pages = {124--129},
	booktitle = {{WORLD} {CONGRESS} {ON} {ENGINEERING} {AND} {COMPUTER} {SCIENCE}, {VOLS} 1 {AND} 2},
	publisher = {Int Assoc Engineers ({IAENG}); {IAENG} Soc Artificial Intelligence; {IAENG} Soc Bioinformat; {IAENG} Soc Chem Engn; {IAENG} Soc Comp Sci; {IAENG} Soc Data Min; {IAENG} Soc Elect Engn; {IAENG} Soc Imaging Engn; {IAENG} Soc Ind Engn; {IAENG} Soc Internet Comp \& Web Serv; {IAENG} Soc Oper Res; {IAENG} Soc Sci Comp; {IAENG} Soc Software Engn; {IAENG} Soc Wireless Networks; {IAENG} Soc {HIV}/{AIDS}},
	author = {Gayatri, N. and Nickolas, S. and Reddy, A. V.},
	editor = {{Ao, SI and Douglas, C and Grundfest, WS and Burgstone, J}},
	date = {2010},
	note = {{ISSN}: 2078-0958},
	keywords = {xyes}
}

@inproceedings{shao_software_2017,
	title = {Software defect prediction based on class-association rules},
	doi = {10.1109/ICRSE.2017.8030774},
	abstract = {Although there have lots of studies on using static code attributes to identify defective software modules, there still have many challenges. For instance, it is difficult to implement the Apriori-type algorithm to predict defects by learning from an imbalanced dataset. For more accurate and understandable defect prediction, a novel approach based on class-association rules algorithm is proposed. Class-association rules are looked as a separate class label, which is a specific type of association rules that explores the relationship between attributes and categories. In an empirical comparison with four datasets, the novel approach is superior to other four classification techniques and accordingly, proved it's valuable for defect prediction.},
	pages = {1--5},
	booktitle = {2017 Second International Conference on Reliability Systems Engineering ({ICRSE})},
	author = {Shao, Yuanxun and Liu, Bin and Li, Guoqi and Wang, Shihai},
	date = {2017-07},
	keywords = {Apriori, association rule, Classification algorithms, Itemsets, Modeling, Prediction algorithms, prediction performance, rule pruning, Software, Software algorithms, software defect prediction, Training, xassociation-rules, xyes}
}

@inproceedings{monden_heuristic_2012,
	title = {A Heuristic Rule Reduction Approach to Software Fault-proneness Prediction},
	volume = {1},
	doi = {10.1109/APSEC.2012.103},
	abstract = {Background: Association rules are more comprehensive and understandable than fault-prone module predictors (such as logistic regression model, random forest and support vector machine). One of the challenges is that there are usually too many similar rules to be extracted by the rule mining. Aim: This paper proposes a rule reduction technique that can eliminate complex (long) and/or similar rules without sacrificing the prediction performance as much as possible. Method: The notion of the method is to removing long and similar rules unless their confidence level as a heuristic is high enough than shorter rules. For example, it starts with selecting rules with shortest length (length=1), and then it continues through the 2nd shortest rules selection (length=2) based on the current confidence level, this process is repeated on the selection for longer rules until no rules are worth included. Result: An empirical experiment has been conducted with the Mylyn and Eclipse {PDE} datasets. The result of the Mylyn dataset showed the proposed method was able to reduce the number of rules from 1347 down to 13, while the delta of the prediction performance was only. 015 (from. 757 down to. 742) in terms of the F1 prediction criteria. In the experiment with Eclipsed {PDE} dataset, the proposed method reduced the number of rules from 398 to 12, while the prediction performance even improved (from. 426 to. 441.) Conclusion: The novel technique introduced resolves the rule explosion problem in association rule mining for software proneness prediction, which is significant and provides better understanding of the causes of faulty modules.},
	pages = {838--847},
	booktitle = {2012 19th Asia-Pacific Software Engineering Conference},
	author = {Monden, Akito and Keung, Jacky and Morisaki, Shuji and Kamei, Yasutaka and Matsumoto, Ken-Ichi},
	date = {2012-12},
	note = {{ISSN}: 1530-1362},
	keywords = {association rule mining, Association rules, data mining, defect prediction, Educational institutions, empirical study, Explosions, Measurement, Predictive models, Software, software quality, xassociation-rules, xyes},
	file = {Monden et al. - 2012 - A Heuristic Rule Reduction Approach to Software Fa.pdf:C\:\\Users\\michalm\\Zotero\\storage\\5KLR4JGE\\Monden et al. - 2012 - A Heuristic Rule Reduction Approach to Software Fa.pdf:application/pdf}
}

@article{he_ensemble_2019,
	title = {Ensemble {MultiBoost} Based on {RIPPER} Classifier for Prediction of Imbalanced Software Defect Data},
	volume = {7},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2019.2934128},
	abstract = {Identifying defective software entities is essential to ensure software quality during software development. However, the high dimensionality and class distribution imbalance of software defect data seriously affect software defect prediction performance. In order to solve this problem, this paper proposes an Ensemble {MultiBoost} based on {RIPPER} classifier for prediction of imbalanced Software Defect data, called {EMR}\_SD. Firstly, the algorithm uses principal component analysis ({PCA}) method to find out the most effective features from the original features of the data set, so as to achieve the purpose of dimensionality reduction and redundancy removal. Furthermore, the combined sampling method of adaptive synthetic sampling ({ADASYN}) and random sampling without replacement is performed to solve the problem of data class imbalance. This classifier establishes association rules based on attributes and classes, using {MultiBoost} to reduce deviation and variance, so as to achieve the purpose of reducing classification error. The proposed prediction model is evaluated experimentally on the {NASA} {MDP} public datasets and compared with existing similar algorithms. The results show that {EMR}\_SD algorithm is superior to {DNC}, {CEL} and other defect prediction techniques in most evaluation indicators, which proves the effectiveness of the algorithm.},
	pages = {110333--110343},
	journaltitle = {{IEEE} Access},
	author = {He, Haitao and Zhang, Xu and Wang, Qian and Ren, Jiadong and Liu, Jiaxin and Zhao, Xiaolin and Cheng, Yongqiang},
	date = {2019},
	keywords = {Class distributions, class imbalance, Classification algorithms, Classification errors, combined sampling, Computer software selection and evaluation, Defect prediction, Defects, Dimensionality reduction, Evaluation indicators, Feature extraction, Forecasting, High dimensionality, {MultiBoost}, {NASA}, Prediction algorithms, Predictive analytics, Predictive models, Principal component analysis, Redundancy removal, rule learning, Software, Software algorithms, Software defect prediction, Software design, Software entities, Software quality, xyes},
	file = {Full Text:C\:\\Users\\michalm\\Zotero\\storage\\SKE8UCK9\\He et al. - 2019 - Ensemble MultiBoost Based on RIPPER Classifier for.pdf:application/pdf;Full Text:C\:\\Users\\michalm\\Zotero\\storage\\XKMMDBBD\\He et al. - 2019 - Ensemble MultiBoost Based on RIPPER Classifier for.pdf:application/pdf}
}

@inproceedings{gao_empirical_2019,
	title = {Empirical Study: Are Complex Network Features Suitable for Cross-Version Software Defect Prediction?},
	doi = {10.1109/ICSESS47205.2019.9040793},
	abstract = {Software defect prediction can identify possible defective software modules and improve testing efficiency. Traditional software defect prediction mainly focuses on using code features and process-based features for research. The rules of complex network are suitable for software. Using complex network features to represent defect information provides a new idea for software defect prediction. In this paper, we first select 18 versions of 9 open source projects through certain rules and then build a logistic regression model based on three kinds of features (complex network features, traditional code features, merged features) to evaluate the predictive defect ability of complex network features. The results show that: (1) Compared with traditional code features, complex network features have better ability to predict defects for cross-versions software defect prediction; (2) Merged features are not as good as complex network features in defect prediction for cross-version software defect prediction, but still better than traditional code features.},
	pages = {1--5},
	booktitle = {2019 {IEEE} 10th International Conference on Software Engineering and Service Science ({ICSESS})},
	author = {Gao, Houleng and Lu, Minyan and Pan, Cong and Xu, Biao},
	date = {2019-10},
	note = {{ISSN}: 2327-0594},
	keywords = {Feature extraction, Predictive models, Defects, Software defect prediction, Empirical studies, Open source software, Forecasting, Open source projects, Open systems, Software testing, Complex networks, Software modules, Defect prediction, Logistic regression, Logistic Regression modeling, Network features, Testing efficiency, Software systems, Logistics, Software algorithms, complex network features, cross-version software defect prediction, logistic regression, xno}
}

@inproceedings{anezakis_verification_2018,
	title = {Verification of the effectiveness of fuzzy rule-based fault prediction: A replication study},
	doi = {10.1109/INISTA.2018.8466331},
	abstract = {The prediction success of faulty modules in a software helps practitioners to plan the budget of software maintenance that leads developers to improve the reliability of software systems. Despite various learning algorithms and statistical methods, fault prediction needs novel methods for enhancing the success of the prediction. Fault prediction can be performed using fuzzy rules that are new for this field. In this work, fuzzy rule-based fault prediction approach, which was developed by Singh et al. [11], is replicated to validate the success of fuzzy rule-based fault prediction in open-source data sets. The steps of the experiment and the steps of Singh et al's work, which are applied for replication, both are same. Classification is performed after generating clusters that are constituted using fuzzy rules in normalized data sets. According to the prediction results obtained by applying 10*10 cross-validation, fuzzy rule-based fault prediction produces less errors in open-source data sets when it is compared with industrial data sets. In addition to this, the results validate the findings of Singh et al.'s work in terms of some performance parameters of the fault prediction.},
	pages = {1--8},
	booktitle = {2018 Innovations in Intelligent Systems and Applications ({INISTA})},
	author = {Anezakis, Vardis-Dimitris and Öztürk, Muhammed Maruf},
	date = {2018-07},
	keywords = {Feature extraction, Software metrics, {NASA}, Open source software, Forecasting, Classification (of information), Intelligent systems, Budget control, Software reliability, Fault prediction, Measurement, Learning algorithms, Fuzzy inference, Fuzzy rules, Performance parameters, Fault data, Fuzzy rule based, Industrial datum, Open source datum, Replication study, Prediction algorithms, fault data sets, fault prediction, Fuzzy rule, Modulation, modulator learning, software metrics, xyes}
}

@inproceedings{naufal_software_2015,
	title = {Software complexity metric-based defect classification using {FARM} with preprocessing step {CFS} and {SMOTE} a preliminary study},
	doi = {10.1109/ICITSI.2015.7437685},
	abstract = {One criteria for assessing the software quality is ensuring that there is no defect in the software which is being developed. Software defect classification can be used to prevent software defects. More earlier software defects are detected in the software life cycle, it will minimize the software development costs. This study proposes a software defect classification using Fuzzy Association Rule Mining ({FARM}) based on complexity metrics. However, not all complexity metrics affect on software defect, therefore it requires metrics selection process using Correlation-based Feature Selection ({CFS}) so it can increase the classification performance. This study will conduct experiments on the {NASA} {MDP} open source dataset that is publicly accessible on the {PROMISE} repository. This datasets contain history log of software defects based on software complexity metric. In {NASA} {MDP} dataset the data distribution between defective and not defective modules are not balanced. It is called class imbalanced problem. Class imbalance problem can affect on classification performance. It needs a technique to solve this problem using oversampling method. Synthetic Minority Oversampling Technique ({SMOTE}) is used in this study as oversampling method. With the advantages possessed by {FARM} in learning on dataset which has quantitative data attribute and combined with the software complexity metrics selection process using {CFS} and oversampling using {SMOTE}, this method is expected has a better performance than the previous methods.},
	pages = {1--6},
	booktitle = {2015 International Conference on Information Technology Systems and Innovation ({ICITSI})},
	author = {Naufal, Mohammad Farid and Rochimah, Siti},
	date = {2015-11},
	keywords = {Data mining, Feature extraction, Software, Software metrics, Computer software selection and evaluation, Defects, Software design, {NASA}, Software defects, Open source software, Learning systems, Program debugging, Association rules, Artificial intelligence, Fuzzy rules, Life cycle, Classification performance, Faulting, Bugs, Class imbalance problems, Computational complexity, Correlation based feature selections ({CFS}), Fuzzy association rule, Problem solving, Software development costs, Synthetic minority over-sampling techniques, Fault, Training, Complexity theory, Fuzzy Association Rule Mining, Correlation-based Feature Selection, Defect, Machine Learning, Software Defect Classification, Synthetic Minority Oversampling Technique, xno}
}

@inproceedings{singh_comprehensive_2017,
	title = {Comprehensive model for software fault prediction},
	doi = {10.1109/ICICI.2017.8365311},
	abstract = {Software Fault prediction ({SFP}) is an important task in the fields of software engineering to develop a cost effective software. Most of the software fault prediction is performed on same project date i.e., training and testing with same projects fault data. In case of unavailability of fault training data which is possible for the new project, data from the similar types/category of other projects can be used to train the model for the prediction. The software projects has been categorized into three categories by Boehm. The project within a certain group will be having good similarities with other projects within the group. So it is more suitable to train using the projects from same group. In this work we proposed to develop a model with similar category of data to predict the fault of another project belongs to same category. On basis of {KLOC} we have taken five organic software projects and performed various cross project and within project experiments. To generate a comprehensive generalized model for organic software's fault prediction, we have modeled various rule based to learner. Various rule-based learners used for comparison are {JRip}, {CART}, Conjunctive Rule, C4.5, {NNge}, {OneR}, Ridor, {PART}, and decision table-Naive Bayes hybrid classifier ({DTNB}).},
	pages = {1103--1108},
	booktitle = {2017 International Conference on Inventive Computing and Informatics ({ICICI})},
	author = {Singh, Pradeep},
	date = {2017-11},
	keywords = {Predictive models, Software, Testing, Cost engineering, Forecasting, Software testing, Training and testing, Training data, Fault prediction, Comprehensive model, Software fault prediction, Rule based, Cost effectiveness, Decision tables, Generalized models, Hybrid classifier, Three categories, Training, Computational modeling, Data models, Rule based Learner, xyes, xcross-project},
	file = {Singh - 2017 - Comprehensive model for software fault prediction.pdf:C\:\\Users\\michalm\\Zotero\\storage\\5EF5YECT\\Singh - 2017 - Comprehensive model for software fault prediction.pdf:application/pdf}
}

@inproceedings{mutlu_automatic_2018,
	title = {Automatic Rule Generation of Fuzzy Systems: A Comparative Assessment on Software Defect Prediction},
	doi = {10.1109/UBMK.2018.8566479},
	abstract = {Fuzzy rule base systems are expert systems rely on fuzzy set theory. Here the knowledge of human expert is transfered to the artificial model via fuzzy rules. Therefore, preciseness, completeness and coverage of fuzzy rules in a fuzzy system is vital for the accuracy and plausibility of fuzzy reasoning. However, in such cases where the human expert is unable to supply the rules sufficiently, data-based automatic rule generation methods attract attention. In this study, 2 linear and 2 evolutionary approaches of automatic fuzzy rule generation methods are investigated. The investigated linear solutions contain Wang-Mendel Method and E2E-{HFS}, while {MOGUL} and {IVTURS}-{FARC} are the selected evolutionary approaches. Wang-Mendel and {MOGUL} is commonly considered as basic methods of the group they belong to. {IVTURS}-{FARC} is distinguished with its ability to handle interval valued fuzzy sets. Among the rest of the algorithms, E2E-{HFS} is unique with its weak dependency to data. Because it only use some simple properties of corresponding input variable. In order to compare the completeness and the accuracy of automatically generated fuzzy rules, several experiments are performed on different software defect prediction datasets, and the classification performance of resulting fuzzy systems is evaluated. Provided results show that even if training of evolutionary approaches seem to be more precise, similar accuracy can be achieved by linear approaches, and they perform better regarding the experiments on unseen data.},
	pages = {209--214},
	booktitle = {2018 3rd International Conference on Computer Science and Engineering ({UBMK})},
	author = {Mutlu, Begum and Sezer, Ebru A. and Akcayol, M. Ali},
	date = {2018-09},
	keywords = {Software, Automatically generated, Defects, Software defect prediction, Forecasting, Classification (of information), Expert systems, Computer software, Fuzzy inference, Fuzzy rules, Fuzzy systems, Rule generation, Fuzzy logic, Fuzzy sets, Classification performance, Comparative assessment, Evolutionary rules, Fuzzy inference systems, Fuzzy set theory, Interval-valued fuzzy sets, Linguistics, Software Defect Prediction, Evolutionary Rule Learning, Fuzzy Inference Systems, Fuzzy Rule Generation, Genetics, xyes, xfuzzy},
	file = {Mutlu et al. - 2018 - Automatic Rule Generation of Fuzzy Systems A Comp.pdf:C\:\\Users\\michalm\\Zotero\\storage\\BCXGVN26\\Mutlu et al. - 2018 - Automatic Rule Generation of Fuzzy Systems A Comp.pdf:application/pdf}
}

@inproceedings{kaur_evaluation_2017,
	title = {Evaluation of imbalanced learning with entropy of source code metrics as defect predictors},
	doi = {10.1109/ICTUS.2017.8286041},
	abstract = {This paper evaluates imbalanced learning algorithms with entropy of source code metrics as predictor variables. Four open source software systems are studied. These systems are {ECLIPSE} {JDT}, {EQUINOX}, {MYLYN} and {ECLIPSE} {PDE} {UI}. The results of this paper indicate that imbalanced learning algorithms perform better than classical learning methods in terms of Recall, G-mean 1, G-mean 2 and F-measures. For recall measure Condensed Nearest Neighbor rule +Tomek links ({CNNTL}) perform best, for G-mean 1, G-mean 2 and F-measure Random undersampling ({RUS}) perform best.},
	pages = {403--409},
	booktitle = {2017 International Conference on Infocom Technologies and Unmanned Systems (Trends and Future Directions) ({ICTUS})},
	author = {Kaur, Kamaldeep and Name, Jasmeet Kaur and Malhotra, Jyotsana},
	date = {2017-12},
	keywords = {Defects, Open source software, Learning systems, Open systems, Class imbalance learning, Codes (symbols), Measurement, Defect prediction, Imbalanced Learning, Learning algorithms, Software systems, Computer programming languages, Condensed nearest neighbor rule, Entropy, Open source software systems, Predictor variables, Random under samplings, Source code metrics, Prediction algorithms, Software algorithms, defect prediction, class imbalance learning, entropy of source code metrics, xno}
}

@inproceedings{alhazzaa_trade-offs_2019,
	title = {Trade-Offs between Early Software Defect Prediction versus Prediction Accuracy},
	doi = {10.1109/CSCI49370.2019.00216},
	abstract = {In any software development organization, reliability is crucial. Defect prediction is key in providing management with the tools for release planning. To predict defects we ask the question of how much data is required to make usable predictions? When testing, a rule of thumb is to start defect prediction after 60\% of system test has been accomplished. In an operational phase, managers cannot usually determine what constitutes 60\% of a release and might not want to wait that long to start defect prediction. Here we discuss the trade-offs between the need of early predictions versus making more accurate predictions.},
	pages = {1144--1150},
	booktitle = {2019 International Conference on Computational Science and Computational Intelligence ({CSCI})},
	author = {Alhazzaa, Lamees and Amschler Andrews, Anneliese},
	date = {2019-12},
	keywords = {Predictive models, Software, Software design, Software defect prediction, Forecasting, Economic and social effects, Prediction accuracy, Software reliability, Artificial intelligence, Accurate prediction, Commerce, Defect prediction, Early prediction, Operational phase, Release planning, Software development organizations, Estimation, Data models, change-point, defects, estimation, Mathematical model, prediction, software reliability, xno}
}

@article{riaz_rough_2018,
	title = {Rough Noise-Filtered Easy Ensemble for Software Fault Prediction},
	volume = {6},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2018.2865383},
	abstract = {Software fault prediction is the very important research topic for software quality assurance. Data-driven approaches provide robust mechanisms to deal with software fault prediction. However, the prediction performance of the model highly depends on the quality of the data set. Many software data sets suffer from the problem of class imbalance. In this regard, undersampling is a popular data pre-processing method in dealing with the class imbalance problem; easy ensemble presents a robust approach to achieve a high classification rate and address the biases toward majority class samples. However, imbalance class is not the only issue that harms the performance of classifiers. Some noisy data and irrelevant and redundant features may also reduce the performance of predictive accuracy of the classifier. In this paper, we propose two-stage data pre-processing, which incorporates feature selection and rough set-based K nearest neighbour rule ({KNN}) noise filter afore executing easy ensemble rough-{KNN} noise-filtered easy ensemble ({RKEE}). In the first stage, we eliminate the irrelevant and redundant features by the feature ranking algorithm, and in the second stage, we handle the imbalance class problem by using rough-{KNN} noise filter to eliminate noisy samples from both the minority and the majority class and also handle the uncertainty and the overlapping problem from both the minority and the majority class. Experimental evaluation on real-world software projects, such as {NASA} and Eclipse data set, is performed in order to demonstrate the effectiveness of our proposed approach. Furthermore, this paper comprehensively investigates the influencing factor in our approach, such as the impact of the rough set theory on noise-filter, the relationship between model performance and imbalance ratio, and so on. Comprehensive experiments indicate that the proposed approach shows outstanding performance with significance in terms of area-under-the-curve.},
	pages = {46886--46899},
	journaltitle = {{IEEE} Access},
	author = {Riaz, Saman and Arshad, Ali and Jiao, Licheng},
	date = {2018},
	keywords = {Data mining, Feature extraction, feature selection, Software, Computer software selection and evaluation, {NASA}, Forecasting, Classification (of information), Quality assurance, Class imbalance, Data handling, Software fault prediction, Computer software, Classification algorithm, Data preprocessing, Easy Ensemble, Filtering theory, Noise filters, Noise measurements, Personnel training, Processing, Rough set theory, Classification algorithms, Training, class imbalance, data preprocessing, easy ensemble, noise filter, Noise measurement, rough set theory, Rough sets, xno}
}

@inproceedings{diamantopoulos_towards_2015,
	title = {Towards Interpretable Defect-Prone Component Analysis Using Genetic Fuzzy Systems},
	doi = {10.1109/RAISE.2015.13},
	abstract = {The problem of Software Reliability Prediction is attracting the attention of several researchers during the last few years. Various classification techniques are proposed in current literature which involve the use of metrics drawn from version control systems in order to classify software components as defect-prone or defect-free. In this paper, we create a novel genetic fuzzy rule-based system to efficiently model the defect-proneness of each component. The system uses a Mamdani-Assilian inference engine and models the problem as a one-class classification task. System rules are constructed using a genetic algorithm, where each chromosome represents a rule base (Pittsburgh approach). The parameters of our fuzzy system and the operators of the genetic algorithm are designed with regard to producing interpretable output. Thus, the output offers not only effective classification, but also a comprehensive set of rules that can be easily visualized to extract useful conclusions about the metrics of the software.},
	pages = {32--38},
	booktitle = {2015 {IEEE}/{ACM} 4th International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering},
	author = {Diamantopoulos, Themistoklis and Symeonidis, Andreas},
	date = {2015-05},
	keywords = {Software, Defects, Forecasting, Genetic algorithms, Software engineering, Software component, Software reliability, Artificial intelligence, Software fault prediction, Measurement, Fuzzy inference, Fuzzy systems, Fuzzy logic, Classification technique, Component analysis, Genetic fuzzy systems, One-class Classification, Pittsburgh approach, Version control system, software fault prediction, Genetics, defect-prone components, genetic fuzzy systems, Sociology, Software Reliability Prediction, xyes}
}

@article{cai_design_2019,
	title = {Design Rule Spaces: A New Model for Representing and Analyzing Software Architecture},
	volume = {45},
	issn = {1939-3520},
	doi = {10.1109/TSE.2018.2797899},
	abstract = {In this paper, we propose an architecture model called Design Rule Space ({DRSpace}). We model the architecture of a software system as multiple overlapping {DRSpaces}, reflecting the fact that any complex software system must contain multiple aspects, features, patterns, etc. We show that this model provides new ways to analyze software quality. In particular, we introduce an Architecture Root detection algorithm that captures {DRSpaces} containing large numbers of a project's bug-prone files, which are called Architecture Roots ({ArchRoots}). After investigating {ArchRoots} calculated from 15 open source projects, the following observations become clear: from 35 to 91 percent of a project's most bug-prone files can be captured by just 5 {ArchRoots}, meaning that bug-prone files are likely to be architecturally connected. Furthermore, these {ArchRoots} tend to live in the system for significant periods of time, serving as the major source of bug-proneness and high maintainability costs. Moreover, each {ArchRoot} reveals multiple architectural flaws that propagate bugs among files and this will incur high maintenance costs over time. The implication of our study is that the quality, in terms of bug-proneness, of a large, complex software project cannot be fundamentally improved without first fixing its architectural flaws.},
	pages = {657--682},
	number = {7},
	journaltitle = {{IEEE} Transactions on Software Engineering},
	author = {Cai, Yuanfang and Xiao, Lu and Kazman, Rick and Mo, Ran and Feng, Qiong},
	date = {2019-07},
	keywords = {Computer software selection and evaluation, Open source software, Program debugging, Reverse engineering, Code smell, Computer architecture, Technical debts, Defect prediction, Analytical models, Bug localizations, Computer bugs, Production facility, Software architecture, Software systems, defect prediction, bug localization, code smells, Production facilities, reverse-engineering, technical debt, xyes}
}

@inproceedings{watanabe_identifying_2016,
	title = {Identifying recurring association rules in software defect prediction},
	doi = {10.1109/ICIS.2016.7550867},
	abstract = {Association rule mining discovers patterns of co-occurrences of attributes as association rules in a data set. The derived association rules are expected to be recurrent, that is, the patterns recur in future in other data sets. This paper defines the recurrence of a rule, and aims to find a criteria to distinguish between high recurrent rules and low recurrent ones using a data set for software defect prediction. An experiment with the Eclipse Mylyn defect data set showed that rules of lower than 30 transactions showed low recurrence. We also found that the lower bound of transactions to select high recurrence rules is dependent on the required precision of defect prediction.},
	pages = {1--6},
	booktitle = {2016 {IEEE}/{ACIS} 15th International Conference on Computer and Information Science ({ICIS})},
	author = {Watanabe, Takashi and Monden, Akito and Kamei, Yasutaka and Morisaki, Shuji},
	date = {2016-06},
	keywords = {association rule mining, Data mining, Computer software selection and evaluation, Defects, Software quality, Software defect prediction, Empirical studies, Forecasting, Association rules, Software Quality, Measurement, Defect prediction, Computer software, Data set, Co-occurrence, Information science, Lower bounds, Required precision, data mining, defect prediction, empirical study, software quality, Decision support systems, xno}
}

@inproceedings{karre_defect_2015,
	title = {A defect dependency based approach to improve software quality in integrated software products},
	abstract = {Integrated software products are complex in design. They are prone to defects caused by integrated and non-integrated modules of the entire integrated software suite. In such software products, a small proportion of defects are fixed as soon as they are reported. Rest of the defects are targeted for fixes in future product release cycles. Among such targeted defects, most of them seem to be insignificant and innocuous in the current version but have the potential to become acute in future versions. In this paper, we propose an approach to study defect dependency of the reported defect using a dependency metric. Identifying the dependency of a defect in an integrated product suite can help the product stake-owners to prioritize them and help improve software quality.},
	pages = {110--117},
	booktitle = {2015 International Conference on Evaluation of Novel Approaches to Software Engineering ({ENASE})},
	author = {Karre, Sai Anirudh and Reddy, Y. Raghu},
	date = {2015-04},
	keywords = {Data mining, Software, Testing, Computer software selection and evaluation, Defects, Software products, Classification (of information), Software Quality, Measurement, Product design, Dependency metric, Integrated module, Integrated products, Integrated software, Integrated software suite, Rule-based classification, Training, Defect Dataset, Defect Dependency, Dependency Metric, Influenza, Integrated Software Products, Rule-based Classification, xno}
}

@inproceedings{ibarguren_consolidated_2017,
	title = {The Consolidated Tree Construction algorithm in imbalanced defect prediction datasets},
	doi = {10.1109/CEC.2017.7969629},
	abstract = {In this short paper, we compare well-known rule/tree classifiers in software defect prediction with the {CTC} decision tree classifier designed to deal with class imbalanced. It is well-known that most software defect prediction datasets are highly imbalance (non-defective instances outnumber defective ones). In this work, we focused only on tree/rule classifiers as these are capable of explaining the decision, i.e., describing the metrics and thresholds that make a module error prone. Furthermore, rules/decision trees provide the advantage that they are easily understood and applied by project managers and quality assurance personnel. The {CTC} algorithm was designed to cope with class imbalance and noisy datasets instead of using preprocessing techniques (oversampling or undersampling), ensembles or cost weights of misclassification. The experimental work was carried out using the {NASA} datasets and results showed that induced {CTC} decision trees performed better or similar to the rest of the rule/tree classifiers.},
	pages = {2656--2660},
	booktitle = {2017 {IEEE} Congress on Evolutionary Computation ({CEC})},
	author = {Ibarguren, Igor and Pérez, Jesús M. and Mugerza, Javier and Rodriguez, Daniel and Harrison, Rachel},
	date = {2017-06},
	keywords = {Software, {NASA}, Decision trees, Measurement, Algorithm design and analysis, Prediction algorithms, Software algorithms, xyes},
	file = {Submitted Version:C\:\\Users\\michalm\\Zotero\\storage\\KMXS79CF\\Ibarguren et al. - 2017 - The Consolidated Tree Construction algorithm in im.pdf:application/pdf}
}

@inproceedings{anwar_using_2012,
	title = {Using Association Rules to Identify Similarities between Software Datasets},
	doi = {10.1109/QUATIC.2012.66},
	abstract = {A number of V\&V datasets are publicly available. These datasets have software measurements and defectiveness information regarding the software modules. To facilitate V\&V, numerous defect prediction studies have used these datasets and have detected defective modules effectively. Software developers and managers can benefit from the existing studies to avoid analogous defects and mistakes if they are able to find similarity between their software and the software represented by the public datasets. This paper identifies the similar datasets by comparing association patterns in the datasets. The proposed approach finds association rules from each dataset and identifies the overlapping rules from the 100 strongest rules from each of the two datasets being compared. Afterwards, average support and average confidence of the overlap is calculated to determine the strength of the similarity between the datasets. This study compares eight public datasets and results show that {KC}2 and {PC}2 have the highest similarity 83\% with 97\% support and 100\% confidence. Datasets with similar attributes and almost same number of attributes have shown higher similarity than the other datasets.},
	pages = {114--119},
	booktitle = {2012 Eighth International Conference on the Quality of Information and Communications Technology},
	author = {Anwar, Saba and Rana, Zeeshan Ali and Shamail, Shafay and Awais, Mian M.},
	date = {2012-09},
	keywords = {Defects, Software engineering, Association rules, Software developer, Software modules, Defect prediction, Association patterns, dataset similarity, Software Measurement, Software measures, defect prediction, association rules, software measures, xno}
}

@inproceedings{lopes_margarido_classification_2011,
	title = {Classification of defect types in requirements specifications: Literature review, proposal and assessment},
	abstract = {Requirements defects have a major impact throughout the whole software lifecycle. Having a specific defects classification for requirements is important to analyse the root causes of problems, build checklists that support requirements reviews and to reduce risks associated with requirements problems. In our research we analyse several defects classifiers; select the ones applicable to requirements specifications, following rules to build defects taxonomies; and assess the classification validity in an experiment of requirements defects classification performed by graduate and undergraduate students. Not all subjects used the same type of defect to classify the same defect, which suggests that defects classification is not consensual. Considering our results we give recommendations to industry and other researchers on the design of classification schemes and treatment of classification results.},
	pages = {1--6},
	booktitle = {6th Iberian Conference on Information Systems and Technologies ({CISTI} 2011)},
	author = {Lopes Margarido, Isabel and Faria, João Pascoal and Vidal, Raul Moreira and Vieira, Marco},
	date = {2011-06},
	note = {{ISSN}: 2166-0735},
	keywords = {Software, Testing, Defects, Software engineering, Software life cycles, software, Specifications, Taxonomies, Inspection, Classification of defects, Classification results, Classification scheme, Defects classification, Information systems, Literature reviews, requirements, Requirements specifications, Root cause, Students, Support requirements, Undergraduate students, Programming, defects, classification, Documentation, taxonomy, Taxonomy, xno}
}

@article{menzies_local_2013,
	title = {Local versus Global Lessons for Defect Prediction and Effort Estimation},
	volume = {39},
	issn = {1939-3520},
	doi = {10.1109/TSE.2012.83},
	abstract = {Existing research is unclear on how to generate lessons learned for defect prediction and effort estimation. Should we seek lessons that are global to multiple projects or just local to particular projects? This paper aims to comparatively evaluate local versus global lessons learned for effort estimation and defect prediction. We applied automated clustering tools to effort and defect datasets from the {PROMISE} repository. Rule learners generated lessons learned from all the data, from local projects, or just from each cluster. The results indicate that the lessons learned after combining small parts of different data sources (i.e., the clusters) were superior to either generalizations formed over all the data or local lessons formed from particular projects. We conclude that when researchers attempt to draw lessons from some historical data source, they should 1) ignore any existing local divisions into multiple sources, 2) cluster across all available data, then 3) restrict the learning of lessons to the clusters from other sources that are nearest to the test data.},
	pages = {822--834},
	number = {6},
	journaltitle = {{IEEE} Transactions on Software Engineering},
	author = {Menzies, Tim and Butcher, Andrew and Cok, David and Marcus, Andrian and Layman, Lucas and Shull, Forrest and Turhan, Burak and Zimmermann, Thomas},
	date = {2013-06},
	keywords = {Data mining, Software, Measurement, clustering, Estimation, defect prediction, Data models, Java, Context, effort estimation, Telecommunications, xno}
}

@article{xiaolong_rfc_2021,
	title = {{RFC}: A feature selection algorithm for software defect prediction},
	volume = {32},
	issn = {1004-4132},
	doi = {10.23919/JSEE.2021.000032},
	abstract = {Software defect prediction ({SDP}) is used to perform the statistical analysis of historical defect data to find out the distribution rule of historical defects, so as to effectively predictdefects in the new software. However, there are redundant and irrelevant features in the software defect datasets affecting the performance of defect predictors. In order to identify and remove the redundant and irrelevant features in software defectdatasets, we propose Relief F-based clustering ({RFC}), a cluster-based feature selection algorithm. Then, the correlation between features is calculated based on the symmetric uncertainty. According to the correlation degree, {RFC} partitions features into kclusters based on the k-medoids algorithm, and finally selects the representative features from each cluster to form the final feature subset. In the experiments, we compare the proposed {RFC} with classical feature selection algorithms on nine National Aeronautics and Space Administration ({NASA}) software defectprediction datasets in terms of area under curve ({AUC}) and F-value. The experimental results show that {RFC} can effectively improve the performance of {SDP}.},
	pages = {389--398},
	number = {2},
	journaltitle = {Journal of Systems Engineering and Electronics},
	author = {Xiaolong, Xu and Wen, Chen and Xinheng, Wang},
	date = {2021-04},
	keywords = {Clustering algorithms, Feature extraction, feature selection, Software, Defects, Based clustering, Correlation between features, Correlation degree, Distribution rule, Feature selection algorithm, K-medoids algorithms, {NASA}, Software defect prediction, Software defects, Systems engineering, Prediction algorithms, Software algorithms, cluster, Correlation, Partitioning algorithms, software defect prediction ({SDP}), xno}
}

@inproceedings{gainaru_taming_2012,
	title = {Taming of the Shrew: Modeling the Normal and Faulty Behaviour of Large-scale {HPC} Systems},
	doi = {10.1109/IPDPS.2012.107},
	abstract = {{HPC} systems are complex machines that generate a huge volume of system state data called "events". Events are generated without following a general consistent rule and different hardware and software components of such systems have different failure rates. Distinguishing between normal system behaviour and faulty situation relies on event analysis. Being able to detect quickly deviations from normality is essential for system administration and is the foundation of fault prediction. As {HPC} systems continue to grow in size and complexity, mining event flows become more challenging and with the upcoming 10 Pet flop systems, there is a lot of interest in this topic. Current event mining approaches do not take into consideration the specific behaviour of each type of events and as a consequence, fail to analyze them according to their characteristics. In this paper we propose a novel way of characterizing the normal and faulty behaviour of the system by using signal analysis concepts. All analysis modules create {ELSA} (Event Log Signal Analyzer), a toolkit that has the purpose of modelling the normal flow of each state event during a {HPC} system lifetime, and how it is affected when a failure hits the system. We show that these extracted models provide an accurate view of the system output, which improves the effectiveness of proactive fault tolerance algorithms. Specifically, we implemented a filtering algorithm and short-term fault prediction methodology based on the extracted model and test it against real failure traces from a large-scale system. We show that by analyzing each event according to its specific behaviour, we get a more realistic overview of the entire system.},
	pages = {1168--1179},
	booktitle = {2012 {IEEE} 26th International Parallel and Distributed Processing Symposium},
	author = {Gainaru, Ana and Cappello, Franck and Kramer, William},
	date = {2012-05},
	note = {{ISSN}: 1530-2075},
	keywords = {Data mining, Predictive models, Fault detection, Fault prediction, Fault tolerance, Analytical models, Complex machines, Distributed parameter networks, Entire system, Event analysis, Event mining, Failure rate, Filtering algorithm, Hardware and software components, Normal flow, Proactive fault, Signal analysis, Signal analyzers, System administration, System output, System state, Prediction algorithms, Correlation, fault detection, fault tolerance, large-scale {HPC} systems, Large-scale systems, signal analysis, xno}
}

@article{xu_defect_2021,
	title = {Defect Prediction With Semantics and Context Features of Codes Based on Graph Representation Learning},
	volume = {70},
	issn = {1558-1721},
	doi = {10.1109/TR.2020.3040191},
	abstract = {To optimize the process of software testing and to improve software quality and reliability, many attempts have been made to develop more effective methods for predicting software defects. Previous work on defect prediction has used machine learning and artificial software metrics. Unfortunately, artificial metrics are unable to represent the features of syntactic, semantic, and context information of defective modules. In this article, therefore, we propose a practical approach for identifying software defect patterns via the combination of semantics and context information using abstract syntax tree representation learning. Graph neural networks are also leveraged to capture the latent defect information of defective subtrees, which are pruned based on a fix-inducing change. To validate the proposed approach for predicting defects, we define mining rules based on the {GitHub} workflow and collect 6052 defects from 307 projects. The experiments indicate that the proposed approach performs better than the state-of-the-art approach and five traditional machine learning baselines. An ablation study shows that the information about code concepts leads to a significant increase in accuracy.},
	pages = {613--625},
	number = {2},
	journaltitle = {{IEEE} Transactions on Reliability},
	author = {Xu, Jiaxi and Wang, Fei and Ai, Jun},
	date = {2021-06},
	keywords = {Data mining, Software, software engineering, Semantics, Measurement, Computer bugs, Deep learning, defect prediction, Software development management, graph representation learning, software defect dataset, Syntactics, xyes}
}

@article{souri_formal_2020,
	title = {Formal Verification of a Hybrid Machine Learning-Based Fault Prediction Model in Internet of Things Applications},
	volume = {8},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2020.2967629},
	abstract = {By increasing the complexity of the Internet of Things ({IoT}) applications, fault prediction become an important challenge in interactions between human, and smart devices. Fault prediction is one of the key factors to achieve better arranging the {IoT} applications. Most of the current research studies evaluated the fault prediction methods using simulation environments. However, formal verification of the correctness of a fault prediction method has not been reported yet. This paper presents a behavioral modeling and formal verification of a hybrid machine learning-based fault prediction model with Multi-Layer Perceptron ({MLP}) and Particle Swarm Optimization ({PSO}) algorithms. In particular, the {PSO} is used for feature selection. Then, the fault prediction is considered as a behavior to be verified formally. The fault prediction behavior is divided into two types of behaviors: dimension reduction behavior and prediction behavior. For each of the behaviors, one formal model is designed. The behavioral models designed are mapped into the Labeled Transition System ({LTS}). The Process Analysis Toolkit ({PAT}) model checker is employed to evaluate the behavioral models. The accuracy of the fault prediction method is done by some existing specifications such as deadlock-free and reachability properties in terms of linear temporal logic formulas. Also, the verification of the fault prediction behaviors is used to detect the defect metrics of information-centric {IoT} applications. Experimental results showed that our proposed verification method has minimum verification time and memory usage for evaluating critical specification rules than other research studies.},
	pages = {23863--23874},
	journaltitle = {{IEEE} Access},
	author = {Souri, Alireza and Mohammed, Amin Salih and Yousif Potrus, Moayad and Malik, Mazhar Hussain and Safara, Fatemeh and Hosseinzadeh, Mehdi},
	date = {2020},
	keywords = {Predictive models, Software, Testing, Measurement, fault prediction, formal verification, Internet of Things, Internet of Things applications, Machine learning algorithms, multi-layer perceptron, particle swarm optimization, process analysis toolkit, xno}
}

@inproceedings{chen_inspection_2013,
	title = {Inspection flow of yield impacting systematic defects},
	doi = {10.1109/eMDC.2013.6756065},
	abstract = {Yield impacting systematic defects finding is no longer just relied on Design Rule Checking ({DRC}) provided by designer or Lithography Rule Checking ({LRC}) provided by post-optical proximity correction ({OPC}) results. An inspection flow is proposed in this paper, which is combining the inspection {KLA} tool and Hotspot Pattern Analyzer ({HPA}) database software to do the systematic defects filtering, sorting, grouping, and classification on the data base after hot scan inspection. 2nd time high sensitive inspection is done with new care area, which is reduced into one ten-thousandth of original inspection area. Following this inspection flow, we can identify the process window more accuracy.},
	pages = {1--3},
	booktitle = {2013 e-Manufacturing Design Collaboration Symposium ({eMDC})},
	author = {Chen, Chimin and Yang, {ChengHua} and Liao, Hsiang-Chou and Luoh, Tuung and Yang, Ling-Wu and Yang, Tahone and Chen, Kuang-Chao and Lu, Chih-Yuan and Liu, Donghua and Fan, Jeff and Lv, Rong},
	date = {2013-09},
	keywords = {Defects, Integrated circuit layout, Manufacture, Database software, Design rule checking, Finite element method, Hot Scan, Inspection, Inspection flow, Pattern Grouping, Photolithography, Proximity correction, {PWQ}, Systematic defects, {FEM}, Finite element analysis, Joints, Lead, Systematic Defect, Systematics, xno}
}

@inproceedings{gowda_false_2018,
	title = {False Positive Analysis of Software Vulnerabilities Using Machine Learning},
	doi = {10.1109/CCEM.2018.00010},
	abstract = {Dynamic Application Security Testing is conducted with the help of automated tools that have built-in scanners which automatically crawl all the webpages of the application and report security vulnerabilities based on certain set of pre-defined scan rules. Such pre-defined rules cannot fully determine the accuracy of a vulnerability and very often one needs to manually validate these results to remove the false positives. Eliminating false positives from such results can be a quite painful and laborious task. This article proposes an approach of eliminating false positives by using machine learning . Based on the historic data available on false positives, suitable machine learning models are deployed to predict if the reported defect is a real vulnerability or a false positive},
	pages = {3--6},
	booktitle = {2018 {IEEE} International Conference on Cloud Computing in Emerging Markets ({CCEM})},
	author = {Gowda, Sumanth and Prajapati, Divyesh and Singh, Ranjit and Gadre, Swanand S.},
	date = {2018-11},
	keywords = {Predictive models, Software, Testing, Learning systems, Decision trees, Machine learning, Machine learning models, Software vulnerabilities, Commerce, Automated tools, Cloud computing, Dynamic applications, False positive, Security of data, Security vulnerabilities, Software security, vulnerabilities, Prediction algorithms, Machine Learning, Data models, False Positive Analysis, Software Security, xno}
}

@inproceedings{herzig_empirically_2015,
	title = {Empirically Detecting False Test Alarms Using Association Rules},
	volume = {2},
	doi = {10.1109/ICSE.2015.133},
	abstract = {Applying code changes to software systems and testing these code changes can be a complex task that involves many different types of software testing strategies, e.g. system and integration tests. However, not all test failures reported during code integration are hinting towards code defects. Testing large systems such as the Microsoft Windows operating system requires complex test infrastructures, which may lead to test failures caused by faulty tests and test infrastructure issues. Such false test alarms are particular annoying as they raise engineer attention and require manual inspection without providing any benefit. The goal of this work is to use empirical data to minimize the number of false test alarms reported during system and integration testing. To achieve this goal, we use association rule learning to identify patterns among failing test steps that are typically for false test alarms and can be used to automatically classify them. A successful classification of false test alarms is particularly valuable for product teams as manual test failure inspection is an expensive and time-consuming process that not only costs engineering time and money but also slows down product development. We evaluating our approach on system and integration tests executed during Windows 8.1 and Microsoft Dynamics {AX} development. Performing more than 10,000 classifications for each product, our model shows a mean precision between 0.85 and 0.90 predicting between 34\% and 48\% of all false test alarms.},
	pages = {39--48},
	booktitle = {2015 {IEEE}/{ACM} 37th {IEEE} International Conference on Software Engineering},
	author = {Herzig, Kim and Nagappan, Nachiappan},
	date = {2015-05},
	note = {{ISSN}: 1558-1225},
	keywords = {Testing, Cost engineering, Software engineering, Association rules, Software testing, Alarm systems, Codes (symbols), Software systems, Product development, Integration testing, Empirical data, Engineering time, Integration, Manual inspection, Mean precision, Microsoft windows, Safety engineering, Software testing strategies, Test infrastructures, Windows operating system, Inspection, Manuals, xno}
}

@inproceedings{chen_effects_2016,
	title = {Effects of online fault detection mechanisms on Probabilistic Timing Analysis},
	doi = {10.1109/DFT.2016.7684067},
	abstract = {In real time systems, random caches have been proposed as a way to simplify software timing analysis, by avoiding corner cases usually found in deterministic systems. Using this random approach, one can obtain an application's probabilistic Worst Case Execution Time ({pWCET}) to be used for timing analysis. As with deterministic systems, technology scaling in cache memories is making transient and permanent faults more likely, which in turn affects the system's timing behavior. To mitigate these effects, one can introduce a detection mechanism that classifies a fault as transient or permanent, with the goal of disabling permanently faulty cache blocks to avoid future accesses. In this paper, we compare the effects of two online detection mechanisms for permanent faults, namely rule-based detection and Dynamic Hidden Markov Model (D-{HMM}) based detection, for the generation of safe {pWCET} estimates. Experimental results show that different mechanisms can greatly affect safe {pWCET} margins, and that by using D-{HMM} the {pWCET} of the system can be improved compared to rule-based detection.},
	pages = {41--46},
	booktitle = {2016 {IEEE} International Symposium on Defect and Fault Tolerance in {VLSI} and Nanotechnology Systems ({DFT})},
	author = {Chen, Chao and Panerati, Jacopo and Beltrame, Giovanni},
	date = {2016-09},
	note = {{ISSN}: 2377-7966},
	keywords = {Defects, Fault detection, Fault tolerance, Cache memory, Different mechanisms, Hidden Markov models, Interactive computer systems, Real time systems, Rule based detection, Software timing analysis, Timing circuits, Transient and permanent fault, Worst-case execution time, Reliability, Detection mechanism, Deterministic systems, Markov processes, Nanotechnology, On-line fault detection, {VLSI} circuits, Benchmark testing, Biological system modeling, Silicon, xno}
}

@inproceedings{xu_classification_2020,
	title = {The Classification and Propagation of Program Comments},
	abstract = {Natural language comments are like bridges between human logic and software semantics. Developers use comments to describe the function, implementation, and property of code snippets. This kind of connections contains rich information, like the potential types of a variable and the pre-condition of a method, among other things. In this paper, we categorize comments and use natural language processing techniques to extract information from them. Based on the semantics of programming languages, different rules are built for each comment category to systematically propagate comments among code entities. Then we use the propagated comments to check the code usage and comments consistency. Our demo system finds 37 bugs in real-world projects, 30 of which have been confirmed by the developers. Except for bugs in the code, we also find 304 pieces of defected comments. The 12 of them are misleading and 292 of them are not correct. Moreover, among the 41573 pieces of comments we propagate, 87 comments are for private native methods which had neither code nor comments. We also conduct a user study where we find that propagated comments are as good as human-written comments in three dimensions of consistency, naturalness, and meaningfulness.},
	pages = {1394--1396},
	booktitle = {2020 35th {IEEE}/{ACM} International Conference on Automated Software Engineering ({ASE})},
	author = {Xu, Xiangzhe},
	date = {2020-09},
	note = {{ISSN}: 2643-1572},
	keywords = {Data mining, Software, Program debugging, Extract informations, {NAtural} language processing, Natural language processing systems, Natural languages, Real world projects, Semantics, Semantics of programming languages, Software engineering, Three dimensions, User study, Computer bugs, comment, Computer languages, Natural language processing, programm analysis, xno}
}

@inproceedings{de_castro_ribeiro_detection_2018,
	title = {Detection and Classification of Faults in Aeronautical Gas Turbine Engine: a Comparison Between two Fuzzy Logic Systems},
	doi = {10.1109/FUZZ-IEEE.2018.8491444},
	abstract = {Gas turbines are the most common engine used in the majority of commercial aircraft. Due to its criticality, to detect and classify faults in a gas turbine is extremely important. In this work, a type-1 and singleton fuzzy logic system trained by steepest descent method is used for detecting and classifying gas turbine faults. The data set was obtained through simulations on the software Propulsion Diagnostic Method Evaluation Strategy created by the National Aeronautics and Space Administration. Results are compared to those obtained with a type-1 fuzzy classifier with rule extraction by Wang and Mendel method. Analysis of results shows the effectiveness of the proposed model. When compared to the Wang and Mendel fuzzy classifier, it requires fewer rules to achieve a better performance.},
	pages = {1--7},
	booktitle = {2018 {IEEE} International Conference on Fuzzy Systems ({FUZZ}-{IEEE})},
	author = {de Castro Ribeiro, Mateus Gheorghe and Calderano, Pedro Henrique Souza and Amaral, Renan Piazzaroli Finotti and de Menezes, Ivan Fabio Mota and Tanscheit, Ricardo and Vellasco, Marley Maria Bernardes Rebuzzi and de Aguiar, Eduardo Pestana},
	date = {2018-07},
	keywords = {Computer circuits, {NASA}, Classification (of information), Fault detection, Aircraft, Fuzzy systems, Fuzzy logic, Fuzzy sets, Aeronautical gas turbines, Commercial aircraft, Data set, Diagnostic methods, Error detection, Fuzzy classifiers, Fuzzy logic system, Gas turbines, Gases, Rule extraction, Steepest descent method, Sensors, Classification, Engines, Indexes, Aeronautical Gas Turbine, Atmospheric modeling, Detection., Fuzzy Logic System, Turbines, xno}
}

@article{ma_dynamic_2011,
	title = {Dynamic Hybrid Fault Modeling and Extended Evolutionary Game Theory for Reliability, Survivability and Fault Tolerance Analyses},
	volume = {60},
	issn = {1558-1721},
	doi = {10.1109/TR.2011.2104997},
	abstract = {We introduce a new layered modeling architecture consisting of dynamic hybrid fault modeling and extended evolutionary game theory for reliability, survivability, and fault tolerance analyses. The architecture extends traditional hybrid fault models and their relevant constraints in the Agreement algorithms with survival analysis, and evolutionary game theory. The dynamic hybrid fault modeling (i) transforms hybrid fault models into time- and covariate-dependent models; (ii) makes real-time prediction of reliability more realistic, and allows for real-time prediction of fault-tolerance; (iii) sets the foundation for integrating hybrid fault models with reliability and survivability analyses by integrating them with evolutionary game modeling; and (iv) extends evolutionary game theory by stochastically modeling the survival (or fitness) and behavior of `game players.' To analyse survivability, we extend dynamic hybrid fault modeling with a third-layer, operational level modeling, to develop the three-layer survivability analysis approach (dynamic hybrid fault modeling constitutes the tactical and strategic levels). From the perspective of evolutionary game modeling, the two mathematical fields, i.e., survival analysis and agreement algorithms, which we applied for developing dynamic hybrid fault modeling, can also be utilized to extend the power of evolutionary game theory in modeling complex engineering, biological (ecological), and social systems. Indeed, a common property of the areas where our extensions to evolutionary game theory can be advantageous is that the risk analysis and management are a core issue. Survival analysis (including competing risks analysis, and multivariate survival analysis) offers powerful modeling tools to analyse time-, space-, and/or covariate-dependent uncertainty, vulnerability, and/or frailty which `game players' may experience. The agreement algorithms, which are not limited to the agreement algorithms from distributed computing, when applied to extend evolutionary game modeling, can be any problem (game system) specific rules (algorithms or models) that can be utilized to dynamically check the consensus among game players. We expect that the modeling architecture and approaches discussed in the study should be implemented as a software environment to deal with the necessary sophistication. Evolutionary computing should be particularly convenient to serve as the core optimization engine, and should simplify the implementation. Accordingly, a brief discussion on the software architecture is presented.},
	pages = {180--196},
	number = {1},
	journaltitle = {{IEEE} Transactions on Reliability},
	author = {Ma, Zhanshan and Krings, Axel W.},
	date = {2011-03},
	keywords = {Analytical models, Reliability, Algorithm design and analysis, fault tolerance, Agreement algorithms, Brain models, Byzantine generals problem, dynamic hybrid fault models, extended evolutionary game theory modeling, Game theory, Heuristic algorithms, reliability, survivability, survival analysis, wireless sensor networks, xno}
}

@inproceedings{bandyopadhyay_using_2012,
	title = {Using mean current vector to develop a rule base for identification of {IGBT} faults in induction motor drives},
	doi = {10.1109/ICPEN.2012.6492316},
	abstract = {One of the most common fault in Pulse Width Modulation ({PWM}) Voltage Source Inverter ({VSI}) feeding an induction motor is open gate drive of the switching device ({IGBT}). In addition to these extreme fault cases, there may be less severe faults cases due to improper contact points, problematic solder joints, poor connections etc. Main objective of this work is to develop a rule base that not only can segregate these two different types of fault cases but also can identify the power switch in which such a fault has occurred. Rigorous simulation results using {PSIM} software for creating all variations of the above mentioned fault cases in a {PWM} {VSI} driven induction motor is presented for diagnosing the condition of an inverter. The three phase line currents feeding the motor are recorded and transformed to d-q reference frame using Park's Transformation for further analysis. From d and q current components thus obtained, the mean current vector is calculated. The loci of the mean current vector are plotted on the d-q plane and their patterns are observed. Shapes of these loci have been found to be effective in classifying different fault modes.},
	pages = {1--6},
	booktitle = {2012 1st International Conference on Power and Energy in {NERIST} ({ICPEN})},
	author = {Bandyopadhyay, I. and Das, S. and Purkait, P. and Koley, C.},
	date = {2012-12},
	keywords = {Rule base, Concordia pattern, {IGBT} open base, Mean current vector, Park's Tranformation, {PWM}, {VSI} Drive, xno}
}

@inproceedings{mao_variable_2011,
	title = {Variable Precision Rough Set-Based Fault Diagnosis for Web Services},
	doi = {10.1109/TrustCom.2011.215},
	abstract = {Web service is the emergent technology for constructing more complex and flexible software system for business applications. However, some new features of Web service-based software such as heterogeneity and loose coupling bring great trouble to the latter fault debugging and diagnosis. In the paper, variable precision rough set-based diagnosis framework is presented. In such debugging model, {SOAP} message monitoring and service invocation instrument are used to record service interface information. Meanwhile, factors of execution context are also viewed as conditional attributes of knowledge representation system. The final execution result is treated as the decision attribute, and failure ontology is utilized to classify system's failure behaviors. Based on this extended information system, variable precision rough set reasoning is performed to generate the probability association rules, which are the clues for locating the possible faulty services. In addition, the experiment on a real-world Web services system is performed to demonstrate the feasibility and effectiveness of our proposed method.},
	pages = {1550--1555},
	booktitle = {2011IEEE 10th International Conference on Trust, Security and Privacy in Computing and Communications},
	author = {Mao, Chengying},
	date = {2011-11},
	note = {{ISSN}: 2324-9013},
	keywords = {Ontology, Program debugging, Association rules, Computer system recovery, Failure analysis, Software systems, Embedded systems, Rough set theory, Rough set, Business applications, Decision attribute, Embedded software, Execution context, Failure behaviors, Knowledge representation, Program diagnostics, Service interfaces, Service invocation, Service-based, {SOAP} messages, Variable precision, Variable precision rough sets, Web services, Information systems, association rule, Cognition, Business, fault diagnosis, failure, Linux, rough set, service interface, Wireless communication, xno}
}

@inproceedings{miura_fast_2013,
	title = {Fast and accurate design based binning based on hierarchical clustering with invariant feature vectors for {BEOL}},
	doi = {10.1109/ASMC.2013.6552744},
	abstract = {As design rules continue to shrink, systematic defects have become a serious problem. It becomes very important to review systematic defects effectively by a defect review {SEM} (scanning electron microscope) and to modify the design and the process to keep or improve an yield. In this paper, we propose a fast and accurate design based binning ({DBB}) method that is based on the hierarchical clustering with invariant feature vectors for back end of line ({BEOL}). In order to improve classification accuracy, we employ the hierarchical clustering method. Shift-, rotation-, and flip-invariant feature vectors are extracted from layout data. We propose two variations of {DBB} methods: one-step method and two-step method. The one-step method employs solely the hierarchical clustering. It can improve classification accuracy. However, computational time of the hierarchical clustering is high so that it is not practical to classify many defects by this method. In order to achieve both high accuracy and fast computation, we also propose two-step method that employs the hierarchical clustering after classifying defects by the {DBB} software used in the production line. We apply the proposed two methods to volume production data. The results show that the proposed two-step method can significantly improve accuracy against the production line {DBB} software, despite of slight decrease in purity and slight increase in computation time.},
	pages = {7--12},
	booktitle = {{ASMC} 2013 {SEMI} Advanced Semiconductor Manufacturing Conference},
	author = {Miura, Katsuyoshi and Soga, Yuki and Nakamae, Koji and Kadota, Kenichi and Aritake, Toshiyuki and Yamazaki, Yuichiro},
	date = {2013-05},
	note = {{ISSN}: 2376-6697},
	keywords = {Feature extraction, Software, Vectors, defect, Production, Systematics, accuracy, Accuracy, {DBB} (design based binning), {DBG} (design based grouping), geometric mean, hierarchical clustering, Layout, purity, xno}
}

@article{wang_security_2020,
	title = {Security Assessment of Blockchain in Chinese Classified Protection of Cybersecurity},
	volume = {8},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2020.3036004},
	abstract = {Classified protection is one of primary security policies of information system in many countries. With the increasing popularity of blockchain in various fields of applications, it is extremely necessary to promote classified protection for blockchain's risk assessment in order to push forward the sustainable development of blockchain. Taking the Level 3 in Chinese classified protection 2.0 as an example, this paper proposes the common evaluation rules on blockchain to ensure that blockchain can meet the needs of countries to build it as critical infrastructure. Both assessment requirements and enforcement proposals are presented and analyzed from the standpoint of blockchain's core technologies, e.g., peer-to-peer network, distributed ledger, contract's scripting system, and consensus mechanism. Moreover, the assessment results on three main platforms, Bitcoin, Ethereum, and Hyperledger, are summarized and analyzed in compliance with the control points specified in the level 3. Our investigation indicates that the current blockchain is able to satisfy the requirements of evaluation items in many aspects, such as software fault tolerance, resource control, backup and recovery, but further improvements are still needed for some aspects, including security audit, access control, identification and authentication, data integrity, etc., in order to satisfy the requirements of important fields on national security, economic development and human life.},
	pages = {203440--203456},
	journaltitle = {{IEEE} Access},
	author = {Wang, Di and Zhu, Yan and Zhang, Yi and Liu, Guowei},
	date = {2020},
	keywords = {Software, Access control, Blockchain, Compliance control, Core technology, Cyber security, Evaluation items, Evaluation rules, Fault tolerance, Fault tolerant computer systems, National security, Peer to peer networks, Resource control, Risk assessment, Security assessment, Security policy, Software fault tolerances, {XML}, assessment and analysis, classified protection of cybersecurity, consensus mechanism, Distributed ledger, Peer-to-peer computing, peer-to-peer network, Proposals, Risk management, Sustainable development, xno}
}

@inproceedings{kazman_case_2015,
	title = {A Case Study in Locating the Architectural Roots of Technical Debt},
	volume = {2},
	doi = {10.1109/ICSE.2015.146},
	abstract = {Our recent research has shown that, in large-scale software systems, defective files seldom exist alone. They are usually architecturally connected, and their architectural structures exhibit significant design flaws which propagate bugginess among files. We call these flawed structures the architecture roots, a type of technical debt that incurs high maintenance penalties. Removing the architecture roots of bugginess requires refactoring, but the benefits of refactoring have historically been difficult for architects to quantify or justify. In this paper, we present a case study of identifying and quantifying such architecture debts in a large-scale industrial software project. Our approach is to model and analyze software architecture as a set of design rule spaces ({DRSpaces}). Using data extracted from the project's development artifacts, we were able to identify the files implicated in architecture flaws and suggest refactorings based on removing these flaws. Then we built economic models of the before and (predicted) after states, which gave the organization confidence that doing the refactorings made business sense, in terms of a handsome return on investment.},
	pages = {179--188},
	booktitle = {2015 {IEEE}/{ACM} 37th {IEEE} International Conference on Software Engineering},
	author = {Kazman, Rick and Cai, Yuanfang and Mo, Ran and Feng, Qiong and Xiao, Lu and Haziyev, Serge and Fedak, Volodymyr and Shapochka, Andriy},
	date = {2015-05},
	note = {{ISSN}: 1558-1225},
	keywords = {Software engineering, Computer architecture, Technical debts, Economics, Architectural structure, Design rules, Economic models, Flawed structure, Industrial software, Large-scale software systems, Recent researches, Business, History, Microprocessors, Sonar detection, xno, xeee}
}

@inproceedings{xuedong_strategy_2017,
	title = {Strategy of diagnosis and test in {ICCS} of large-scale laser facility},
	doi = {10.1109/CAC.2017.8243207},
	abstract = {Integrated Computer Control System (abbr. {ICCS}) of large-scale laser facility uses a scalable software architecture to manage more than 10,000 control points to operate 48 powerful laser beamlines, provides for the integration of all elements of laser and target area distributed subsystems to form an overall operational control system. Fault detection and diagnosis test are the important technique to ensure the system's safety, reliability and healthy manage. This paper is an overview of the diagnostic and test strategy used in {ICCS} of large-scale laser facility, including design rules, task decomposition strategy, classification strategy, the scheme of on-line diagnosis and offline diagnosis, diagnosis process and data preconditioning about data standardization and quality control.},
	pages = {2563--2566},
	booktitle = {2017 Chinese Automation Congress ({CAC})},
	author = {Xuedong, Zhang and Xiaoli, Wang and Tianyou, Yun},
	date = {2017-10},
	keywords = {Software, Testing, Computer aided diagnosis, Fault detection, Hardware, Computer control systems, Data preconditioning, Data standardization, Distributed subsystems, Fault detection and diagnosis, {ICCS}, Integrated computer control systems, On-line diagnosis, Operational control, Quality control, Standardization, Fault diagnosis, Control systems, diagnosis and test, Laser theory, off-line diagnosis, on-line diagnosis, standardization, xno}
}

@article{wang_practical_2017,
	title = {Practical Network-Wide Packet Behavior Identification by {AP} Classifier},
	volume = {25},
	issn = {1558-2566},
	doi = {10.1109/TNET.2017.2720637},
	abstract = {Identifying the network-wide forwarding behaviors of a packet is essential for many network management applications, including rule verification, policy enforcement, attack detection, traffic engineering, and fault localization. Current tools that can perform packet behavior identification either incur large time and memory costs or do not support real-time updates. In this paper, we present {AP} Classifier, a control plane tool for packet behavior identification. {AP} Classifier is developed based on the concept of atomic predicates, which can be used to characterize the forwarding behaviors of packets. Experiments using the data plane network state of two real networks show that the processing speed of {AP} Classifier is faster than existing tools by at least an order of magnitude. Furthermore, {AP} Classifier uses very small memory and is able to support real-time updates.},
	pages = {2886--2899},
	number = {5},
	journaltitle = {{IEEE}/{ACM} Transactions on Networking},
	author = {Wang, Huazhe and Qian, Chen and Yu, Ye and Yang, Hongkun and Lam, Simon S.},
	date = {2017-10},
	keywords = {Computer aided diagnosis, Fault detection, Software defined networking, Failure analysis, Interactive computer systems, Real time systems, Computer networks, Data structures, Fault localization, {IEEE} transactions, Management applications, Packet classification, Packet networks, Policy enforcement, Ports (Computers), Practical networks, Throughput, Tools, Traffic Engineering, Fault diagnosis, Network-wide behavior, packet classification, Real-time systems, software-defined networking, xno}
}

@inproceedings{li_fractal_2010,
	title = {Fractal study on fault system of Carboniferous in Junggar Basin based on {GIS}},
	doi = {10.1109/GEOINFORMATICS.2010.5567793},
	abstract = {Fault system is a significant evidence of tectonic movement during crust tectonic evolution and may play an more important role in oil-gas accumulation process than other tectonic types in sedimentary basin. Carboniferous surface faults in Junggar Basin developed well and varied in size and distribution. There are about 200 faults in Carboniferous, and 187 of them are thrust faults. Chaos-fractals theories have been widely investigated and great progress has been made in the past three decades. One of the important conception-fractal dimension had become a powerful tool for describing non-linearity dynamical system characteristic. The clustered objects in nature are often fractal and fault system distribution in space is inhomogeneous, always occurs in groups, so we can describe spatial distribution of faults from the point of fractal dimension. Fractal dimension of fault system is a comprehensive factor associated with fault number, size, combination modes and dynamics mechanism, so it can evaluate the complexity of fault system quantitatively. The relationship between fault system and oil-gas accumulation is a focus and difficulty problem in petroleum geology, and fractal dimension is a new tool for describing fault distribution and predicting potential areas of hydrocarbon resources. Geographic Information System ({GIS}) is a kind of technological system collecting, storing, managing, computing, analyzing, displaying and describing the geospatial information supported by computer software and hardware. In the last 15-20 years, {GIS} have been increasingly used to address a wide variety of geoscience problems. Weights-of-evidence models use the theory of conditional probability to quantify spatial association between fractal dimension and oil-gas accumulation. The weights of evidence are combined with the prior probability of occurrence of oil-gas accumulation using Bayes'rule in a loglinear form under an assumption of conditional independence of the dimension maps to derive posterior probability of occurrence of oil-gas accumulation. In this paper, we first vectorize the fault system in Carboniferous of Junggar Basin in {GIS} software and store it as polyline layer in Geodatabase of {GIS} to manage and analyze, then calculate the fractal dimension of three types which are box dimension, information dimension and cumulative length dimension using spatial functions of {GIS}, in the last use weights-of-evidence model to calculate the correlation coefficients in {GIS} environment between oil-gas accumulation and three types of fractal dimension in order to quantity the importance of fault system.},
	pages = {1--5},
	booktitle = {2010 18th International Conference on Geoinformatics},
	author = {Li, Bo and Zhang, Tingshan and Ding, Guangming and Wang, Weiyuan and Xiang, Yu},
	date = {2010-06},
	note = {{ISSN}: 2161-0258},
	keywords = {Partial discharges, Analytical models, Weights of evidences, Gases, Computer hardware, Information systems, Bayes' rule, Box dimension, Combination modes, Conditional independences, Conditional probabilities, Correlation coefficient, Dynamical systems, Evidence model, Fault distribution, Fault system, Fractal dimension, Fractal studies, Geodatabase, Geographic information, Geographic information systems, Geosciences, Geospatial information, {GIS} software, Hydrocarbon resources, Hydrocarbons, Information dimensions, Junggar Basin, Non-Linearity, Oil-gas accumulation, Petroleum geology, Posterior probability, Prior probability, Probability, Sedimentary basin, Size distribution, Spatial distribution, Spatial functions, Surface faults, System characteristics, Technological system, Tectonic evolution, Tectonic movements, Tectonics, Thrust faults, Computational modeling, Correlation, fault system, fractal dimension, Fractals, Geographic Information System, Geographic Information Systems, Geology, oil-gas accumulation, Petroleum, weights-of-evidence model, xno}
}

@article{maddeh_decision_2021,
	title = {Decision tree-based Design Defects Detection},
	volume = {9},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2021.3078724},
	abstract = {Design defects affect project quality and hinder development and maintenance. Consequently, experts need to minimize these defects in software systems. A promising approach is to apply the concepts of refactoring at higher level of abstraction based on {UML} diagrams instead of code level. Unfortunately, we find in literature many defects that are described textually and there is no consensus on how to decide if a particular design violates model quality. Defects could be quantified as metrics based rules that represent a combination of software metrics. However, it is difficult to find manually the best threshold values for these metrics. In this paper, we propose a new approach to identify design defects at the model level using the {ID}3 decision tree algorithm. We aim to create a decision tree for each defect. We experimented our approach on four design defects: The Blob, Data class, Lazy class and Feature Envy defect, using 15 Object-Oriented metrics. The rules generated using decision tree give a very promising detection results for the four open source projects tested in this paper. In Lucene 1.4 project, we found that the precision is 67\% for a recall of 100\%. In general, the accuracy varies from 49\%, reaching for Lucene 1.4 project 80\%.},
	pages = {71606--71614},
	journaltitle = {{IEEE} Access},
	author = {Maddeh, Mohamed and Ayouni, Sarra and Alyahya, Sultan and Hajjej, Fahima},
	date = {2021},
	keywords = {Predictive models, Software, Decision trees, Anti-patterns, Measurement, Software algorithms, Object oriented modeling, Unified modeling language, bad smells, decision tree, model refactoring, object oriented metrics, xyes},
	file = {Full Text:C\:\\Users\\michalm\\Zotero\\storage\\VI9T3DA7\\Maddeh et al. - 2021 - Decision tree-based Design Defects Detection.pdf:application/pdf}
}

@inproceedings{yang_future_2019,
	title = {The Future of Broadband Access Network Architecture and Intelligent Operations},
	doi = {10.1109/CyberC.2019.00060},
	abstract = {This paper presents an overview of the evolution towards a broadband optical access network. Network operations transformation with emphasis on automated broadband service monitoring, fault detection, fault recovery, and fault prediction is also discussed. However, in order to support broadband access network evolution and to accelerate telco operations transformation, traditional network operations, administration and maintenance ({OAM}) methods will not be sufficient. Therefore, we describe our key research contributions regarding new approaches to support closed-loop broadband network {OAM} that will eliminate human touches and automate non-physical fault recovery for daily routine tasks. We have designed and implemented a network knowledge engine ({NKE}) centered software framework to enable data collection, correlation, and analysis for troubleshooting complex issues in the broadband access network. Our solutions will address the following network management domains, including (a) knowledge management, (b) surveillance management, (c) incident management, and (d) problem management. In this paper, we focus on auto proactive and predictive trouble management which can verify and confirm a passive device problem location by {NKE}. We will describe a step-by-step methodology to control data process flow, optical fiber path discovery management, and auto-detection of failed passive devices by using the following techniques: (a) big data analytics, (b) knowledge graph, (c) machine learning ({ML}), and (d) artificial intelligence ({AI}) prediction rule and learning policy involving the spatial-temporal algorithm. With combinations of these technique, our preliminary study indicates that the accuracy of auto fault detection is 93\%, while the false positive rate is 0.01\%. As results, we demonstrated the promise of this novel technology framework in supporting transformation from expert's domain knowledge into machine knowledge.},
	pages = {308--316},
	booktitle = {2019 International Conference on Cyber-Enabled Distributed Computing and Knowledge Discovery ({CyberC})},
	author = {Yang, Charlie Chen-Yui and Li, Guangzhi and Liu, Xiang and Wu, Zonghuan and Zhang, Kaiyu},
	date = {2019-10},
	keywords = {Machine learning, Fault detection, Advanced Analytics, Big data, Broad-band access networks, Broadband networks, Computer programming, Computer system recovery, Data Analytics, False positive rates, Flow graphs, Incident Management, Intelligent operations, Knowledge graphs, Knowledge management, Network architecture, Network operations, Optical access networks, Optical fibers, Software frameworks, Wireless communication, Bandwidth, Big Data Analytics, Broadband Access Network, Broadband communication, Cable {TV}, Coaxial cables, Knowledge engineering, Knowledge Graph, {ML} and {AI}, Telephone sets, xno}
}

@inproceedings{mutlu_end--end_2018,
	title = {End-to-End Hierarchical Fuzzy Inference Solution},
	doi = {10.1109/FUZZ-IEEE.2018.8491481},
	abstract = {Hierarchical Fuzzy System ({HFS}) is a popular approach for handling curse of dimensionality problem occurred in complex fuzzy rule-based systems with various and numerous inputs. However, the processes of modeling and reasoning of {HFS} have some critical issues to be considered. In this study, the effect of these issues on the accuracy and stability of the resulting system has been investigated, and an end-to-end {HFS} framework has been proposed. The proposed framework has three main steps such as single system modeling, rule partitioning and {HFS} reasoning. It is fully automated, generic, almost independent from data, and applicable for any kind of inference problem. In addition, the proposed framework preserves accuracy and stability during the {HFS} reasoning. These judgments have been ensured by a number of experimental studies on several datasets about software faulty prediction ({SFP}) problem with a large feature space. The main contributions of this paper are as follows: (i) it provides the entire {HFS} implementation from problem definition to calculation of final output, (ii) it increases the accuracy of recently proposed rule generation scheme in the literature, (iii) it presents the only possible fuzzy system solution for {SFP} problem containing a large feature space with reasonable accuracy.},
	pages = {1--9},
	booktitle = {2018 {IEEE} International Conference on Fuzzy Systems ({FUZZ}-{IEEE})},
	author = {Mutlu, Begum and Sezer, Ebru A. and Ali Akcayol, M.},
	date = {2018-07},
	keywords = {Fuzzy inference, Fuzzy systems, Fuzzy sets, Decision making, Critical issues, Curse of dimensionality, Fully automated, Hierarchical fuzzy, Hierarchical fuzzy systems, Hierarchical systems, Inference problem, Problem definition, Reasonable accuracy, Cognition, Production, Hafnium, Stability analysis, xyes}
}

@inproceedings{basuki_online_2018,
	title = {Online Dissolved Gas Analysis of Power Transformers Based on Decision Tree Model},
	doi = {10.1109/ICPERE.2018.8739761},
	abstract = {This paper presents the possibility of using one of machine learning model, decision tree with C4.5 algorithm for gas interpretation in online condition monitoring and diagnostic application of power transformers. Decision tree selection is based on the best learning outcomes of machine learning software ({WEKA} and Orange) compared to naïve Bayes, neural network, nearest neighbour and support vector machine models. The decision tree was built from 715 data, 7 attributes of gas and 9 types of fault which were cleaned by interquartile range method become 471 data. Evaluation result based on correction prediction are 95.54\% using data training, 88.32\% using cross validation and 87.23\% using 10\% random data from data training. The decision tree rule was implemented in online condition monitoring and diagnostic of power transformer which is integrated into {SCADA} system. This implementation result can predict transformers fault from gas values by online better than conventional {DGA} methods.},
	pages = {1--6},
	booktitle = {2018 Conference on Power Engineering and Renewable Energy ({ICPERE})},
	author = {Basuki, Arief and {Suwarno}},
	date = {2018-10},
	keywords = {Decision trees, Machine learning, Condition monitoring, Machine Learning, Machine learning algorithms, Discharges (electric), C4.5 Algorithm, Decision Tree, Dissolved Gas Analysis, Oil insulation, Power transformers, {SCADA}, xno}
}

@inproceedings{jafri_outlier_2018,
	title = {Outlier Detection in {WSN}},
	doi = {10.1109/ICICT43934.2018.9034286},
	abstract = {In Wireless Sensor Network ({WSN}), anomaly detection is a consequential challenge for tasks like fault detection, intrusion detection and supervising applications. The sensor nodes with constrained resources execute freely for collaborating and managing the network of wireless via which the amassment and transferring of raw data is considered towards the decision makers or the terminus users. This network could be utilized in challenging applications like home automation, health monitoring system, fire detection system and enemy target monitoring and so on in which there is a dependency on {WSN}. These types of applications have precise and reliable data. {WSN} could be incognizant to the anomalies that occur due to less costly hardware and software and non-operative Operating system ({OS}) that may affect the communication of network. The hybrid algorithms have been developed for anomaly detection that considers the intrinsic limits of wireless sensor networks in their development so that the energy consumption for the wireless sensor nodes is minimized and the throughput of the {WSN} is maximized during the simulation. Hamamoto et.al has also utilized Genetic Algorithm for signature generation and fuzzy logic to generate rule sets for the anomaly detection and the problems are identified in Hamamoto et al. research work. Generation of signature in each iteration may consume a lot of time. For large networks, the number of rules will be very high. The problem of this work is the removal of the problems occurred in Hamamoto using hybrid algorithm predicated on Genetic Algorithm and Artificial Neural Network. According to the fitness function, we will optimize the property of each node those are involved in the simulation of {WSN}. So, we design a novel fitness function for Genetic Algorithm ({GA}) and using Artificial Neural Network ({ANN}) as a classifier to detect the anomaly nodes. The simulation would be executed in {MATLAB} and for the authentication of the work; various performance parameters like Accuracy, Error Rate, True positive rate and False positive rate, Throughput and Energy Consumption will be calculated.},
	pages = {236--241},
	booktitle = {2018 3rd International Conference on Inventive Computation Technologies ({ICICT})},
	author = {Jafri, Rana and Kumar, Rakesh},
	date = {2018-11},
	keywords = {Genetic algorithms, Iterative methods, {MATLAB}, Hardware and software, Fault detection, Support vector machines, False positive rates, Health, Fuzzy logic, Neural networks, Anomaly detection, Constrained resources, Decision making, Energy utilization, Fire detection systems, Fire detectors, Gallium, Health monitoring system, Intrusion detection, Monitoring, Performance parameters, Sensor nodes, Signature generation, Wireless sensor node, Wireless sensor networks, {ANN}, Anomaly Detection, Artificial neural networks, {GA}, Neurons, Telecommunication traffic, {WSN}, xno}
}