
@article{mhawish_predicting_2020,
	title = {Predicting Code Smells and Analysis of Predictions: Using Machine Learning Techniques and Software Metrics},
	volume = {35},
	issn = {1000-9000, 1860-4749},
	url = {http://link.springer.com/10.1007/s11390-020-0323-7},
	doi = {10.1007/s11390-020-0323-7},
	shorttitle = {Predicting Code Smells and Analysis of Predictions},
	pages = {1428--1445},
	number = {6},
	journaltitle = {Journal of Computer Science and Technology},
	shortjournal = {J. Comput. Sci. Technol.},
	author = {Mhawish, Mohammad Y. and Gupta, Manjari},
	urldate = {2021-07-04},
	date = {2020-11},
	langid = {english},
	keywords = {xyes}
}

@article{krishna_learning_2020,
	title = {Learning actionable analytics from multiple software projects},
	volume = {25},
	issn = {1382-3256, 1573-7616},
	url = {http://link.springer.com/10.1007/s10664-020-09843-6},
	doi = {10.1007/s10664-020-09843-6},
	pages = {3468--3500},
	number = {5},
	journaltitle = {Empirical Software Engineering},
	shortjournal = {Empir Software Eng},
	author = {Krishna, Rahul and Menzies, Tim},
	urldate = {2021-07-04},
	date = {2020-09},
	langid = {english},
	keywords = {xyes},
	file = {Submitted Version:C\:\\Users\\michalm\\Zotero\\storage\\PBNRDJEQ\\Krishna and Menzies - 2020 - Learning actionable analytics from multiple softwa.pdf:application/pdf}
}

@article{kaur_sp-j48_2020,
	title = {{SP}-J48: a novel optimization and machine-learning-based approach for solving complex problems: special application in software engineering for detecting code smells},
	volume = {32},
	issn = {0941-0643, 1433-3058},
	url = {http://link.springer.com/10.1007/s00521-019-04175-z},
	doi = {10.1007/s00521-019-04175-z},
	shorttitle = {{SP}-J48},
	pages = {7009--7027},
	number = {11},
	journaltitle = {Neural Computing and Applications},
	shortjournal = {Neural Comput \& Applic},
	author = {Kaur, Amandeep and Jain, Sushma and Goel, Shivani},
	urldate = {2021-07-04},
	date = {2020-06},
	langid = {english},
	keywords = {xyes}
}

@article{higo_ammonia_2020,
	title = {Ammonia: an approach for deriving project-specific bug patterns},
	volume = {25},
	issn = {1382-3256, 1573-7616},
	url = {http://link.springer.com/10.1007/s10664-020-09807-w},
	doi = {10.1007/s10664-020-09807-w},
	shorttitle = {Ammonia},
	abstract = {Abstract
            Finding and fixing buggy code is an important and cost-intensive maintenance task, and static analysis ({SA}) is one of the methods developers use to perform it. {SA} tools warn developers about potential bugs by scanning their source code for commonly occurring bug patterns, thus giving those developers opportunities to fix the warnings (potential bugs) before they release the software. Typically, {SA} tools scan for general bug patterns that are common to any software project (such as null pointer dereference), and not for project specific patterns. However, past research has pointed to this lack of customizability as a severe limiting issue in {SA}. Accordingly, in this paper, we propose an approach called , which is based on statically analyzing changes across the development history of a project, as a means to identify project-specific bug patterns. Furthermore, the bug patterns identified by our tool do not relate to just one developer or one specific commit, they reflect the project as a whole and compliment the warnings from other {SA} tools that identify general bug patterns. Herein, we report on the application of our implemented tool and approach to four Java projects: , , , and . The results obtained show that our tool could detect 19 project specific bug patterns across those four projects. Next, through manual analysis, we determined that six of those change patterns were actual bugs and submitted pull requests based on those bug patterns. As a result, five of the pull requests were merged.},
	pages = {1951--1979},
	number = {3},
	journaltitle = {Empirical Software Engineering},
	shortjournal = {Empir Software Eng},
	author = {Higo, Yoshiki and Hayashi, Shinpei and Hata, Hideaki and Nagappan, Meiyappan},
	urldate = {2021-07-04},
	date = {2020-05},
	langid = {english},
	keywords = {xyes},
	file = {Full Text:C\:\\Users\\michalm\\Zotero\\storage\\L9XS95KH\\Higo et al. - 2020 - Ammonia an approach for deriving project-specific.pdf:application/pdf}
}

@article{bhushan_classifying_2021,
	title = {Classifying and resolving software product line redundancies using an ontological first-order logic rule based method},
	volume = {168},
	issn = {09574174},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099231625&doi=10.1016%2fj.eswa.2020.114167&partnerID=40&md5=d3d7a804f51c4eeecd20365e7e7d4ad4},
	doi = {10.1016/j.eswa.2020.114167},
	abstract = {Software product line engineering improves software quality and diminishes development cost and time by efficiently developing software products. Its success lies in identifying the commonalities and variabilities of a set of software products which are generally modeled using feature models. The success of software product lines heavily relies upon the quality of feature models to derive high quality products. However, there are various defects that reduce profits of software product line. One of such defect is redundancy. While the majority of research work focuses on the identification of redundancies, their causes and corrections have been poorly explored. Causes and corrections must be as accurate and comprehensible as possible in order to support the developer in resolving the cause of a redundancy. This research work classified redundancies in the form of a typology. An ontological first-order logic rule based method is proposed to deal with redundancies. A two-step process is presented for mapping model to ontology based on predicate logic. First-order logic based rules are developed and applied to the generated ontology for identifying redundancies, their causes and corrections to resolve redundancies. The proposed method is illustrated using a case study from software product lines online tools repository. The results of experiments performed on 35 models with varied sizes of real world models as well as automatically-generated models from the Software Product Line Online Tools repository and models created via {FeatureIDE} tool conclude that the method is accurate, efficient and scalable with {FM} up to 30,000 features. Thus, enables deriving redundancy free end products from the product line and ultimately, improves its quality. © 2020 Elsevier Ltd},
	journaltitle = {Expert Systems with Applications},
	author = {Bhushan, M. and Ángel Galindo Duarte, J. and Samant, P. and Kumar, A. and Negi, A.},
	date = {2021},
	note = {Publisher: Elsevier Ltd},
	keywords = {Automatically generated, Computer circuits, Computer software selection and evaluation, Cost engineering, Defects, Development costs, Engineering research, First order logic, Formal logic, High-quality products, Ontology, Redundancy, Software design, Software Product Line, Software product line engineerings, Software products, Software quality, Two-step process, xyes}
}

@article{mahmood_mining_2021,
	title = {Mining software repository for cleaning bugs using data mining technique},
	volume = {69},
	issn = {15462218},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107792741&doi=10.32604%2fcmc.2021.016614&partnerID=40&md5=5f9bc74622b43439b6c231bbc7001e10},
	doi = {10.32604/cmc.2021.016614},
	abstract = {Despite advances in technological complexity and efforts, software repository maintenance requires reusing the data to reduce the effort and complexity. However, increasing ambiguity, irrelevance, and bugs while extracting similar data during software development generate a large amount of data from those data that reside in repositories. Thus, there is a need for a repository mining technique for relevant and bug-free data prediction. This paper proposes a fault prediction approach using a data-mining technique to find good predictors for high-quality software. To predict errors in mining data, the Apriori algorithm was used to discover association rules by fixing confidence at more than 40\% and support at least 30\%. The pruning strategy was adopted based on evaluation measures. Next, the rules were extracted from three projects of different domains; the extracted rules were then combined to obtain the most popular rules based on the evaluation measure values. To evaluate the proposed approach, we conducted an experimental study to compare the proposed rules with existing ones using four different industrial projects. The evaluation showed that the results of our proposal are promising. Practitioners and developers can utilize these rules for defect prediction during early software development. © 2021 Tech Science Press. All rights reserved.},
	pages = {873--893},
	number = {1},
	journaltitle = {Computers, Materials and Continua},
	author = {Mahmood, N. and Hafeez, Y. and Iqbal, K. and Hussain, S. and Aqib, M. and Jamal, M. and Song, O.-Y.},
	date = {2021},
	note = {Publisher: Tech Science Press},
	keywords = {Data mining, Software design, Forecasting, Apriori algorithms, Different domains, Evaluation measures, High-quality software, Industrial projects, Mining software repositories, Program debugging, Software repositories, Technological complexity, xyes}
}

@article{baum_gimo_2020,
	title = {{GIMO}: A multi-objective anytime rule mining system to ease iterative feedback from domain experts},
	volume = {8},
	issn = {25901885},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090830401&doi=10.1016%2fj.eswax.2020.100040&partnerID=40&md5=0c2a7764b294fd08cc3c164779f2860d},
	doi = {10.1016/j.eswax.2020.100040},
	abstract = {Data extracted from software repositories is used intensively in Software Engineering research, for example, to predict defects in source code. In our research in this area, with data from open source projects as well as an industrial partner, we noticed several shortcomings of conventional data mining approaches for classification problems: (1) Domain experts’ acceptance is of critical importance, and domain experts can provide valuable input, but it is hard to use this feedback. (2) Evaluating the quality of the model is not a matter of calculating {AUC} or accuracy. Instead, there are multiple objectives of varying importance with hard to quantify trade-offs. Furthermore, the performance of the model cannot be evaluated on a per-instance level in our case, because it shares aspects with the set cover problem. To overcome these problems, we take a holistic approach and develop a rule mining system that simplifies iterative feedback from domain experts and can incorporate the domain-specific evaluation needs. A central part of the system is a novel multi-objective anytime rule mining algorithm. The algorithm is based on the {GRASP}-{PR} meta-heuristic but extends it with ideas from several other approaches. We successfully applied the system in the industrial context. In the current article, we focus on the description of the algorithm and the concepts of the system. We make an implementation of the system available. © 2020 The Authors},
	journaltitle = {Expert Systems with Applications: X},
	author = {Baum, T. and Herbold, S. and Schneider, K.},
	date = {2020},
	note = {Publisher: Elsevier Ltd},
	keywords = {Data mining, Open source software, Software repositories, Rule mining, Conventional data mining, Economic and social effects, Electronic trading, Holistic approach, Industrial context, Industrial partners, Industrial research, Iterative methods, Knowledge acquisition, Mining machinery, Multiple-objectives, Open source projects, Rule mining algorithms, Explainable artificial intelligence, Human-in-the-loop, Interpretable artificial intelligence, Multi-objective, Set cover, xyes},
	file = {Full Text:C\:\\Users\\michalm\\Zotero\\storage\\MQY3F42D\\Baum et al. - 2020 - GIMO A multi-objective anytime rule mining system.pdf:application/pdf;Full Text:C\:\\Users\\michalm\\Zotero\\storage\\RKJXJ5TB\\Baum et al. - 2020 - GIMO A multi-objective anytime rule mining system.pdf:application/pdf}
}

@article{abaei_fuzzy_2020,
	title = {A fuzzy logic expert system to predict module fault proneness using unlabeled data},
	volume = {32},
	issn = {13191578},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053032476&doi=10.1016%2fj.jksuci.2018.08.003&partnerID=40&md5=dc1ca5a715bad9a6287aa6e72f63f14d},
	doi = {10.1016/j.jksuci.2018.08.003},
	abstract = {Several techniques have been proposed to predict the fault proneness of software modules in the absence of fault data. However, the application of these techniques requires an expert assistant and is based on fixed thresholds and rules, which potentially prevents obtaining optimal prediction results. In this study, the development of a fuzzy logic expert system for predicting the fault proneness of software modules is demonstrated in the absence of fault data. The problem of strong dependability with the prediction model for expert assistance as well as deciding on the module fault proneness based on fixed thresholds and fixed rules have been solved in this study. In fact, involvement of experts is more relaxed or provides more support now. Two methods have been proposed and implemented using the fuzzy logic system. In the first method, the Takagi and Sugeno-based fuzzy logic system is developed manually. In the second method, the rule-base and data-base of the fuzzy logic system are adjusted using a genetic algorithm. The second method can determine the optimal values of the thresholds while recommending the most appropriate rules to guide the testing of activities by prioritizing the module's defects to improve the quality of software testing with a limited budget and limited time. Two datasets from {NASA} and the Turkish white-goods manufacturer that develops embedded controller software are used for evaluation. The results based on the second method show improvement in the false negative rate, f-measure, and overall error rate. To obtain optimal prediction results, developers and practitioners are recommended to apply the proposed fuzzy logic expert system for predicting the fault proneness of software modules in the absence of fault data. © 2018 The Authors},
	pages = {684--699},
	number = {6},
	journaltitle = {Journal of King Saud University - Computer and Information Sciences},
	author = {Abaei, G. and Selamat, A. and Al Dallal, J.},
	date = {2020},
	note = {Publisher: King Saud bin Abdulaziz University},
	keywords = {xyes}
}

@article{shao_software_2020,
	title = {Software defect prediction based on correlation weighted class association rule mining},
	volume = {196},
	issn = {09507051},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082463500&doi=10.1016%2fj.knosys.2020.105742&partnerID=40&md5=aafe1f60cfd93765b177edb265fe468d},
	doi = {10.1016/j.knosys.2020.105742},
	abstract = {Software defect prediction based on supervised learning plays a crucial role in guiding software testing for resource allocation. In particular, it is worth noticing that using associative classification with high accuracy and comprehensibility can predict defects. But owing to the imbalance data distribution inherent, it is easy to generate a large number of non-defective class association rules, but the defective class association rules are easily ignored. Furthermore, classical associative classification algorithms mainly measure the interestingness of rules by the occurrence frequency, such as support and confidence, without considering the importance of features, resulting in combinations of the insignificant frequent itemset. This promotes the generation of weighted associative classification. However, the feature weighting based on domain knowledge is subjective and unsuitable for a high dimensional dataset. Hence, we present a novel software defect prediction model based on correlation weighted class association rule mining ({CWCAR}). It leverages a multi-weighted supports-based framework rather than the traditional support-confidence approach to handle class imbalance and utilizes the correlation-based heuristic approach to assign feature weight. Besides, we also optimize the ranking, pruning and prediction stages based on weighted support. Results show that {CWCAR} is significantly superior to state-of-the-art classifiers in terms of Balance, {MCC}, and Gmean. © 2020 Elsevier B.V.},
	journaltitle = {Knowledge-Based Systems},
	author = {Shao, Y. and Liu, B. and Wang, S. and Li, G.},
	date = {2020},
	note = {Publisher: Elsevier B.V.},
	keywords = {Data mining, Defects, Software defect prediction, Forecasting, Apriori, Association rules, Associative classification, Attribute weighting, Class imbalance, Heuristic methods, Software testing, Association rule, xyes}
}

@article{chatterjee_fuzzy_2019,
	title = {A fuzzy rule-based generation algorithm in interval type-2 fuzzy logic system for fault prediction in the early phase of software development},
	volume = {31},
	issn = {0952813X},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058179117&doi=10.1080%2f0952813X.2018.1552315&partnerID=40&md5=39c1d24ec8744497e3521e77af9da471},
	doi = {10.1080/0952813X.2018.1552315},
	abstract = {Reliability, a measure of software, deals in total number of faults count up to a certain period of time. The present study aims at estimating the total number of software faults during the early phase of software life cycle. Such estimation helps in producing more reliable software as there may be a scope to take necessary corrective actions for improving the reliability within optimum time and cost by the software developers. The proposed interval type-2 fuzzy logic-based model considers reliability-relevant software metric and earlier project data as model inputs. Type-2 fuzzy sets have been used to reduce uncertainties in the vague linguistic values of the software metrics. A rule formation algorithm has been developed to overcome inconsistency in the consequent parts of large number of rules. Twenty-six software project data help to validate the model, and a comparison has been provided to analyse the proposed model’s performance. © 2018, © 2018 Informa {UK} Limited, trading as Taylor \& Francis Group.},
	pages = {369--391},
	number = {3},
	journaltitle = {Journal of Experimental and Theoretical Artificial Intelligence},
	author = {Chatterjee, S. and Maji, B. and Pham, H.},
	date = {2019},
	note = {Publisher: Taylor and Francis Ltd.},
	keywords = {Computer circuits, Software design, Software developer, Software reliability, Fuzzy inference, Fuzzy rules, Corrective actions, Early fault, Fuzzy logic, Fuzzy rule base, Generation algorithm, Interval type-2 fuzzy logic, Interval type-2 fuzzy logic systems, Life cycle, Software life cycles, xyes}
}

@article{mori_balancing_2019,
	title = {Balancing the trade-off between accuracy and interpretability in software defect prediction},
	volume = {24},
	issn = {13823256},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050694331&doi=10.1007%2fs10664-018-9638-1&partnerID=40&md5=24406dcf95de7405b06a8ce16df8182c},
	doi = {10.1007/s10664-018-9638-1},
	abstract = {Context: Classification techniques of supervised machine learning have been successfully applied to various domains of practice. When building a predictive model, there are two important criteria: predictive accuracy and interpretability, which generally have a trade-off relationship. In particular, interpretability should be accorded greater emphasis in the domains where the incorporation of expert knowledge into a predictive model is required. Objective: The aim of this research is to propose a new classification model, called superposed naive Bayes ({SNB}), which transforms a naive Bayes ensemble into a simple naive Bayes model by linear approximation. Method: In order to evaluate the predictive accuracy and interpretability of the proposed method, we conducted a comparative study using well-known classification techniques such as rule-based learners, decision trees, regression models, support vector machines, neural networks, Bayesian learners, and ensemble learners, over 13 real-world public datasets. Results: A trade-off analysis between the accuracy and interpretability of different classification techniques was performed with a scatter plot comparing relative ranks of accuracy with those of interpretability. The experiment results show that the proposed method ({SNB}) can produce a balanced output that satisfies both accuracy and interpretability criteria. Conclusions: {SNB} offers a comprehensible predictive model based on a simple and transparent model structure, which can provide an effective way for balancing the trade-off between accuracy and interpretability. © 2018, Springer Science+Business Media, {LLC}, part of Springer Nature.},
	pages = {779--825},
	number = {2},
	journaltitle = {Empirical Software Engineering},
	author = {Mori, T. and Uchihira, N.},
	date = {2019},
	note = {Publisher: Springer New York {LLC}},
	keywords = {Defects, Software defect prediction, Classification (of information), Decision trees, Economic and social effects, Bayesian networks, Classifiers, Ensemble learning, Interpretability, Mathematical transformations, Model approximations, Naive Bayes classifiers, Predictive accuracy, Regression analysis, Supervised learning, Trade-off analysis, Weights of evidences, xyes},
	file = {Mori and Uchihira - 2019 - Balancing the trade-off between accuracy and inter.pdf:C\:\\Users\\michalm\\Zotero\\storage\\MJIVB7Y2\\Mori and Uchihira - 2019 - Balancing the trade-off between accuracy and inter.pdf:application/pdf}
}

@article{juneja_fuzzy-filtered_2019,
	title = {A fuzzy-filtered neuro-fuzzy framework for software fault prediction for inter-version and inter-project evaluation},
	volume = {77},
	issn = {15684946},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061619023&doi=10.1016%2fj.asoc.2019.02.008&partnerID=40&md5=95a51edee31eacae45d0bea5ffbf7d64},
	doi = {10.1016/j.asoc.2019.02.008},
	abstract = {Fault Prediction is the most required measure to estimate the software quality and reliability. Several methods, measures, aspects and testing methodologies are available to evaluate the software fault. In this paper, a fuzzy-filtered neuro-fuzzy framework is introduced to predict the software faults for internal and external software projects. The suggested framework is split into three primary phases. At the earlier phase, the effective metrics or measures are identified, which can derive the accurate decision on prediction of software fault. In this phase, the composite analytical observation of each software attribute is calculated using Information Gain and Gain Ratio measures. In the second phase, these fuzzy rules are applied on these measures for selection of effective and high-impact features. In the last phase, the Neuro-fuzzy classifier is applied on fuzzy-filtered training and testing sets. The proposed framework is applied to identify the software faults based on inter-version and inter-project evaluation. In this framework, the earlier projects or project-versions are considered as training sets and the new projects or versions are taken as testing sets. The experimentation is conducted on nine open source projects taken from {PROMISE} repository as well as on {PDE} and {JDT} projects. The approximation is applied on internal version-specific fault prediction and external software projects evaluation. The comparative analysis is performed against Decision Tree, Random Tree, Random Forest, Naive Bayes and Multilevel Perceptron classifiers. This prediction result signifies that the proposed framework has gained the higher accuracy, lesser error rate and significant {AUC} and {GM} for inter-project and inter-version evaluations. © 2019 Elsevier B.V.},
	pages = {696--713},
	journaltitle = {Applied Soft Computing Journal},
	author = {Juneja, K.},
	date = {2019},
	note = {Publisher: Elsevier Ltd},
	keywords = {Computer software selection and evaluation, Open source software, Forecasting, Classification (of information), Decision trees, Software testing, Project management, Software reliability, Software fault prediction, Defect prediction, Fuzzy inference, Fuzzy systems, Comparative analysis, Fuzzy, Fuzzy sets, Inter project, Intra project, Multi-level perceptron, Neural networks, Neuro fuzzy classifier, xyes}
}

@article{agrawal_cross_2019,
	title = {Cross project defect prediction for open source software},
	issn = {25112104},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085313661&doi=10.1007%2fs41870-019-00299-6&partnerID=40&md5=a9a79b98b2b446f274e92299a20f483d},
	doi = {10.1007/s41870-019-00299-6},
	abstract = {Software defect prediction is the process of identification of defects early in the life cycle so as to optimize the testing resources and reduce maintenance efforts. Defect prediction works well if sufficient amount of data is available to train the prediction model. However, not always this is the case. For example, when the software is the first release or the company has not maintained significant data. In such cases, cross project defect prediction may identify the defective classes. In this work, we have studied the feasibility of cross project defect prediction and empirically validated the same. We conducted our experiments on 12 open source datasets. The prediction model is built using 12 software metrics. After studying the various train test combinations, we found that cross project defect prediction was feasible in 35 out of 132 cases. The success of prediction is determined via precision, recall and {AUC} of the prediction model. We have also analyzed 14 descriptive characteristics to construct the decision tree. The decision tree learnt from this data has 15 rules which describe the feasibility of successful cross project defect prediction. © 2019, Bharati Vidyapeeth's Institute of Computer Applications and Management.},
	journaltitle = {International Journal of Information Technology (Singapore)},
	author = {Agrawal, A. and Malhotra, R.},
	date = {2019},
	note = {Publisher: Springer Science and Business Media B.V.},
	keywords = {xyes}
}

@article{shao_novel_2018,
	title = {A novel software defect prediction based on atomic class-association rule mining},
	volume = {114},
	issn = {09574174},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050891830&doi=10.1016%2fj.eswa.2018.07.042&partnerID=40&md5=2fcaad1a183db92a46af14f4bb2c2b67},
	doi = {10.1016/j.eswa.2018.07.042},
	abstract = {To ensure the rational allocation of software testing resources and reduce costs, software defect prediction has drawn notable attention to many “white-box” and “black-box” classification algorithms. Although there have been lots of studies on using software product metrics to identify defect-prone modules, defect prediction algorithms are still worth exploring. For instance, it is not easy to directly implement the Apriori algorithm to classify defect-prone modules across a skewed dataset. Therefore, we propose a novel supervised approach for software defect prediction based on atomic class-association rule mining ({ACAR}). It holds the characteristics of only one feature of the antecedent and a unique class label of the consequent, which is a specific kind of association rules that explores the relationship between attributes and categories. It holds the characteristics of only one feature of the antecedent and a unique class label of the consequent, which is a specific kind of association rules that explores the relationship between attributes and categories. Such association patterns can provide meaningful knowledge that can be easily understood by software engineers. A new software defect prediction model infrastructure based on association rules is employed to improve the prediction of defect-prone modules, which is divided into data preprocessing, rule model building and performance evaluation. Moreover, {ACAR} can achieve a satisfactory classification performance compared with other seven benchmark learners (the extension of classification based on associations ({CBA}2), Support Vector Machine, Naive Bayesian, Decision Tree, {OneR}, K-nearest Neighbors and {RIPPER}) on {NASA} {MDP} and {PROMISE} datasets. In light of software defect associative prediction, a comparative experiment between {ACAR} and {CBA}2 is discussed in details. It is demonstrated that {ACAR} is better than {CBA}2 in terms of {AUC}, G-mean, Balance, and understandability. In addition, the average {AUC} of {ACAR} is increased by 2.9\% compared with {CBA}2, which can reach 81.1\%. © 2018 Elsevier Ltd},
	pages = {237--254},
	journaltitle = {Expert Systems with Applications},
	author = {Shao, Y. and Liu, B. and Wang, S. and Li, G.},
	date = {2018},
	note = {Publisher: Elsevier Ltd},
	keywords = {Data mining, Defects, {NASA}, Software defect prediction, Forecasting, Learning systems, Classification (of information), Decision trees, Apriori, Association rules, Benchmarking, Classification algorithm, Black-box testing, Class association rules, Classification based on associations, Classification performance, Comparative experiments, Nearest neighbor search, Software product metrics, xyes},
	file = {Shao et al. - 2018 - A novel software defect prediction based on atomic.pdf:C\:\\Users\\michalm\\Zotero\\storage\\YQK7ZUBN\\Shao et al. - 2018 - A novel software defect prediction based on atomic.pdf:application/pdf}
}

@inproceedings{amasaki_cross-version_2018,
	title = {Cross-version defect prediction using cross-project defect prediction approaches: Does it work?},
	isbn = {978-1-4503-6593-2},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056731345&doi=10.1145%2f3273934.3273938&partnerID=40&md5=4e9a55dd5411a2ac5b51ea8893ac4538},
	doi = {10.1145/3273934.3273938},
	abstract = {Background: Specifying and removing defects before release deserve extra cost for the success of software projects. Long-running projects experience multiple releases, and it is a natural choice to adopt cross-version defect prediction ({CVDP}) that uses information from older versions. A past study shows that feeding multi older versions data may have a positive influence on the performance. The study also suggests that cross-project defect prediction ({CPDP}) may fit the situation but one {CPDP} approach was only examined. Aims: To investigate whether feeding multiple older versions data is effective for {CVDP} using {CPDP} approaches. The investigation also involves performance comparisons of the {CPDP} approaches under {CVDP} situation. Method: We chose a style of replication of the comparative study on {CPDP} approaches by Herbold et al. under {CVDP} situation. Results: Feeding multiple older versions had a positive effect for more than a half {CPDP} approaches. However, almost all of the {CPDP} approaches did not perform significantly better than a simple rule-based prediction. Although the best {CPDP} approach could work better than it and with-in project defect prediction, we found no effect of feeding multiple older versions for it. Conclusions: Feeding multiple older versions could improve {CPDP} approaches under {CVDP} situation. However, it did not work for the best {CPDP} approach in the study. © 2018 Association for Computing Machinery.},
	pages = {32--41},
	booktitle = {{ACM} International Conference Proceeding Series},
	publisher = {Association for Computing Machinery},
	author = {Amasaki, S.},
	date = {2018},
	keywords = {Defects, Forecasting, Predictive analytics, Software engineering, Defect prediction, Comparative studies, Feeding, Multiple release, Performance comparison, Rule based, Software project, xyes}
}

@article{pattnaik_empirical_2018,
	title = {Empirical analysis of software quality prediction using a {TRAINBFG} algorithm},
	volume = {7},
	issn = {2227524X},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067271067&partnerID=40&md5=bbf61e10f0de3da8953415cd3b3f9aaf},
	abstract = {Software quality plays a major role in software fault proneness. That's why prediction of software quality is essential for measuring the anticipated faults present in the software. In this paper we have proposed a Neuro-Fuzzy model for prediction of probable values for a predefined set of software characteristics by virtue of using a rule base. In course of it, we have used several training algorithms among which {TRAINBFG} algorithm is observed to be the best one for the purpose. There are various training algorithm available in {MATLAB} for training the neural network input data set. The prediction using fuzzy logic and neural network provides better result in comparison with only neural network. We find out from our implementation that {TRAINBFG} algorithm can provide better predicted value as com-pared to other algorithm in {MATLAB}. We have validated this result using the tools like {SPSS} and {MATLAB}. © 2018, Science Publishing Corporation Inc.},
	pages = {259--268},
	number = {2},
	journaltitle = {International Journal of Engineering and Technology({UAE})},
	author = {Pattnaik, S. and Pattanayak, B.K.},
	date = {2018},
	note = {Publisher: Science Publishing Corporation Inc},
	keywords = {xyes}
}

@article{dhanalaxmi_rule-based_2017,
	title = {A rule-based prediction method for defect detection in software system},
	volume = {95},
	issn = {19928645},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026770283&partnerID=40&md5=5b4b377b1bb3a7cec928c6c88d3fbef8},
	abstract = {Software is a complex object that consists of different modules with changing degrees of defect occurrence. By efficiently and appropriate predicting the frequency of defects in software, software project managers can better utilize their workforce, cost and time to obtain better quality assurance. This paper proposes a rule-based prediction ({RBP}) method for defect detection and for planing the better maintenance strategy, which can support in the forecast a defective or non-defective software module before it can deploy for any software project. The {RBP} extends the Ripple-down rule ({RDR}) classifier method to construct an effective rule-basedmodel for accurately classifying the software defects. The method will enhance the software defect prediction so that software testers can spend more time in testing those components which are expected to contain errors. The experiment evaluation is performed over a software repository datasets and the obtained results showa satisfactory improvement. © 2005 – ongoing {JATIT} \& {LLS}.},
	pages = {3403--3412},
	number = {14},
	journaltitle = {Journal of Theoretical and Applied Information Technology},
	author = {Dhanalaxmi, B. and Appa Rao Naidu, G. and Anuradha, K.},
	date = {2017},
	note = {Publisher: Asian Research Publishing Network},
	keywords = {xyes}
}

@article{goyal_fuzzy_2017,
	title = {Fuzzy inferencing to identify degree of interaction in the development of fault prediction models},
	volume = {29},
	issn = {13191578},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006791416&doi=10.1016%2fj.jksuci.2014.12.008&partnerID=40&md5=3e0f7210b23221ef38cf0b4799ed0bdf},
	doi = {10.1016/j.jksuci.2014.12.008},
	abstract = {The software fault prediction models, based on different modeling techniques have been extensively researched to improve software quality for the last three decades. Out of the analytical techniques used by the researchers, fuzzy modeling and its variants are bringing out a major share of the attention of research communities. In this work, we demonstrate the models developed through data driven fuzzy inference system. A comprehensive set of rules induced by such an inference system, followed by a simplification process provides deeper insight into the linguistically identified level of interaction. This work makes use of a publicly available data repository for four software modules, advocating the consideration of compound effects in the model development, especially in the area of software measurement. One related objective is the identification of influential metrics in the development of fault prediction models. A fuzzy rule intrinsically represents a form of interaction between fuzzified inputs. Analysis of these rules establishes that Low and {NOT} (High) level of inheritance based metrics significantly contributes to the F-measure estimate of the model. Further, the Lack of Cohesion of Methods ({LCOM}) metric was found insignificant in this empirical study. © 2015 The Authors},
	pages = {93--102},
	number = {1},
	journaltitle = {Journal of King Saud University - Computer and Information Sciences},
	author = {Goyal, R. and Chandra, P. and Singh, Y.},
	date = {2017},
	note = {Publisher: King Saud bin Abdulaziz University},
	keywords = {Software fault prediction, Object oriented metrics, Fuzzy inference system, Influential metrics, xyes}
}

@article{chatterjee_new_2016,
	title = {A new fuzzy rule based algorithm for estimating software faults in early phase of development},
	volume = {20},
	issn = {14327643},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84932188500&doi=10.1007%2fs00500-015-1738-x&partnerID=40&md5=7befbdbcf21dcc50b99c0587eaeacc49},
	doi = {10.1007/s00500-015-1738-x},
	abstract = {Estimation of reliability and the number of faults present in software in its early development phase, i.e., requirement analysis or design phase is very beneficial for developing reliable software with optimal cost. Software reliability prediction in early phase of development is highly desirable to the stake holders, software developers, managers and end users. Since, the failure data are unavailable in early phase of software development, different reliability relevant software metrics and similar project data are used to develop models for early software fault prediction. The proposed model uses the linguistic values of software metrics in fuzzy inference system to predict the total number of faults present in software in its requirement analysis phase. Considering specific target reliability, weightage of each input software metrics and size of software, an algorithm has been proposed here for developing general fuzzy rule base. For model validation of the proposed model, 20 real software project data have been used here. The linguistic values from four software metrics related to requirement analysis phase have been considered as model inputs. The performance of the proposed model has been compared with two existing early software fault prediction models. © 2015, Springer-Verlag Berlin Heidelberg.},
	pages = {4023--4035},
	number = {10},
	journaltitle = {Soft Computing},
	author = {Chatterjee, S. and Maji, B.},
	date = {2016},
	note = {Publisher: Springer Verlag},
	keywords = {Software metrics, Software design, Forecasting, Reliability analysis, Software reliability, Computer software, Fuzzy inference, Fuzzy rules, Fuzzy systems, Early fault, Fuzzy rule base, Fuzzy inference systems, Reliability, Algorithms, Computational linguistics, Linguistics, Requirement analysis, xyes},
	file = {Chatterjee and Maji - 2016 - A new fuzzy rule based algorithm for estimating so.pdf:C\:\\Users\\michalm\\Zotero\\storage\\X46IHY9V\\Chatterjee and Maji - 2016 - A new fuzzy rule based algorithm for estimating so.pdf:application/pdf}
}

@article{czibula_detecting_2015,
	title = {Detecting software design defects using relational association rule mining},
	volume = {42},
	issn = {02191377},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891894965&doi=10.1007%2fs10115-013-0721-z&partnerID=40&md5=b53495712645fce717363a3a8717fdb7},
	doi = {10.1007/s10115-013-0721-z},
	abstract = {In this paper, we are approaching, from a machine learning perspective, the problem of automatically detecting defective software entities (classes and methods) in existing software systems, a problem of major importance during software maintenance and evolution. In order to improve the internal quality of a software system, identifying faulty entities such as classes, modules, methods is essential for software developers. As defective software entities are hard to identify, machine learning-based classification models are still developed to approach the problem of detecting software design defects. We are proposing a novel method based on relational association rule mining for detecting faulty entities in existing software systems. Relational association rules are a particular type of association rules and describe numerical orderings between attributes that commonly occur over a dataset. Our method is based on the discovery of relational association rules for identifying design defects in software. Experiments on open source software are conducted in order to detect defective classes in object-oriented software systems, and a comparison of our approach with similar existing approaches is provided. The obtained results show that our method is effective for software design defect detection and confirms the potential of our proposal. © 2014, Springer-Verlag London.},
	pages = {545--577},
	number = {3},
	journaltitle = {Knowledge and Information Systems},
	author = {Czibula, G. and Marian, Z. and Czibula, I.G.},
	date = {2015},
	note = {Publisher: Springer London},
	keywords = {Data mining, Defects, Software design, Open source software, Turing machines, Learning systems, Object oriented programming, Machine learning, Open systems, Association rules, Software developer, Object detection, Software systems, Software entities, Classification models, Defect detection, Internal quality, Object-oriented software systems, Software maintenance and evolution, xyes}
}

@article{jin_novel_2015,
	title = {A novel rule base representation and its inference method using the evidential reasoning approach},
	volume = {87},
	issn = {09507051},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992236535&doi=10.1016%2fj.knosys.2015.06.018&partnerID=40&md5=f718eef9dceaa05f257e054f66c9037f},
	doi = {10.1016/j.knosys.2015.06.018},
	abstract = {In this paper, a novel rule base, Certainty Rule Base ({CeRB}), and its inference method are proposed. This rule base is firstly designed with certainty degrees embedded in the antecedent terms as well as in the consequent terms. {CeRB} is shown to be capable of capturing vagueness, incompleteness, uncertainty, and nonlinear causal relationships in an integrated way. Secondly, the {CeRB} inference method using the evidential reasoning approach is provided. The overall representation and inference framework offer a further improvement and a great extension of the recently uncertainty inference methods. Namely, the knowledge is represented by {CeRB} and the evidential reasoning approach is applied to the rule combination. In the end, two case studies including a numerical example and a software defect prediction are provided to illustrate the proposed {CeRB} representation, generation and inference procedure as well as demonstrate its high performance by comparing with some existing approaches. © 2015 Elsevier B.V.},
	pages = {80--91},
	journaltitle = {Knowledge-Based Systems},
	author = {Jin, L. and Liu, J. and Xu, Y. and Fang, X.},
	date = {2015},
	note = {Publisher: Elsevier},
	keywords = {Software engineering, Artificial intelligence, Knowledge based systems, Evidential reasoning approaches, Inference methods, Representation of knowledge, Rule base, Similarity measure, xyes},
	file = {Jin et al. - 2015 - A novel rule base representation and its inference.pdf:C\:\\Users\\michalm\\Zotero\\storage\\HYC49AJY\\Jin et al. - 2015 - A novel rule base representation and its inference.pdf:application/pdf}
}

@article{han_residual_2015,
	title = {Residual defect prediction using multiple technologies},
	volume = {10},
	issn = {19750080},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941051136&doi=10.14257%2fijmue.2015.10.8.01&partnerID=40&md5=65afabcbb0453293432c2e7f2e6543b3},
	doi = {10.14257/ijmue.2015.10.8.01},
	abstract = {Finding defects in a software system is not easy. Effective detection of software defects is an important activity of software development process. In this paper, we propose an approach to predict residual defects, which applies machine learning algorithms (classifiers) and defect distribution model. This approach includes two steps. Firstly, use machine learning Algorithms and Association Rules to get defect classification table, then confirm the defect distribution trend referring to several distribution models. Experiment results on a {GUI} project show that the approach can effectively improve the accuracy of defect prediction and be used for test planning and implementation. © 2015 {SERSC}.},
	pages = {1--12},
	number = {8},
	journaltitle = {International Journal of Multimedia and Ubiquitous Engineering},
	author = {Han, W.-J. and Jiang, L.-X. and Lu, T.-B. and Zhang, X.-Y.},
	date = {2015},
	note = {Publisher: Science and Engineering Research Support Society},
	keywords = {Defects, Software design, Software defects, Forecasting, Learning systems, Software engineering, Artificial intelligence, Defect prediction, Learning algorithms, Classifiers, Defect distribution, Software development process, Defect classification, Distribution models, Multiple technology, Residual defects, xyes}
}

@article{han_software_2015,
	title = {Software defect model based on similarity and association rule},
	volume = {10},
	issn = {19750080},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938590184&doi=10.14257%2fijmue.2015.10.7.01&partnerID=40&md5=0ec914701879865dc39b740ae967168d},
	doi = {10.14257/ijmue.2015.10.7.01},
	abstract = {In order to detect defects efficiently and improve the quality of products, this paper puts forward the concept about defect classification model and defect association model by a lot of defect data. The technology of similarity is applied to defect classification model, and the idea of Knowledge Discovery in Database is applied to defect association model. Defect classification model can analyze the defect efficiently and provides the basis of solving problems quickly while defect association model can be used to detect early and prevent problem, which can make effective improvements for testing and development. This paper summed up {GUI} defect model based on a large number of interface defects. The model is useful to improve the accuracy of forecast and be used for test planning and implementation through the practice of several projects. © 2015 {SERSC}.},
	pages = {1--10},
	number = {7},
	journaltitle = {International Journal of Multimedia and Ubiquitous Engineering},
	author = {Han, W.J. and Jiang, H.Y. and Lu, T.B. and Zhang, X.Y. and Li, W.},
	date = {2015},
	note = {Publisher: Science and Engineering Research Support Society},
	keywords = {Defects, Software defects, Classification (of information), Defect model, Association rules, Defect classification, Defect associations, Interface defects, Knowledge discovery in database, Quality of product, Similarity, xyes}
}

@article{moeyersoms_comprehensible_2015,
	title = {Comprehensible software fault and effort prediction: A data mining approach},
	volume = {100},
	issn = {01641212},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919361139&doi=10.1016%2fj.jss.2014.10.032&partnerID=40&md5=2eab3043a0a68f06cf9b0418d024f57f},
	doi = {10.1016/j.jss.2014.10.032},
	abstract = {Software fault and effort prediction are important tasks to minimize costs of a software project. In software effort prediction the aim is to forecast the effort needed to complete a software project, whereas software fault prediction tries to identify fault-prone modules. In this research both tasks are considered, thereby using different data mining techniques. The predictive models not only need to be accurate but also comprehensible, demanding that the user can understand the motivation behind the model's prediction. Unfortunately, to obtain predictive performance, comprehensibility is often sacrificed and vice versa. To overcome this problem, we extract trees from well performing Random Forests ({RFs}) and Support Vector Machines for regression ({SVRs}) making use of a rule extraction algorithm {ALPA}. This method builds trees (using C4.5 and {REPTree}) that mimic the black-box model ({RF}, {SVR}) as closely as possible. The proposed methodology is applied to publicly available datasets, complemented with new datasets that we have put together based on the Android repository. Surprisingly, the trees extracted from the black-box models by {ALPA} are not only comprehensible and explain how the black-box model makes (most of) its predictions, but are also more accurate than the trees obtained by working directly on the data. © 2014 Elsevier Inc. All rights reserved.},
	pages = {80--90},
	journaltitle = {Journal of Systems and Software},
	author = {Moeyersoms, J. and Junqué De Fortuny, E. and Dejaeger, K. and Baesens, B. and Martens, D.},
	date = {2015},
	note = {Publisher: Elsevier Inc.},
	keywords = {Data mining, Forecasting, Decision trees, Forestry, Effort prediction, Extraction, Software fault prediction, Computer software, Rule extraction, Comprehensibility, Computer Programs, Data Processing, Fault-prone modules, Predictive performance, Rule extraction algorithms, Software effort prediction, Trees, Software fault and effort prediction, xyes},
	file = {Accepted Version:C\:\\Users\\michalm\\Zotero\\storage\\VYDKZISE\\Moeyersoms et al. - 2015 - Comprehensible software fault and effort predictio.pdf:application/pdf}
}

@article{rodriguez_study_2013,
	title = {A study of subgroup discovery approaches for defect prediction},
	volume = {55},
	issn = {09505849},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880777880&doi=10.1016%2fj.infsof.2013.05.002&partnerID=40&md5=4bd143673df3bbb32127deb28c13f397},
	doi = {10.1016/j.infsof.2013.05.002},
	abstract = {Context Although many papers have been published on software defect prediction techniques, machine learning approaches have yet to be fully explored. Objective In this paper we suggest using a descriptive approach for defect prediction rather than the precise classification techniques that are usually adopted. This allows us to characterise defective modules with simple rules that can easily be applied by practitioners and deliver a practical (or engineering) approach rather than a highly accurate result. Method We describe two well-known subgroup discovery algorithms, the {SD} algorithm and the {CN}2-{SD} algorithm to obtain rules that identify defect prone modules. The empirical work is performed with publicly available datasets from the Promise repository and object-oriented metrics from an Eclipse repository related to defect prediction. Subgroup discovery algorithms mitigate against characteristics of datasets that hinder the applicability of classification algorithms and so remove the need for preprocessing techniques. Results The results show that the generated rules can be used to guide testing effort in order to improve the quality of software development projects. Such rules can indicate metrics, their threshold values and relationships between metrics of defective modules. Conclusions The induced rules are simple to use and easy to understand as they provide a description rather than a complete classification of the whole dataset. Thus this paper represents an engineering approach to defect prediction, i.e., an approach which is useful in practice, easily understandable and can be applied by practitioners. © 2013 Elsevier B.V. All rights reserved.},
	pages = {1810--1822},
	number = {10},
	journaltitle = {Information and Software Technology},
	author = {Rodriguez, D. and Ruiz, R. and Riquelme, J.C. and Harrison, R.},
	date = {2013},
	keywords = {Defects, Software defect prediction, Forecasting, Classification (of information), Defect prediction, Machine learning approaches, Algorithms, Imbalanced Data-sets, Classification technique, Rules, Preprocessing techniques, Subgroup discovery, xyes},
	file = {Rodriguez et al. - 2013 - A study of subgroup discovery approaches for defec.pdf:C\:\\Users\\michalm\\Zotero\\storage\\H3YLINFQ\\Rodriguez et al. - 2013 - A study of subgroup discovery approaches for defec.pdf:application/pdf}
}

@article{najadat_enhance_2012,
	title = {Enhance rule based detection for software fault prone modules},
	volume = {6},
	issn = {17389984},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859858705&partnerID=40&md5=0c2be465f65ed4ded20fcb863ceac2ec},
	abstract = {Software quality assurance is necessary to increase the level of confidence in the developed software and reduce the overall cost for developing software projects. The problem addressed in this research is the prediction of fault prone modules using data mining techniques. Predicting fault prone modules allows the software managers to allocate more testing and resources to such modules. This can also imply a good investment in better design in future systems to avoid building error prone modules. Software quality models that are based upon data mining from previous projects can identify fault-prone modules in the current similar development project, once similarity between projects is established. In this paper, we applied different data mining rule-based classification techniques on several publicly available datasets of the {NASA} software repository (e.g. {PC}1, {PC}2, etc). The goal was to classify the software modules into either fault prone or not fault prone modules. The paper proposed a modification on the {RIDOR} algorithm on which the results show that the enhanced {RIDOR} algorithm is better than other classification techniques in terms of the number of extracted rules and accuracy. The implemented algorithm learns defect prediction using mining static code attributes. Those attributes are then used to present a new defect predictor with high accuracy and low error rate.},
	pages = {75--86},
	number = {1},
	journaltitle = {International Journal of Software Engineering and its Applications},
	author = {Najadat, H. and Alsmadi, I.},
	date = {2012},
	keywords = {xyes}
}

@inproceedings{mishra_support_2011,
	title = {Support vector machine based fuzzy classification model for software fault prediction},
	isbn = {978-0-9727412-8-6},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872196229&partnerID=40&md5=89b748f95b05d7826d68addbf7331afa},
	abstract = {Defect proneness prediction of software modules always attracts the developers because it can reduce the testing cost as well as software development time. In the current context, with constantly increasing constraints like requirement ambiguity and complex development process, developing a fault free reliable software is a daunting task. To deliver reliable software, software engineers are required to execute exhaustive test cases which become tedious and costly for software enterprises. To ameliorate the testing process one can use a defect prediction model so that testers can focus their efforts on defect prone modules. Software defect prediction models use historical defect database to forecast error-prone modules. Defect prediction models require empirical validation to ensure their relevance to a software company. In this paper, a new Support Vector Machine based Fuzzy classification based prediction model has been proposed and evaluated on bug data base of an open source software project. In the proposed model a rule base is constructed using support vectors and the membership grade is calculated using Gaussian membership functions. Rule set optimization is done using Genetic algorithm. It is found that the proposed model gives very promising results on the criteria of probability of bug detection, probability of false alarm and accuracy.},
	pages = {693--708},
	booktitle = {Proceedings of the 5th Indian International Conference on Artificial Intelligence, {IICAI} 2011},
	author = {Mishra, B. and Shukla, K.K.},
	date = {2011},
	keywords = {Defects, Software defect prediction, Defect prediction models, Genetic algorithms, Open systems, Software testing, Development process, Fault prediction, Support vector machines, Artificial intelligence, Software fault prediction, Software modules, Fuzzy systems, Fuzzy rule base, Open source software projects, Genetic programming, Rule base, Fuzzy classification, Mathematical models, Bug detection, Empirical validation, Error prones, Exhaustive tests, Fault, Gaussian membership function, Industry, Membership functions, Membership grade, Prediction model, Probability of false alarm, {ROC}, Rule set, Software company, Software engineers, Software enterprise, Support vector, Testing process, xyes}
}

@article{macdonell_impact_2011,
	title = {The impact of sampling and rule set size on generated fuzzy inference system predictive accuracy: Analysis of a software engineering data set},
	volume = {364 {AICT}},
	issn = {18684238},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-80055061905&doi=10.1007%2f978-3-642-23960-1_43&partnerID=40&md5=c8a7de97df99b17d991cb0f94bac94e9},
	doi = {10.1007/978-3-642-23960-1_43},
	abstract = {Software project management makes extensive use of predictive modeling to estimate product size, defect proneness and development effort. Although uncertainty is acknowledged in these tasks, fuzzy inference systems, designed to cope well with uncertainty, have received only limited attention in the software engineering domain. In this study we empirically investigate the impact of two choices on the predictive accuracy of generated fuzzy inference systems when applied to a software engineering data set: sampling of observations for training and testing; and the size of the rule set generated using fuzzy c-means clustering. Over ten samples we found no consistent pattern of predictive performance given certain rule set size. We did find, however, that a rule set compiled from multiple samples generally resulted in more accurate predictions than single sample rule sets. More generally, the results provide further evidence of the sensitivity of empirical analysis outcomes to specific model-building decisions. © 2011 {IFIP} International Federation for Information Processing.},
	pages = {360--369},
	issue = {{PART} 2},
	journaltitle = {{IFIP} Advances in Information and Communication Technology},
	author = {{MacDonell}, S.G.},
	date = {2011},
	note = {{ISBN}: 9783642239595},
	keywords = {Forecasting, Software engineering, Software testing, Project management, Training and testing, Fuzzy C means clustering, Artificial intelligence, Accurate prediction, Fuzzy inference, Fuzzy systems, Predictive accuracy, Fuzzy inference systems, Source codes, C (programming language), Information management, Predictive performance, Rule set, Certain rule, Empirical analysis, Innovation, Multiple samples, Predictive modeling, Product sizes, Sampling, Sensitivity analysis, Single sample, Software engineering data, Software engineering domain, Software project management, software size, Software size, Statistical tests, xyes}
}

@inproceedings{sami_design-level_2010,
	title = {Design-level metrics estimation based on code metrics},
	isbn = {978-1-60558-638-0},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954733232&doi=10.1145%2f1774088.1774612&partnerID=40&md5=d78121d5c680fa18faad511b0b68faa2},
	doi = {10.1145/1774088.1774612},
	abstract = {Fault detection based on mining code and design metrics has been an active research area for many years. Basically "module"-based metrics for source code and design level are calculated or obtained and data mining is used to build predictor models. However, in many projects due to organizational or software process models, design level metrics are not available and/or accurate. It has been shown that performance of these classifiers or predictors decline if only source code features are used for training them. Based on best of our know knowledge no set of rule to estimate design level metrics based on code level metrics has been presented since it is believed that design level metrics have additional information and cannot be estimated without access to design artifacts. In this study we present a fuzzy modeling system to find and present these relationships for projects presented in {NASA} Metrics Data Repository ({MDP}) datasets. Interestingly, we could find a set of empirical rules that govern all the projects regardless of size, programming language and software development methodology. Comparison of fault detectors built based on estimated design metrics with actual design metrics on various projects showed a very small difference in accuracy of classifiers and validated our hypothesis that estimation of design metrics based on source code attributes can become a practical exercise. © 2010 {ACM}.},
	pages = {2531--2535},
	booktitle = {Proceedings of the {ACM} Symposium on Applied Computing},
	author = {Sami, A. and Fakhrahmad, S.M.},
	date = {2010},
	keywords = {Software metrics, Defects, Software design, {NASA}, Software defect prediction, Fault detection, Computer software, Fuzzy systems, Classifiers, Source codes, Set of rules, Design, Fuzzy classification, Data sets, Parameter estimation, Code metrics, Data repositories, Design artifacts, Design levels, Design metrics, Fault detector, Fuzzy modeling systems, Mining codes, Programming language, Research areas, Software development methodologies, Software process models, xyes, xnacc}
}

@article{draz_code_2021,
	title = {Code Smell Detection Using Whale Optimization Algorithm},
	volume = {68},
	issn = {1546-2218},
	doi = {10.32604/cmc.2021.015586},
	abstract = {Software systems have been employed in many fields as a means to reduce human efforts; consequently, stakeholders are interested in more updates of their capabilities. Code smells arise as one of the obstacles in the software industry. They are characteristics of software source code that indicate a deeper problem in design. These smells appear not only in the design but also in software implementation. Code smells introduce bugs, affect software maintainability, and lead to higher maintenance costs. Uncovering code smells can be formulated as an optimization problem of finding the best detection rules. Although researchers have recommended different techniques to improve the accuracy of code smell detection, these methods are still unstable and need to be improved. Previous research has sought only to discover a few at a time (three or five types) and did not set rules for detecting their types. Our research improves code smell detection by applying a search-based technique; we use the Whale Optimization Algorithm as a classifier to find ideal detection rules. Applying this algorithm, the Fisher criterion is utilized as a fitness function to maximize the between-class distance over the within class variance. The proposed framework adopts if-then detection rules during the software development life cycle. Those rules identify the types for both medium and large projects. Experiments are conducted on five open-source software projects to discover nine smell types that mostly appear in codes. The proposed detection framework has an average of 94.24\% precision and 93.4\% recall. These accurate values are better than other search-based algorithms of the same field. The proposed framework improves code smell detection, which increases software quality while minimizing maintenance effort, time, and cost. Additionally, the resulting classification rules are analyzed to find the software metrics that differentiate the nine code smells.},
	pages = {1919--1935},
	number = {2},
	journaltitle = {{CMC}-{COMPUTERS} {MATERIALS} \& {CONTINUA}},
	author = {Draz, Moatasem M. and Farhan, Marwa S. and Abdulkader, Sarah N. and Gafar, M. G.},
	date = {2021},
	keywords = {xyes}
}

@article{malik_empirical_2019,
	title = {Empirical Role Rule Classification Model for Software Fault Forecast with Vector Machine Analysis},
	volume = {19},
	issn = {1738-7906},
	abstract = {Our research aims to be analyses the software fault forecast with the help of machine learning and data mining tools. The analysis depends upon defected and non-defected datasets models. The datasets model we have used here are {NASA} datasets models. Our research proposed methodology is rule classification classifier with the help of vector machine. We have illustrated results in tp-rate, f- measure, area under curve ({ROC}) and correctly classified instances. Basically, these are measure efficiency unit which are used for measuring the accuracy and improvement of software fault forecast we have used here for analysis the proposed methodology vector machine with rule classification classifiers and without using of vector machine analysis. We observed that M5rule classifier is worst classifier in all over rule classification because it decreased his efficiency in all scenario case during the use of vector machine. But without using proposed solution methodology we can use it for analysis and can compare their results with other classifiers. {ONER} and {PART} classifiers are very good in all scenario cases because they have enhanced the efficiency and also improved the correctly classified instance c.c.i \% ratio.},
	pages = {195--201},
	number = {9},
	journaltitle = {{INTERNATIONAL} {JOURNAL} {OF} {COMPUTER} {SCIENCE} {AND} {NETWORK} {SECURITY}},
	author = {Malik, Maaz Rasheed and Yining, Liu and Shaikh, Salahuddin},
	date = {2019-09-30},
	keywords = {xyes}
}

@article{aydilek_analyzing_2018,
	title = {Analyzing and improving information gain of metrics used in software defect prediction in decision trees},
	volume = {24},
	issn = {1300-7009},
	doi = {10.5505/pajes.2018.93584},
	abstract = {{McCabe} and Halstead method-level metrics are among the well-known and widely used quantitative software metrics are used to measure software quality in a concrete way. Software defect prediction can guess which or which of the sub-modules in the software to be developed may be more prone to defect Thus, loss of labor and time can be avoided. The datasets which are used for software defect prediction, usually have an unbalanced class distribution, since the number of records with defective class can be fewer than the number of records with not defective class and this situation adversely affect the results of the machine learning methods. Information gain is employed in decision trees and decision tree based rule classifier and attribute selection methods. In this study, software metrics that provide important information for software defect prediction have been investigated and {CM}1, {JM}1, {KC}1 and {PC}1 datasets of {NASA}'s {PROMISE} software repository have been balanced with the synthetic data over-sampling Smote algorithm and improved in terms of information gain. As a result, the software defect prediction datasets with higher classification success performance and the software metrics with increased information gain ratio are obtained in the decision trees.},
	pages = {906--914},
	number = {5},
	journaltitle = {{PAMUKKALE} {UNIVERSITY} {JOURNAL} {OF} {ENGINEERING} {SCIENCES}-{PAMUKKALE} {UNIVERSITESI} {MUHENDISLIK} {BILIMLERI} {DERGISI}},
	author = {Aydilek, Ibrahim Berkan},
	date = {2018},
	keywords = {xyes}
}

@inproceedings{alrajeh_logic-based_2016,
	title = {Logic-based Learning in Software Engineering},
	isbn = {978-1-4503-4205-6},
	doi = {10.1145/2889160.2891050},
	abstract = {In recent years, research efforts have been directed towards the use of Machine Learning ({ML}) techniques to support and automate activities such as program repair, specification mining and risk assessment. The focus has largely been on techniques for classification, clustering and regression. Although beneficial, these do not produce a declarative, interpretable representation of the learned information. Hence, they cannot readily be used to inform, revise and elaborate software models. On the other hand, recent advances in {ML} have witnessed the emergence of new logic-based learning approaches that differ from traditional {ML} in that their output is represented in a declarative, rule-based manner, making them well-suited for many software engineering tasks. In this technical briefing, we will introduce the audience to the latest advances in logic-based learning, give an overview of how logic-based learning systems can successfully provide automated support to a variety of software engineering tasks, demonstrate the application to two real case studies from the domain of requirements engineering and software design and highlight future challenges and directions.},
	pages = {892--893},
	booktitle = {2016 {IEEE}/{ACM} 38TH {INTERNATIONAL} {CONFERENCE} {ON} {SOFTWARE} {ENGINEERING} {COMPANION} ({ICSE}-C)},
	publisher = {{IEEE}; Assoc Comp Machinery; {IEEE} Comp Soc; {IEEE} Tech Council Software Engn; Special Interest Grp Software Engn},
	author = {Alrajeh, Dalal and Russo, Alessandra and Uchitel, Sebastian and Kramer, Jeff},
	date = {2016},
	keywords = {xyes}
}

@article{sahin_code-smell_2014,
	title = {Code-Smell Detection as a Bilevel Problem},
	volume = {24},
	issn = {1049-331X},
	doi = {10.1145/2675067},
	abstract = {Code smells represent design situations that can affect the maintenance and evolution of software. They make the system difficult to evolve. Code smells are detected, in general, using quality metrics that represent some symptoms. However, the selection of suitable quality metrics is challenging due to the absence of consensus in identifying some code smells based on a set of symptoms and also the high calibration effort in determining manually the threshold value for each metric. In this article, we propose treating the generation of code-smell detection rules as a bilevel optimization problem. Bilevel optimization problems represent a class of challenging optimization problems, which contain two levels of optimization tasks. In these problems, only the optimal solutions to the lower-level problem become possible feasible candidates to the upper-level problem. In this sense, the code-smell detection problem can be treated as a bilevel optimization problem, but due to lack of suitable solution techniques, it has been attempted to be solved as a single-level optimization problem in the past. In our adaptation here, the upper-level problem generates a set of detection rules, a combination of quality metrics, which maximizes the coverage of the base of code-smell examples and artificial code smells generated by the lower level. The lower level maximizes the number of generated artificial code smells that cannot be detected by the rules produced by the upper level. The main advantage of our bilevel formulation is that the generation of detection rules is not limited to some code-smell examples identified manually by developers that are difficult to collect, but it allows the prediction of new code-smell behavior that is different from those of the base of examples. The statistical analysis of our experiments over 31 runs on nine open-source systems and one industrial project shows that seven types of code smells were detected with an average of more than 86\% in terms of precision and recall. The results confirm the outperformance of our bilevel proposal compared to state-of-art code-smell detection techniques. The evaluation performed by software engineers also confirms the relevance of detected code smells to improve the quality of software systems.},
	number = {1},
	journaltitle = {{ACM} {TRANSACTIONS} {ON} {SOFTWARE} {ENGINEERING} {AND} {METHODOLOGY}},
	author = {Sahin, Dilan and Kessentini, Marouane and Bechikh, Slim and Deb, Kalyanmoy},
	date = {2014-09},
	keywords = {xyes}
}

@article{czibula_software_2014,
	title = {Software defect prediction using relational association rule mining},
	volume = {264},
	issn = {0020-0255},
	doi = {10.1016/j.ins.2013.12.031},
	abstract = {This paper focuses on the problem of defect prediction, a problem of major importance during software maintenance and evolution. It is essential for software developers to identify defective software modules in order to continuously improve the quality of a software system. As the conditions for a software module to have defects are hard to identify, machine learning based classification models are still developed to approach the problem of defect prediction. We propose a novel classification model based on relational association rules mining. Relational association rules are an extension of ordinal association rules, which are a particular type of association rules that describe numerical orderings between attributes that commonly occur over a dataset. Our classifier is based on the discovery of relational association rules for predicting whether a software module is or it is not defective. An experimental evaluation of the proposed model on the open source {NASA} datasets, as well as a comparison to similar existing approaches is provided. The obtained results show that our classifier overperforms, for most of the considered evaluation measures, the existing machine learning based techniques for defect prediction. This confirms the potential of our proposal. (C) 2014 Elsevier Inc. All rights reserved.},
	pages = {260--278},
	journaltitle = {{INFORMATION} {SCIENCES}},
	author = {Czibula, Gabriela and Marian, Zsuzsanna and Czibula, Istvan Gergely},
	date = {2014-04-20},
	keywords = {xyes}
}

@article{khoshgoftaar_software_2014,
	title = {Software quality assessment using a multi-strategy classifier},
	volume = {259},
	issn = {0020-0255},
	doi = {10.1016/j.ins.2010.11.028},
	abstract = {Classifying program Modules as fault-prone or not fault-prone is a valuable technique for guiding the software development process, so that resources can be allocated to components most likely to have faults. The rule-based classification and the case-based learning techniques are commonly used in software quality classification problems. However, studies show that these two techniques share some complementary strengths and weaknesses. Therefore, in this paper we propose a new multi-strategy classification model, {RB}2CBL, which integrates a rule-based ({RB}) model with two case-based learning ({CBL}) models. {RB}2CBL possesses the merits of both the {RB} model and {CBL} model and restrains their drawbacks. In the {RB}2CBL model, the parameter optimization of the {CBL} models is critical and an embedded genetic algorithm optimizer is used. Two case studies were carried out to validate the proposed method. The results show that, by suitably choosing the accuracy of the {RB} model, the {RB}2CBL model outperforms the {RB} model alone without overfitting. (C) 2010 Elsevier Inc. All rights reserved.},
	pages = {555--570},
	journaltitle = {{INFORMATION} {SCIENCES}},
	author = {Khoshgoftaar, Taghi M. and Xiao, Yudong and Gao, Kehan},
	date = {2014-02-20},
	keywords = {Computer software selection and evaluation, Genetic algorithms, Software engineering, Software Quality, Software development process, Classification models, Rule-based classification, Case based learning, Parameter optimization, Rule-based models, Software quality assessment, xyes}
}

@article{yu_experience_2012,
	title = {Experience in Predicting Fault-Prone Software Modules Using Complexity Metrics},
	volume = {9},
	issn = {1684-3703},
	doi = {10.1080/16843703.2012.11673302},
	abstract = {Complexity metrics have been intensively studied in predicting fault-prone software modules. However, little work is done in studying how to effectively use the complexity metrics and the prediction models under realistic conditions. In this paper, we present a study showing how to utilize the prediction models generated from existing projects to improve the fault detection on other projects. The binary logistic regression method is used in studying publicly available data of five commercial products. Our study shows (1) models generated using more datasets can improve the prediction accuracy but not the recall rate; (2) lowering the cut-off value can improve the recall rate, but the number of false positives will be increased, which will result in higher maintenance effort. We further suggest that in order to improve model prediction efficiency, the selection of source datasets and the determination of cut-Off values should be based on specific properties of a project. So far, there are no general rules that have been found and reported to follow},
	pages = {421--433},
	number = {4},
	journaltitle = {{QUALITY} {TECHNOLOGY} {AND} {QUANTITATIVE} {MANAGEMENT}},
	author = {Yu, Liguo and Mishra, Alok},
	date = {2012-12},
	keywords = {xyes}
}

@article{rodriguez_searching_2012,
	title = {Searching for rules to detect defective modules: A subgroup discovery approach},
	volume = {191},
	issn = {0020-0255},
	doi = {10.1016/j.ins.2011.01.039},
	abstract = {Data mining methods in software engineering are becoming increasingly important as they can support several aspects of the software development life-cycle such as quality. In this work, we present a data mining approach to induce rules extracted from static software metrics characterising fault-prone modules. Due to the special characteristics of the defect prediction data (imbalanced, inconsistency, redundancy) not all classification algorithms are capable of dealing with this task conveniently. To deal with these problems, Subgroup Discovery ({SD}) algorithms can be used to find groups of statistically different data given a property of interest. We propose {EDER}-{SD} (Evolutionary Decision Rules for Subgroup Discovery), a {SD} algorithm based on evolutionary computation that induces rules describing only fault-prone modules. The rules are a well-known model representation that can be easily understood and applied by project managers and quality engineers. Thus, rules can help them to develop software systems that can be justifiably trusted. Contrary to other approaches in {SD}, our algorithm has the advantage of working with continuous variables as the conditions of the rules are defined using intervals. We describe the rules obtained by applying our algorithm to seven publicly available datasets from the {PROMISE} repository showing that they are capable of characterising subgroups of fault-prone modules. We also compare our results with three other well known {SD} algorithms and the {EDER}-{SD} algorithm performs well in most cases. (C) 2011 Elsevier Inc. All rights reserved.},
	pages = {14--30},
	journaltitle = {{INFORMATION} {SCIENCES}},
	author = {Rodriguez, D. and Ruiz, R. and Riquelme, J. C. and Aguilar-Ruiz, J. S.},
	date = {2012-05-15},
	keywords = {Data mining, Software metrics, Defects, Software engineering, Project management, Defect prediction, Classification algorithm, Software systems, Evolutionary algorithms, Imbalanced Data-sets, Rules, Fault-prone modules, Subgroup discovery, Data sets, Continuous variables, Data mining methods, Decision rules, Model representation, Project managers, Quality engineers, Software development life cycle, xyes, xbest},
	file = {Submitted Version:C\:\\Users\\michalm\\Zotero\\storage\\BQZNCWEN\\Rodríguez et al. - 2012 - Searching for rules to detect defective modules A.pdf:application/pdf}
}

@inproceedings{gayatri_feature_2010,
	title = {Feature Selection Using Decision Tree Induction in Class level Metrics Dataset for Software Defect Predictions},
	isbn = {978-988-17012-0-6},
	series = {Lecture Notes in Engineering and Computer Science},
	abstract = {The importance of software testing for quality assurance cannot be over emphasized. The estimation of quality factors is important for minimizing the cost and improving the effectiveness of the software testing process. One of the quality factors is fault proneness, for which unfortunately there is no generalized technique available to effectively identify fault proneness. Many researchers have concentrated on how to select software metrics that are likely to indicate fault proneness. At the same time dimensionality reduction (feature selection of software metrics) also plays a vital role for the effectiveness of the model or best quality model. Feature selection is important for a variety of reasons such as generalization, performance, computational efficiency and feature interpretability. In this paper a new method for feature selection is proposed based on Decision Tree Induction. Relevant features are selected from the class level dataset based on decision tree classifiers used in the classification process. The attributes which form rules for the classifiers are taken as the relevant feature set or new feature set named Decision Tree Induction Rule based ({DTIRB}) feature set. Different classifiers are learned with this new data set obtained by decision tree induction process and achieved better performance. The performance of 18 classifiers is studied with the proposed method. Comparison is made with the Support Vector Machines ({SVM}) and {RELIEF} feature selection techniques. It is observed that the proposed method outperforms the other two for most of the classifiers considered. Overall improvement in classification process is also found with original feature set and reduced feature set. The proposed method has the advantage of easy interpretability and comprehensibility. Class level metrics dataset is used for evaluating the performance of the model. Receiver Operating Characteristics ({ROC}) and Mean Absolute Error ({MAE}) and Root Mean Squared Error ({RMSE}) error measures are used as the performance measures for checking effectiveness of the model.},
	pages = {124--129},
	booktitle = {{WORLD} {CONGRESS} {ON} {ENGINEERING} {AND} {COMPUTER} {SCIENCE}, {VOLS} 1 {AND} 2},
	publisher = {Int Assoc Engineers ({IAENG}); {IAENG} Soc Artificial Intelligence; {IAENG} Soc Bioinformat; {IAENG} Soc Chem Engn; {IAENG} Soc Comp Sci; {IAENG} Soc Data Min; {IAENG} Soc Elect Engn; {IAENG} Soc Imaging Engn; {IAENG} Soc Ind Engn; {IAENG} Soc Internet Comp \& Web Serv; {IAENG} Soc Oper Res; {IAENG} Soc Sci Comp; {IAENG} Soc Software Engn; {IAENG} Soc Wireless Networks; {IAENG} Soc {HIV}/{AIDS}},
	author = {Gayatri, N. and Nickolas, S. and Reddy, A. V.},
	editor = {{Ao, SI and Douglas, C and Grundfest, WS and Burgstone, J}},
	date = {2010},
	note = {{ISSN}: 2078-0958},
	keywords = {xyes}
}

@inproceedings{karthik_defect_2010,
	title = {Defect association and complexity prediction by mining association and clustering rules},
	volume = {7},
	doi = {10.1109/ICCET.2010.5485608},
	abstract = {Number of defects remaining in a system provides an insight into the quality of the system. Software defect prediction focuses on classifying the modules of a system into fault prone and non-fault prone modules. This paper focuses on predicting the fault prone modules as well as identifying the types of defects that occur in the fault prone modules. Software defect prediction is combined with association rule mining to determine the associations that occur among the detected defects and the effort required for isolating and correcting these defects. Clustering rules are used to classify the defects into groups indicating their complexity: {SIMPLE}, {MODERATE} and {COMPLEX}. Moreover the defects are used to predict the effect on the project schedules and the nature of risk concerning the completion of such projects.},
	pages = {V7--569--V7--573},
	booktitle = {2010 2nd International Conference on Computer Engineering and Technology},
	author = {Karthik, R. and Manikandan, N.},
	date = {2010-04},
	keywords = {Association rule mining, Association rules, Associative processing, Clustering, Clustering rules, Complexity predictions, Data mining, Defect Associations, Defect classification, Defect Classification, Defect correction effort, Defect Correction Effort, Defects, Fault diagnosis, Fault-prone, Fault-prone modules, Forecasting, Information technology, Mining associations, Prediction methods, Project management, Project schedules, Resource management, Risk analysis, Software defect prediction, Software systems, Testing, xyes},
	file = {Karthik and Manikandan - 2010 - Defect association and complexity prediction by mi.pdf:C\:\\Users\\michalm\\Zotero\\storage\\TCX4Z455\\Karthik and Manikandan - 2010 - Defect association and complexity prediction by mi.pdf:application/pdf}
}

@inproceedings{shao_software_2017,
	title = {Software defect prediction based on class-association rules},
	doi = {10.1109/ICRSE.2017.8030774},
	abstract = {Although there have lots of studies on using static code attributes to identify defective software modules, there still have many challenges. For instance, it is difficult to implement the Apriori-type algorithm to predict defects by learning from an imbalanced dataset. For more accurate and understandable defect prediction, a novel approach based on class-association rules algorithm is proposed. Class-association rules are looked as a separate class label, which is a specific type of association rules that explores the relationship between attributes and categories. In an empirical comparison with four datasets, the novel approach is superior to other four classification techniques and accordingly, proved it's valuable for defect prediction.},
	pages = {1--5},
	booktitle = {2017 Second International Conference on Reliability Systems Engineering ({ICRSE})},
	author = {Shao, Yuanxun and Liu, Bin and Li, Guoqi and Wang, Shihai},
	date = {2017-07},
	keywords = {Apriori, association rule, Classification algorithms, Itemsets, Modeling, Prediction algorithms, prediction performance, rule pruning, Software, Software algorithms, software defect prediction, Training, xassociation-rules, xyes}
}

@inproceedings{monden_heuristic_2012,
	title = {A Heuristic Rule Reduction Approach to Software Fault-proneness Prediction},
	volume = {1},
	doi = {10.1109/APSEC.2012.103},
	abstract = {Background: Association rules are more comprehensive and understandable than fault-prone module predictors (such as logistic regression model, random forest and support vector machine). One of the challenges is that there are usually too many similar rules to be extracted by the rule mining. Aim: This paper proposes a rule reduction technique that can eliminate complex (long) and/or similar rules without sacrificing the prediction performance as much as possible. Method: The notion of the method is to removing long and similar rules unless their confidence level as a heuristic is high enough than shorter rules. For example, it starts with selecting rules with shortest length (length=1), and then it continues through the 2nd shortest rules selection (length=2) based on the current confidence level, this process is repeated on the selection for longer rules until no rules are worth included. Result: An empirical experiment has been conducted with the Mylyn and Eclipse {PDE} datasets. The result of the Mylyn dataset showed the proposed method was able to reduce the number of rules from 1347 down to 13, while the delta of the prediction performance was only. 015 (from. 757 down to. 742) in terms of the F1 prediction criteria. In the experiment with Eclipsed {PDE} dataset, the proposed method reduced the number of rules from 398 to 12, while the prediction performance even improved (from. 426 to. 441.) Conclusion: The novel technique introduced resolves the rule explosion problem in association rule mining for software proneness prediction, which is significant and provides better understanding of the causes of faulty modules.},
	pages = {838--847},
	booktitle = {2012 19th Asia-Pacific Software Engineering Conference},
	author = {Monden, Akito and Keung, Jacky and Morisaki, Shuji and Kamei, Yasutaka and Matsumoto, Ken-Ichi},
	date = {2012-12},
	note = {{ISSN}: 1530-1362},
	keywords = {association rule mining, Association rules, data mining, defect prediction, Educational institutions, empirical study, Explosions, Measurement, Predictive models, Software, software quality, xassociation-rules, xyes},
	file = {Monden et al. - 2012 - A Heuristic Rule Reduction Approach to Software Fa.pdf:C\:\\Users\\michalm\\Zotero\\storage\\5KLR4JGE\\Monden et al. - 2012 - A Heuristic Rule Reduction Approach to Software Fa.pdf:application/pdf}
}

@article{he_ensemble_2019,
	title = {Ensemble {MultiBoost} Based on {RIPPER} Classifier for Prediction of Imbalanced Software Defect Data},
	volume = {7},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2019.2934128},
	abstract = {Identifying defective software entities is essential to ensure software quality during software development. However, the high dimensionality and class distribution imbalance of software defect data seriously affect software defect prediction performance. In order to solve this problem, this paper proposes an Ensemble {MultiBoost} based on {RIPPER} classifier for prediction of imbalanced Software Defect data, called {EMR}\_SD. Firstly, the algorithm uses principal component analysis ({PCA}) method to find out the most effective features from the original features of the data set, so as to achieve the purpose of dimensionality reduction and redundancy removal. Furthermore, the combined sampling method of adaptive synthetic sampling ({ADASYN}) and random sampling without replacement is performed to solve the problem of data class imbalance. This classifier establishes association rules based on attributes and classes, using {MultiBoost} to reduce deviation and variance, so as to achieve the purpose of reducing classification error. The proposed prediction model is evaluated experimentally on the {NASA} {MDP} public datasets and compared with existing similar algorithms. The results show that {EMR}\_SD algorithm is superior to {DNC}, {CEL} and other defect prediction techniques in most evaluation indicators, which proves the effectiveness of the algorithm.},
	pages = {110333--110343},
	journaltitle = {{IEEE} Access},
	author = {He, Haitao and Zhang, Xu and Wang, Qian and Ren, Jiadong and Liu, Jiaxin and Zhao, Xiaolin and Cheng, Yongqiang},
	date = {2019},
	keywords = {Class distributions, class imbalance, Classification algorithms, Classification errors, combined sampling, Computer software selection and evaluation, Defect prediction, Defects, Dimensionality reduction, Evaluation indicators, Feature extraction, Forecasting, High dimensionality, {MultiBoost}, {NASA}, Prediction algorithms, Predictive analytics, Predictive models, Principal component analysis, Redundancy removal, rule learning, Software, Software algorithms, Software defect prediction, Software design, Software entities, Software quality, xyes},
	file = {Full Text:C\:\\Users\\michalm\\Zotero\\storage\\SKE8UCK9\\He et al. - 2019 - Ensemble MultiBoost Based on RIPPER Classifier for.pdf:application/pdf;Full Text:C\:\\Users\\michalm\\Zotero\\storage\\XKMMDBBD\\He et al. - 2019 - Ensemble MultiBoost Based on RIPPER Classifier for.pdf:application/pdf}
}

@inproceedings{pravin_efficient_2012,
	title = {An efficient programming rule extraction and detection of violations in software source code using neural networks},
	doi = {10.1109/ICoAC.2012.6416837},
	abstract = {The larger size and complexity of software source code builds many challenges in bug detection. Data mining based bug detection methods eliminate the bugs present in software source code effectively. Rule violation and copy paste related defects are the most concerns for bug detection system. Traditional data mining approaches such as frequent Itemset mining and frequent sequence mining are relatively good but they are lacking in accuracy and pattern recognition. Neural networks have emerged as advanced data mining tools in cases where other techniques may not produce satisfactory predictive models. The neural network is trained for possible set of errors that could be present in software source code. From the training data the neural network learns how to predict the correct output. The processing elements of neural networks are associated with weights which are adjusted during the training period.},
	pages = {1--4},
	booktitle = {2012 Fourth International Conference on Advanced Computing ({ICoAC})},
	author = {Pravin, A. and Srinivasan, S.},
	date = {2012-12},
	note = {{ISSN}: 2377-6927},
	keywords = {Data mining, Software, Computer bugs, Inspection, Biological neural networks, Data Mining, Decision Trees, Defect Detection, Neural Networks Association Rules, Programming, Programming Rule, xyes}
}

@article{singh_fuzzy_2017,
	title = {Fuzzy Rule-Based Approach for Software Fault Prediction},
	volume = {47},
	issn = {2168-2232},
	doi = {10.1109/TSMC.2016.2521840},
	abstract = {Knowing faulty modules prior to testing makes testing more effective and helps to obtain reliable software. Here, we develop a framework for automatic extraction of human understandable fuzzy rules for software fault detection/classification. This is an integrated framework to simultaneously identify useful determinants (attributes) of faults and fuzzy rules using those attributes. At the beginning of the training, the system assumes every attribute (feature) as a useless feature and then uses a concept of feature attenuating gate to select useful features. The learning process opens the gates or closes them more tightly based on utility of the features. Our system can discard derogatory and indifferent attributes and select the useful ones. It can also exploit subtle nonlinear interaction between attributes. In order to demonstrate the effectiveness of the framework, we have used several publicly available software fault data sets and compared the performance of our method with that of some existing methods. The results using tenfold cross-validation setup show that our system can find useful fuzzy rules for fault prediction.},
	pages = {826--837},
	number = {5},
	journaltitle = {{IEEE} Transactions on Systems, Man, and Cybernetics: Systems},
	author = {Singh, Pradeep and Pal, Nikhil R. and Verma, Shrish and Vyas, Om Prakash},
	date = {2017-05},
	keywords = {Feature extraction, Software, Software metrics, Forecasting, Learning systems, Software testing, Fault detection, Software reliability, Software fault prediction, Computer software, Fuzzy inference, Fuzzy rules, Rule generation, Software metrices, Automatic extraction, Feature modulating gates, Integrated frameworks, Nonlinear interactions, Software fault detection, machine learning, fuzzy rule generation, Logic gates, software fault prediction, software metric selection, xyes, xfuzzy, xcross-project}
}

@inproceedings{cheng_software_2011,
	title = {Software fault detection using program patterns},
	doi = {10.1109/ICSESS.2011.5982308},
	abstract = {Effective detection of software faults is an important activity of software development process. The main difficulty of detecting software fault is finding faults in a large and complex software system. In this paper, we propose an approach that applies program patterns to detect and locate software fault so that programmer can fix bug and increase software quality. The advantage of the proposed approach is that the defect-prone code segments can be detected. To facilitate the programmer to detect program bugs, this approach also includes a Graphic User Interface to locate the defect-prone code segments.},
	pages = {278--281},
	booktitle = {2011 {IEEE} 2nd International Conference on Software Engineering and Service Science},
	author = {Cheng, Ko-Li and Chang, Ching-Pao and Chu, Chih-Ping},
	date = {2011-07},
	note = {{ISSN}: 2327-0594},
	keywords = {Software quality, Software engineering, Association rules, Computer bugs, Programming, Defect Prediction, Program pattern, Program segement, xyes}
}

@inproceedings{anezakis_verification_2018,
	title = {Verification of the effectiveness of fuzzy rule-based fault prediction: A replication study},
	doi = {10.1109/INISTA.2018.8466331},
	abstract = {The prediction success of faulty modules in a software helps practitioners to plan the budget of software maintenance that leads developers to improve the reliability of software systems. Despite various learning algorithms and statistical methods, fault prediction needs novel methods for enhancing the success of the prediction. Fault prediction can be performed using fuzzy rules that are new for this field. In this work, fuzzy rule-based fault prediction approach, which was developed by Singh et al. [11], is replicated to validate the success of fuzzy rule-based fault prediction in open-source data sets. The steps of the experiment and the steps of Singh et al's work, which are applied for replication, both are same. Classification is performed after generating clusters that are constituted using fuzzy rules in normalized data sets. According to the prediction results obtained by applying 10*10 cross-validation, fuzzy rule-based fault prediction produces less errors in open-source data sets when it is compared with industrial data sets. In addition to this, the results validate the findings of Singh et al.'s work in terms of some performance parameters of the fault prediction.},
	pages = {1--8},
	booktitle = {2018 Innovations in Intelligent Systems and Applications ({INISTA})},
	author = {Anezakis, Vardis-Dimitris and Öztürk, Muhammed Maruf},
	date = {2018-07},
	keywords = {Feature extraction, Software metrics, {NASA}, Open source software, Forecasting, Classification (of information), Intelligent systems, Budget control, Software reliability, Fault prediction, Measurement, Learning algorithms, Fuzzy inference, Fuzzy rules, Performance parameters, Fault data, Fuzzy rule based, Industrial datum, Open source datum, Replication study, Prediction algorithms, fault data sets, fault prediction, Fuzzy rule, Modulation, modulator learning, software metrics, xyes}
}

@inproceedings{singh_comprehensive_2017,
	title = {Comprehensive model for software fault prediction},
	doi = {10.1109/ICICI.2017.8365311},
	abstract = {Software Fault prediction ({SFP}) is an important task in the fields of software engineering to develop a cost effective software. Most of the software fault prediction is performed on same project date i.e., training and testing with same projects fault data. In case of unavailability of fault training data which is possible for the new project, data from the similar types/category of other projects can be used to train the model for the prediction. The software projects has been categorized into three categories by Boehm. The project within a certain group will be having good similarities with other projects within the group. So it is more suitable to train using the projects from same group. In this work we proposed to develop a model with similar category of data to predict the fault of another project belongs to same category. On basis of {KLOC} we have taken five organic software projects and performed various cross project and within project experiments. To generate a comprehensive generalized model for organic software's fault prediction, we have modeled various rule based to learner. Various rule-based learners used for comparison are {JRip}, {CART}, Conjunctive Rule, C4.5, {NNge}, {OneR}, Ridor, {PART}, and decision table-Naive Bayes hybrid classifier ({DTNB}).},
	pages = {1103--1108},
	booktitle = {2017 International Conference on Inventive Computing and Informatics ({ICICI})},
	author = {Singh, Pradeep},
	date = {2017-11},
	keywords = {Predictive models, Software, Testing, Cost engineering, Forecasting, Software testing, Training and testing, Training data, Fault prediction, Comprehensive model, Software fault prediction, Rule based, Cost effectiveness, Decision tables, Generalized models, Hybrid classifier, Three categories, Training, Computational modeling, Data models, Rule based Learner, xyes, xcross-project},
	file = {Singh - 2017 - Comprehensive model for software fault prediction.pdf:C\:\\Users\\michalm\\Zotero\\storage\\5EF5YECT\\Singh - 2017 - Comprehensive model for software fault prediction.pdf:application/pdf}
}

@inproceedings{mutlu_automatic_2018,
	title = {Automatic Rule Generation of Fuzzy Systems: A Comparative Assessment on Software Defect Prediction},
	doi = {10.1109/UBMK.2018.8566479},
	abstract = {Fuzzy rule base systems are expert systems rely on fuzzy set theory. Here the knowledge of human expert is transfered to the artificial model via fuzzy rules. Therefore, preciseness, completeness and coverage of fuzzy rules in a fuzzy system is vital for the accuracy and plausibility of fuzzy reasoning. However, in such cases where the human expert is unable to supply the rules sufficiently, data-based automatic rule generation methods attract attention. In this study, 2 linear and 2 evolutionary approaches of automatic fuzzy rule generation methods are investigated. The investigated linear solutions contain Wang-Mendel Method and E2E-{HFS}, while {MOGUL} and {IVTURS}-{FARC} are the selected evolutionary approaches. Wang-Mendel and {MOGUL} is commonly considered as basic methods of the group they belong to. {IVTURS}-{FARC} is distinguished with its ability to handle interval valued fuzzy sets. Among the rest of the algorithms, E2E-{HFS} is unique with its weak dependency to data. Because it only use some simple properties of corresponding input variable. In order to compare the completeness and the accuracy of automatically generated fuzzy rules, several experiments are performed on different software defect prediction datasets, and the classification performance of resulting fuzzy systems is evaluated. Provided results show that even if training of evolutionary approaches seem to be more precise, similar accuracy can be achieved by linear approaches, and they perform better regarding the experiments on unseen data.},
	pages = {209--214},
	booktitle = {2018 3rd International Conference on Computer Science and Engineering ({UBMK})},
	author = {Mutlu, Begum and Sezer, Ebru A. and Akcayol, M. Ali},
	date = {2018-09},
	keywords = {Software, Automatically generated, Defects, Software defect prediction, Forecasting, Classification (of information), Expert systems, Computer software, Fuzzy inference, Fuzzy rules, Fuzzy systems, Rule generation, Fuzzy logic, Fuzzy sets, Classification performance, Comparative assessment, Evolutionary rules, Fuzzy inference systems, Fuzzy set theory, Interval-valued fuzzy sets, Linguistics, Software Defect Prediction, Evolutionary Rule Learning, Fuzzy Inference Systems, Fuzzy Rule Generation, Genetics, xyes, xfuzzy},
	file = {Mutlu et al. - 2018 - Automatic Rule Generation of Fuzzy Systems A Comp.pdf:C\:\\Users\\michalm\\Zotero\\storage\\BCXGVN26\\Mutlu et al. - 2018 - Automatic Rule Generation of Fuzzy Systems A Comp.pdf:application/pdf}
}

@inproceedings{miholca_improved_2018,
	title = {An Improved Approach to Software Defect Prediction using a Hybrid Machine Learning Model},
	doi = {10.1109/SYNASC.2018.00074},
	abstract = {Software defect prediction is an intricate but essential software testing related activity. As a solution to it, we have recently proposed {HyGRAR}, a hybrid classification model which combines Gradual Relational Association Rules ({GRARs}) with {ANNs}. {ANNs} were used to learn gradual relations that were then considered in a mining process so as to discover the interesting {GRARs} characterizing the defective and non-defective software entities, respectively. The classification of a new entity based on the discriminative {GRARs} was made through a non-adaptive heuristic method. In current paper, we propose to enhance {HyGRAR} through autonomously learning the classification methodology. Evaluation experiments performed on two open-source data sets indicate that the enhanced {HyGRAR} classifier outperforms the related approaches evaluated on the same two data sets.},
	pages = {443--448},
	booktitle = {2018 20th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing ({SYNASC})},
	author = {Miholca, Diana-Lucia},
	date = {2018-09},
	keywords = {Data mining, Software, software defect prediction, Software metrics, Neural networks, Training, Machine Learning, Computational modeling, Artificial Neural Networks, Drugs, Gradual Relational Association Rules, xyes}
}

@inproceedings{li_mining_2010,
	title = {Mining Frequent Patterns from Software Defect Repositories for Black-Box Testing},
	doi = {10.1109/IWISA.2010.5473578},
	abstract = {Software defects are usually detected by inspection, black-box testing or white-box testing. Current software defect mining work focuses on mining frequent patterns without distinguishing these different kinds of defects, and mining with respect to defect type can only give limited guidance on software development due to overly broad classification of defect type. In this paper, we present four kinds of frequent patterns from defects detected by black-box testing (called black-box defect) based on a kind of detailed classification named {ODC}-{BD} (Orthogonal Defect Classification for Blackbox Defect). The frequent patterns include the top 10 conditions (data or operation) which most easily result in defects or severe defects, the top 10 defect phenomena which most frequently occur and have a great impact on users, association rules between function modules and defect types. We aim to help project managers, black-box testers and developers improve the efficiency of software defect detection and analysis using these frequent patterns. Our study is based on 5023 defect reports from 56 large industrial projects and 2 open source projects.},
	pages = {1--4},
	booktitle = {2010 2nd International Workshop on Intelligent Systems and Applications},
	author = {Li, Ning and Li, Zhanhuai and Zhang, Lijun},
	date = {2010-05},
	keywords = {Data mining, Defects, Software design, Software quality, Software defects, Open source software, Intelligent systems, Association rules, Software testing, Project management, Computer software, Black-box testing, Defect classification, Inspection, Associative processing, Frequent Itemsets, Software defect, Itemsets, Programming, Pattern analysis, Software development management, xyes, xassociation-rules}
}

@inproceedings{chai_software_2018,
	title = {A Software Defect Management System Based on Knowledge Base},
	doi = {10.1109/QRS-C.2018.00118},
	abstract = {Software testing is an effective way to improving software quality, a software defect is identified before it goes live, a massive amount of bug-related data is accumulated during software testing, there is a point in studying how to improve the working efficiency of software testing through integration and use of such data. In this thesis, a software defect management system based on knowledge base is designed, where the three-tier knowledge base architecture is used to manage defects, data mining is performed with factual knowledge generated from the testing to derive rule knowledge that will be used for defect prediction, and the appropriate strategy knowledge is configured to manage bugs, so as to improve the working efficiency of software testing.},
	pages = {652--653},
	booktitle = {2018 {IEEE} International Conference on Software Quality, Reliability and Security Companion ({QRS}-C)},
	author = {Chai, Haiyan and Zhang, Nan and Liu, Bojiang and Tang, Longli},
	date = {2018-07},
	keywords = {Data mining, Software quality, Software testing, Computer bugs, Knowledge based systems, data mining, software testing, defect management, knowledge base, xyes}
}

@inproceedings{jin_software_2010,
	title = {Software Fault Prediction Model Based on Adaptive Dynamical and Median Particle Swarm Optimization},
	volume = {1},
	doi = {10.1109/MMIT.2010.11},
	abstract = {Software quality prediction can play a role of importance in software management, and thus in improve the quality of software systems. By mining software with data mining technique, predictive models can be induced that software managers the insights they need to tackle these quality problems in an efficient way. This paper deals with the adaptive dynamic and median particle swarm optimization ({ADMPSO}) based on the {PSO} classification technique. {ADMPSO} can act as a valid data mining technique to predict erroneous software modules. The predictive model in this paper extracts the relationship rules of software quality and metrics. Information entropy approach is applied to simplify the extraction rule set. The empirical result shows that this method set of rules can be streamlined and the forecast accuracy can be improved.},
	pages = {44--47},
	booktitle = {2010 Second International Conference on Multimedia and Information Technology},
	author = {Jin, Cong and Dong, En-Mei and Qin, Li-Na},
	date = {2010-04},
	keywords = {Data mining, Predictive models, Computer software selection and evaluation, Software quality, Forecasting, Classification (of information), Software Quality, Fault prediction, Particle swarm optimization ({PSO}), Software fault prediction, Software modules, Software systems, Set of rules, Classification technique, Quality of softwares, Mathematical models, Predictive control systems, Empirical results, Adaptive dynamics, Classification, Data mining techniques, Extraction rule, Forecast accuracy, Information entropy, Information technology, Mining software, Quality problems, Relationship rules, Software management, Software managers, Software quality prediction, Computer science, Conference management, Multimedia systems, Particle swarm optimization, Quality management, xyes}
}

@inproceedings{diamantopoulos_towards_2015,
	title = {Towards Interpretable Defect-Prone Component Analysis Using Genetic Fuzzy Systems},
	doi = {10.1109/RAISE.2015.13},
	abstract = {The problem of Software Reliability Prediction is attracting the attention of several researchers during the last few years. Various classification techniques are proposed in current literature which involve the use of metrics drawn from version control systems in order to classify software components as defect-prone or defect-free. In this paper, we create a novel genetic fuzzy rule-based system to efficiently model the defect-proneness of each component. The system uses a Mamdani-Assilian inference engine and models the problem as a one-class classification task. System rules are constructed using a genetic algorithm, where each chromosome represents a rule base (Pittsburgh approach). The parameters of our fuzzy system and the operators of the genetic algorithm are designed with regard to producing interpretable output. Thus, the output offers not only effective classification, but also a comprehensive set of rules that can be easily visualized to extract useful conclusions about the metrics of the software.},
	pages = {32--38},
	booktitle = {2015 {IEEE}/{ACM} 4th International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering},
	author = {Diamantopoulos, Themistoklis and Symeonidis, Andreas},
	date = {2015-05},
	keywords = {Software, Defects, Forecasting, Genetic algorithms, Software engineering, Software component, Software reliability, Artificial intelligence, Software fault prediction, Measurement, Fuzzy inference, Fuzzy systems, Fuzzy logic, Classification technique, Component analysis, Genetic fuzzy systems, One-class Classification, Pittsburgh approach, Version control system, software fault prediction, Genetics, defect-prone components, genetic fuzzy systems, Sociology, Software Reliability Prediction, xyes}
}

@inproceedings{liu_research_2019,
	title = {Research on Fault Prediction and Health Management Technology Based on Machine Learning},
	doi = {10.1109/QR2MSE46217.2019.9021182},
	abstract = {With the continuous development of fault detection and identification technology of complex equipment systems, the traditional fault identification technology has been slightly insufficient. Now artificial intelligence is applied to the field of large-scale information system fault identification. In order to find and judge information system faults more accurately, this paper proposes a fault identification method based on machine learning. Firstly, the fault data characteristics are analyzed and the fault information identification method is obtained. Then, based on the fault information feature words, the algorithm for correlating invalid rules is obtained. Based on the association failure model, the fault identification algorithm is obtained. Finally, the information system fault data management and fault identification prototype system is developed.},
	pages = {789--794},
	booktitle = {2019 International Conference on Quality, Reliability, Risk, Maintenance, and Safety Engineering ({QR}2MSE)},
	author = {Liu, Bojiang and Wu, Lijin and Han, Xinyu and Tang, Longli},
	date = {2019-08},
	keywords = {Predictive models, Software, Machine learning, Fault tolerance, machine learning, Training, fault prediction, Data models, association failure rule, Fault tolerant systems, feature word extraction, information system, xyes}
}

@article{cai_design_2019,
	title = {Design Rule Spaces: A New Model for Representing and Analyzing Software Architecture},
	volume = {45},
	issn = {1939-3520},
	doi = {10.1109/TSE.2018.2797899},
	abstract = {In this paper, we propose an architecture model called Design Rule Space ({DRSpace}). We model the architecture of a software system as multiple overlapping {DRSpaces}, reflecting the fact that any complex software system must contain multiple aspects, features, patterns, etc. We show that this model provides new ways to analyze software quality. In particular, we introduce an Architecture Root detection algorithm that captures {DRSpaces} containing large numbers of a project's bug-prone files, which are called Architecture Roots ({ArchRoots}). After investigating {ArchRoots} calculated from 15 open source projects, the following observations become clear: from 35 to 91 percent of a project's most bug-prone files can be captured by just 5 {ArchRoots}, meaning that bug-prone files are likely to be architecturally connected. Furthermore, these {ArchRoots} tend to live in the system for significant periods of time, serving as the major source of bug-proneness and high maintainability costs. Moreover, each {ArchRoot} reveals multiple architectural flaws that propagate bugs among files and this will incur high maintenance costs over time. The implication of our study is that the quality, in terms of bug-proneness, of a large, complex software project cannot be fundamentally improved without first fixing its architectural flaws.},
	pages = {657--682},
	number = {7},
	journaltitle = {{IEEE} Transactions on Software Engineering},
	author = {Cai, Yuanfang and Xiao, Lu and Kazman, Rick and Mo, Ran and Feng, Qiong},
	date = {2019-07},
	keywords = {Computer software selection and evaluation, Open source software, Program debugging, Reverse engineering, Code smell, Computer architecture, Technical debts, Defect prediction, Analytical models, Bug localizations, Computer bugs, Production facility, Software architecture, Software systems, defect prediction, bug localization, code smells, Production facilities, reverse-engineering, technical debt, xyes}
}

@inproceedings{lenarduzzi_are_2020,
	title = {Are {SonarQube} Rules Inducing Bugs?},
	doi = {10.1109/SANER48275.2020.9054821},
	abstract = {The popularity of tools for analyzing Technical Debt, and particularly the popularity of {SonarQube}, is increasing rapidly. {SonarQube} proposes a set of coding rules, which represent something wrong in the code that will soon be reflected in a fault or will increase maintenance effort. However, our local companies were not confident in the usefulness of the rules proposed by {SonarQube} and contracted us to investigate the fault-proneness of these rules. In this work we aim at understanding which {SonarQube} rules are actually fault-prone and to understand which machine learning models can be adopted to accurately identify fault-prone rules. We designed and conducted an empirical study on 21 well-known mature open-source projects. We applied the {SZZ} algorithm to label the fault-inducing commits. We analyzed the fault-proneness by comparing the classification power of seven machine learning models. Among the 202 rules defined for Java by {SonarQube}, only 25 can be considered to have relatively low fault-proneness. Moreover, violations considered as “bugs” by {SonarQube} were generally not fault-prone and, consequently, the fault-prediction power of the model proposed by {SonarQube} is extremely low. The rules applied by {SonarQube} for calculating technical debt should be thoroughly investigated and their harmfulness needs to be further confirmed. Therefore, companies should carefully consider which rules they really need to apply, especially if their goal is to reduce fault-proneness.},
	pages = {501--511},
	booktitle = {2020 {IEEE} 27th International Conference on Software Analysis, Evolution and Reengineering ({SANER})},
	author = {Lenarduzzi, Valentina and Lomio, Francesco and Huttunen, Heikki and Taibi, Davide},
	date = {2020-02},
	note = {{ISSN}: 1534-5351},
	keywords = {machine learning, code smells, architectural smells, coding style, {SonarQube}, static analysis, Technical Debt, xyes, xsonarqube},
	file = {Lenarduzzi et al. - 2020 - Are SonarQube Rules Inducing Bugs.pdf:C\:\\Users\\michalm\\Zotero\\storage\\2YIQLJ3W\\Lenarduzzi et al. - 2020 - Are SonarQube Rules Inducing Bugs.pdf:application/pdf}
}

@inproceedings{shaikh_attribute_2020,
	title = {Attribute Rule performance in Data Mining for Software Deformity Prophecy Datasets Models},
	doi = {10.1109/AECT47998.2020.9194187},
	abstract = {In recently, all the developers, programmer and software engineers, they are working specially on software component and software testing to compete the software technology in the world. For this competition, they are using different kind of sources to analysis the software reliability and importance. Nowadays Data mining is one of source, which is used in software for overcome the problem of software fault which occur during the software test and its analysis. This kind of problem leads software deformity prophecy in software. In this research paper, we are also trying to overcome the software deformity prophecy problem with the help of our proposed solution called {ONER} rule attribute. We have used {REPOSITORY} datasets models, these datasets models are defected and non-defected datasets models. Our analysis class of interest is defected models. In our research, we have analyzed the efficiency of our proposed solution methods. The experiments results showed that using of {ONER} with discretize, have improved the efficiency of correctly classified instances in all. Using percentage split and training datasets with {ONER} discretize rule attribute have improved correctly classified in all datasets models. The analysis of positive accuracy f-measure is also increased in percentage split during the use of {ONER} with discretize but in some datasets models, the training data and cross validation is better with use of {ONER} rule attribute. The area under curve ({ROC}) in both scenarios using {ONER} rule attribute and discretize with {ONER} rule attribute is almost same or equal with each other.},
	pages = {1--6},
	booktitle = {2019 International Conference on Advances in the Emerging Computing Technologies ({AECT})},
	author = {Shaikh, Salahuddin and Changan, Liu and Malik, Maaz Rasheed},
	date = {2020-02},
	keywords = {Data mining, Software, Testing, Measurement, Data Mining, Machine Learning, Data models, Object oriented modeling, Attribute Rule, Datasets Model, Deformable models, {ONER}, Software Deformity Prophecy, xyes},
	file = {Shaikh et al. - 2020 - Attribute Rule performance in Data Mining for Soft.pdf:C\:\\Users\\michalm\\Zotero\\storage\\LSYLLI85\\Shaikh et al. - 2020 - Attribute Rule performance in Data Mining for Soft.pdf:application/pdf}
}

@inproceedings{mutlu_new_2019,
	title = {A New Fuzzy Rule Generation Scheme based on Multiple-Selection of Influencing Factors},
	doi = {10.1109/FUZZ-IEEE.2019.8858990},
	abstract = {Automatic rule generation of fuzzy systems is a extensively studied topic which is generally handled by using a labeled dataset. This paper presents a new method for this problem which does not rely on training data. It only requests a tiny information which can be roughly acquired from any data and/or domain expert. By using this information, it selects single or multiple influencing factor(s) to determine the consequent part of rules. The experiments were performed on software fault prediction problem, and the resulting rules are compared with the rules obtained from 3 existing rule generation methods which are data-based, expert-based and partially data-based solutions. Results shows that, the proposed method is able to outperform its counterparts in most of the cases.},
	pages = {1--6},
	booktitle = {2019 {IEEE} International Conference on Fuzzy Systems ({FUZZ}-{IEEE})},
	author = {Mutlu, Begum and Sezer, Ebru A.},
	date = {2019-06},
	note = {{ISSN}: 1558-4739},
	keywords = {Software, Testing, Machine learning, Training data, Fuzzy systems, Fuzzy sets, Linguistics, software fault prediction, Fuzzy rule generation, fuzzy systems, xyes, xrbs},
	file = {Mutlu and Sezer - 2019 - A New Fuzzy Rule Generation Scheme based on Multip.pdf:C\:\\Users\\michalm\\Zotero\\storage\\BHGT3R57\\Mutlu and Sezer - 2019 - A New Fuzzy Rule Generation Scheme based on Multip.pdf:application/pdf}
}

@inproceedings{ibarguren_consolidated_2017,
	title = {The Consolidated Tree Construction algorithm in imbalanced defect prediction datasets},
	doi = {10.1109/CEC.2017.7969629},
	abstract = {In this short paper, we compare well-known rule/tree classifiers in software defect prediction with the {CTC} decision tree classifier designed to deal with class imbalanced. It is well-known that most software defect prediction datasets are highly imbalance (non-defective instances outnumber defective ones). In this work, we focused only on tree/rule classifiers as these are capable of explaining the decision, i.e., describing the metrics and thresholds that make a module error prone. Furthermore, rules/decision trees provide the advantage that they are easily understood and applied by project managers and quality assurance personnel. The {CTC} algorithm was designed to cope with class imbalance and noisy datasets instead of using preprocessing techniques (oversampling or undersampling), ensembles or cost weights of misclassification. The experimental work was carried out using the {NASA} datasets and results showed that induced {CTC} decision trees performed better or similar to the rest of the rule/tree classifiers.},
	pages = {2656--2660},
	booktitle = {2017 {IEEE} Congress on Evolutionary Computation ({CEC})},
	author = {Ibarguren, Igor and Pérez, Jesús M. and Mugerza, Javier and Rodriguez, Daniel and Harrison, Rachel},
	date = {2017-06},
	keywords = {Software, {NASA}, Decision trees, Measurement, Algorithm design and analysis, Prediction algorithms, Software algorithms, xyes},
	file = {Submitted Version:C\:\\Users\\michalm\\Zotero\\storage\\KMXS79CF\\Ibarguren et al. - 2017 - The Consolidated Tree Construction algorithm in im.pdf:application/pdf}
}

@inproceedings{danphitsanuphan_code_2012,
	title = {Code Smell Detecting Tool and Code Smell-Structure Bug Relationship},
	doi = {10.1109/SCET.2012.6342082},
	abstract = {This paper proposes an approach for detecting the so- called bad smells in software known as Code Smell. In considering software bad smells, object-oriented software metrics were used to detect the source code whereby Eclipse Plugins were developed for detecting in which location of Java source code the bad smell appeared so that software refactoring could then take place. The detected source code was classified into 7 types: Large Class, Long Method, Parallel Inheritance Hierarchy, Long Parameter List, Lazy Class, Switch Statement, and Data Class. This work conducted analysis by using 323 java classes to ascertain the relationship between the code smell and structural defects of software by using the data mining techniques of Naive Bayes and Association Rules. The result of the Naive Bayes test showed that the Lazy Class caused structural defects in {DLS}, {DE}, and Se. Also, Data Class caused structural defects in {UwF}, {DE}, and Se, while Long Method, Large Class, Data Class, and Switch Statement caused structural defects in {UwF} and Se. Finally, Parallel Inheritance Hierarchy caused structural defects in Se. However, Long Parameter List caused no structural defects whatsoever. The results of the Association Rules test found that the Lazy Class code smell caused structural defects in {DLS} and {DE}, which corresponded to the results of the Naive Bayes test.},
	pages = {1--5},
	booktitle = {2012 Spring Congress on Engineering and Technology},
	author = {Danphitsanuphan, Phongphan and Suwantada, Thanitta},
	date = {2012-05},
	keywords = {Software, Software metrics, Association rules, Computer bugs, Educational institutions, Java, xyes, xassociation-rules},
	file = {Danphitsanuphan and Suwantada - 2012 - Code Smell Detecting Tool and Code Smell-Structure.pdf:C\:\\Users\\michalm\\Zotero\\storage\\IHL3YBSW\\Danphitsanuphan and Suwantada - 2012 - Code Smell Detecting Tool and Code Smell-Structure.pdf:application/pdf}
}

@inproceedings{liu_rule_2010,
	title = {Rule Engine based on improvement Rete algorithm},
	doi = {10.1109/ICACIA.2010.5709916},
	abstract = {Rete algorithm is the mainstream of the algorithm in the rules engine; it provides efficient local entities data and rules pattern matching method. But Rete algorithm applied in rules engine has defects in performance and demand aspect, this paper applies three methods to improve the Rete algorithm in the rule engine: rule decomposition, Alpha-Node-Hashing and Beta-Node-Indexing. Rules engine is enterprise applications, business logic framework for extracting rules from software application, and make it more flexible. This paper describes the principle of Rule Engine at first, secondly show the Rete algorithm, and finally we do some improvement to Rete algorithm. It makes Rule Engine widely meet the application demand and be greatly improved the efficiency.},
	pages = {346--349},
	booktitle = {The 2010 International Conference on Apperceiving Computing and Intelligence Analysis Proceeding},
	author = {Liu, Di and Gu, Tao and Xue, Jiang-Ping},
	date = {2010-12},
	keywords = {Pattern matching, Algorithm design and analysis, Classification algorithms, Software algorithms, Alpha-Node-Hashing, Beta-Node-Indexing, Business, Engines, Production, Rete algorithm, rule decomposition, Rule Engine, xyes}
}

@article{xu_defect_2021,
	title = {Defect Prediction With Semantics and Context Features of Codes Based on Graph Representation Learning},
	volume = {70},
	issn = {1558-1721},
	doi = {10.1109/TR.2020.3040191},
	abstract = {To optimize the process of software testing and to improve software quality and reliability, many attempts have been made to develop more effective methods for predicting software defects. Previous work on defect prediction has used machine learning and artificial software metrics. Unfortunately, artificial metrics are unable to represent the features of syntactic, semantic, and context information of defective modules. In this article, therefore, we propose a practical approach for identifying software defect patterns via the combination of semantics and context information using abstract syntax tree representation learning. Graph neural networks are also leveraged to capture the latent defect information of defective subtrees, which are pruned based on a fix-inducing change. To validate the proposed approach for predicting defects, we define mining rules based on the {GitHub} workflow and collect 6052 defects from 307 projects. The experiments indicate that the proposed approach performs better than the state-of-the-art approach and five traditional machine learning baselines. An ablation study shows that the information about code concepts leads to a significant increase in accuracy.},
	pages = {613--625},
	number = {2},
	journaltitle = {{IEEE} Transactions on Reliability},
	author = {Xu, Jiaxi and Wang, Fei and Ai, Jun},
	date = {2021-06},
	keywords = {Data mining, Software, software engineering, Semantics, Measurement, Computer bugs, Deep learning, defect prediction, Software development management, graph representation learning, software defect dataset, Syntactics, xyes}
}

@inproceedings{elberzhager_optimizing_2014,
	title = {Optimizing Quality Assurance Strategies through an Integrated Quality Assurance Approach – Guiding Quality Assurance with Assumptions and Selection Rules},
	doi = {10.1109/SEAA.2014.12},
	abstract = {Quality assurance activities are often still expensive or do not offer the expected quality. A recent trend aimed at overcoming this problem is tighter integration of several quality assurance techniques such as analysis and testing in order to exploit synergy effects and thus reduce costs or improve the coverage of quality assurance activities. However, one main challenge in exploiting such benefits is that knowledge about the relationships between many different factors is needed, such as the quality assurance techniques considered, the number of defects, the remaining defect-proneness, or product and budget data. Such knowledge is often not available. Based on a combined analysis and testing methodology called In {QA}, we developed an iterative rule-based procedure that considers several factors in order to gather knowledge and allows deriving different strategies to guide the quality assurance activities. We derived several specific and reasonable strategies to demonstrate the approach.},
	pages = {402--405},
	booktitle = {2014 40th {EUROMICRO} Conference on Software Engineering and Advanced Applications},
	author = {Elberzhager, Frank and Bauer, Thomas},
	date = {2014-08},
	note = {{ISSN}: 2376-9505},
	keywords = {Testing, Defects, Forecasting, Quality assurance, Iterative methods, Software engineering, Budget control, Application programs, Knowledge based systems, Defect proneness, Integration testing, Analysis and testing, Quality control, Integration, analysis, assumptions, Combined analysis, Electronic guidance systems, Integrated quality, Quality assurance strategies, rules, Inspection, prediction, Context, Calibration, Concrete, guidance, integration, testing, tool prototype, xyes}
}

@inproceedings{wu_evolutionary_2011,
	title = {An evolutionary approach to evaluate the quality of software systems},
	doi = {10.1109/IWACI.2011.6160036},
	abstract = {Modern software applications are characterized as large, complex, and component-based systems. These applications can be viewed as modeling solutions that are created to cope with daily living in both the public and the private organizations, as well as in every business enterprise. A model solution is subject to evolutionary improvement; the more the improvement, the better the quality of software. An improvement can be carried out by means of defect prediction at the component level of the software systems. This paper discusses an evolutionary computing approach to model defects in complex adaptive software systems based on mathematical elements, graphs, sets, and rough sets, in addition to domain specific rules that are necessary for defect collections and defect analyses. This approach is applied to the evaluation of software quality, and it is fundamental for automation of such an evaluation tool.},
	pages = {381--386},
	booktitle = {The Fourth International Workshop on Advanced Computational Intelligence},
	author = {Wu, Binghui Helen},
	date = {2011-10},
	keywords = {Software quality, Analytical models, Software architecture, Software systems, Programming, Rough sets, xyes},
	file = {Wu - 2011 - An evolutionary approach to evaluate the quality o.pdf:C\:\\Users\\michalm\\Zotero\\storage\\NIWVQNDM\\Wu - 2011 - An evolutionary approach to evaluate the quality o.pdf:application/pdf}
}

@article{rajapaksha_sqaplanner_2021,
	title = {{SQAPlanner}: Generating Data-Informed Software Quality Improvement Plans},
	issn = {1939-3520},
	doi = {10.1109/TSE.2021.3070559},
	abstract = {Software Quality Assurance ({SQA}) planning aims to define proactive plans, such as defining maximum file size, to prevent the occurrence of software defects in future releases. To aid this, defect prediction models have been proposed to generate insights as the most important factors that are associated with software quality. Such insights that are derived from traditional defect models are far from actionable—i.e., practitioners still do not know what they should do or avoid to decrease the risk of having defects, and what is the risk threshold for each metric. A lack of actionable guidance and risk threshold can lead to inefficient and ineffective {SQA} planning processes. In this paper, we investigate the practitioners' perceptions of current {SQA} planning activities, current challenges of such {SQA} planning activities, and propose four types of guidance to support {SQA} planning. We then propose and evaluate our {AI}-Driven {SQAPlanner} approach, a novel approach for generating four types of guidance and their associated risk thresholds in the form of rule-based explanations for the predictions of defect prediction models. Finally, we develop and evaluate a visualization for our {SQAPlanner} approach. Through the use of qualitative survey and empirical evaluation, our results lead us to conclude that {SQAPlanner} is needed, effective, stable, and practically applicable. We also find that 80\% of our survey respondents perceived that our visualization is more actionable. Thus, our {SQAPlanner} paves a way for novel research in actionable software analytics—i.e., generating actionable guidance on what should practitioners do and not do to decrease the risk of having defects to support {SQA} planning.},
	pages = {1--1},
	journaltitle = {{IEEE} Transactions on Software Engineering},
	author = {Rajapaksha, Dilini and Tantithamthavorn, Chakkrit and Bergmeir, Christoph and Buntine, Wray and Jiarpakdee, Jirayus and Grundy, John},
	date = {2021},
	keywords = {Predictive models, Software, Computer software selection and evaluation, Defects, Software quality, Software defects, Defect prediction models, Forecasting, Predictive analytics, Air navigation, Defect model, Empirical evaluations, Planning process, Qualitative surveys, Quality assurance, Risk threshold, Risks, Software quality improvements, Surveys, Visualization, Artificial intelligence, Tools, Actionable Software Analytics, Explainable {AI}, Planning, Software Quality Assurance, {SQA} Planning, xyes},
	file = {Submitted Version:C\:\\Users\\michalm\\Zotero\\storage\\L4PA8RXG\\Rajapaksha et al. - 2021 - SQAPlanner Generating Data-Informed Software Qual.pdf:application/pdf}
}

@inproceedings{menzies_local_2011,
	title = {Local vs. global models for effort estimation and defect prediction},
	doi = {10.1109/ASE.2011.6100072},
	abstract = {Data miners can infer rules showing how to improve either (a) the effort estimates of a project or (b) the defect predictions of a software module. Such studies often exhibit conclusion instability regarding what is the most effective action for different projects or modules. This instability can be explained by data heterogeneity. We show that effort and defect data contain many local regions with markedly different properties to the global space. In other words, what appears to be useful in a global context is often irrelevant for particular local contexts. This result raises questions about the generality of conclusions from empirical {SE}. At the very least, {SE} researchers should test if their supposedly general conclusions are valid within subsets of their data. At the very most, empirical {SE} should become a search for local regions with similar properties (and conclusions should be constrained to just those regions).},
	pages = {343--351},
	booktitle = {2011 26th {IEEE}/{ACM} International Conference on Automated Software Engineering ({ASE} 2011)},
	author = {Menzies, Tim and Butcher, Andrew and Marcus, Andrian and Zimmermann, Thomas and Cok, David},
	date = {2011-11},
	note = {{ISSN}: 1938-4300},
	keywords = {Data mining, Software, Defects, Software engineering, Software modules, Defect prediction, Principal component analysis, Data miners, Effort Estimation, Estimation, Global models, validation, Context, Couplings, defect/effort estimation, empirical {SE}, Runtime, {USA} Councils, xyes, xdataset}
}

@article{maddeh_decision_2021,
	title = {Decision tree-based Design Defects Detection},
	volume = {9},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2021.3078724},
	abstract = {Design defects affect project quality and hinder development and maintenance. Consequently, experts need to minimize these defects in software systems. A promising approach is to apply the concepts of refactoring at higher level of abstraction based on {UML} diagrams instead of code level. Unfortunately, we find in literature many defects that are described textually and there is no consensus on how to decide if a particular design violates model quality. Defects could be quantified as metrics based rules that represent a combination of software metrics. However, it is difficult to find manually the best threshold values for these metrics. In this paper, we propose a new approach to identify design defects at the model level using the {ID}3 decision tree algorithm. We aim to create a decision tree for each defect. We experimented our approach on four design defects: The Blob, Data class, Lazy class and Feature Envy defect, using 15 Object-Oriented metrics. The rules generated using decision tree give a very promising detection results for the four open source projects tested in this paper. In Lucene 1.4 project, we found that the precision is 67\% for a recall of 100\%. In general, the accuracy varies from 49\%, reaching for Lucene 1.4 project 80\%.},
	pages = {71606--71614},
	journaltitle = {{IEEE} Access},
	author = {Maddeh, Mohamed and Ayouni, Sarra and Alyahya, Sultan and Hajjej, Fahima},
	date = {2021},
	keywords = {Predictive models, Software, Decision trees, Anti-patterns, Measurement, Software algorithms, Object oriented modeling, Unified modeling language, bad smells, decision tree, model refactoring, object oriented metrics, xyes},
	file = {Full Text:C\:\\Users\\michalm\\Zotero\\storage\\VI9T3DA7\\Maddeh et al. - 2021 - Decision tree-based Design Defects Detection.pdf:application/pdf}
}

@inproceedings{mutlu_end--end_2018,
	title = {End-to-End Hierarchical Fuzzy Inference Solution},
	doi = {10.1109/FUZZ-IEEE.2018.8491481},
	abstract = {Hierarchical Fuzzy System ({HFS}) is a popular approach for handling curse of dimensionality problem occurred in complex fuzzy rule-based systems with various and numerous inputs. However, the processes of modeling and reasoning of {HFS} have some critical issues to be considered. In this study, the effect of these issues on the accuracy and stability of the resulting system has been investigated, and an end-to-end {HFS} framework has been proposed. The proposed framework has three main steps such as single system modeling, rule partitioning and {HFS} reasoning. It is fully automated, generic, almost independent from data, and applicable for any kind of inference problem. In addition, the proposed framework preserves accuracy and stability during the {HFS} reasoning. These judgments have been ensured by a number of experimental studies on several datasets about software faulty prediction ({SFP}) problem with a large feature space. The main contributions of this paper are as follows: (i) it provides the entire {HFS} implementation from problem definition to calculation of final output, (ii) it increases the accuracy of recently proposed rule generation scheme in the literature, (iii) it presents the only possible fuzzy system solution for {SFP} problem containing a large feature space with reasonable accuracy.},
	pages = {1--9},
	booktitle = {2018 {IEEE} International Conference on Fuzzy Systems ({FUZZ}-{IEEE})},
	author = {Mutlu, Begum and Sezer, Ebru A. and Ali Akcayol, M.},
	date = {2018-07},
	keywords = {Fuzzy inference, Fuzzy systems, Fuzzy sets, Decision making, Critical issues, Curse of dimensionality, Fully automated, Hierarchical fuzzy, Hierarchical fuzzy systems, Hierarchical systems, Inference problem, Problem definition, Reasonable accuracy, Cognition, Production, Hafnium, Stability analysis, xyes}
}

@article{miholca_comet_2020,
	title = {{COMET}: A conceptual coupling based metrics suite for software defect prediction},
	volume = {176},
	issn = {1877-0509},
	url = {https://www.sciencedirect.com/science/article/pii/S1877050920318287},
	doi = {https://doi.org/10.1016/j.procs.2020.08.004},
	abstract = {Identifying defective software components is an essential activity during software development which contributes to continuously improving the software quality. Since relatively numerous defects are due to violated software dependencies, coupling metrics could increase the performance of software defect prediction. Among various measures expressing the coupling between software components, the conceptual coupling metrics capture similarities based on the semantic information contained in the source code. We are introducing a new conceptual coupling based metric suite, named {COMET}, for software defect prediction. Experiments conducted on publicly available data sets, using both unsupervised and supervised learning models, emphasize that {COMET} metrics suite is superior to the software metrics widely used in the defect prediction literature.},
	pages = {31--40},
	journaltitle = {Procedia Computer Science},
	author = {Miholca, Diana-Lucia and Czibula, Gabriela and Tomescu, Vlad},
	date = {2020},
	keywords = {Software metrics, Software defect prediction, Machine learning, 03B52, 68T35, Conceptual coupling, Source code 2000 {MSC}: 68T05, xyes}
}

@article{phan_automatically_2018,
	title = {Automatically classifying source code using tree-based approaches},
	volume = {114},
	issn = {0169-023X},
	url = {https://www.sciencedirect.com/science/article/pii/S0169023X17300344},
	doi = {https://doi.org/10.1016/j.datak.2017.07.003},
	abstract = {Analyzing source code to solve software engineering problems such as fault prediction, cost, and effort estimation always receives attention of researchers as well as companies. The traditional approaches are based on machine learning, and software metrics obtained by computing standard measures of software projects. However, these methods have faced many challenges due to limitations of using software metrics which were not enough to capture the complexity of programs. To overcome the limitations, this paper aims to solve software engineering problems by exploring information of programs' abstract syntax trees ({ASTs}) instead of software metrics. We propose two combination models between a tree-based convolutional neural network ({TBCNN}) and k-Nearest Neighbors ({kNN}), support vector machines ({SVMs}) to exploit both structural and semantic {ASTs}' information. In addition, to deal with high-dimensional data of {ASTs}, we present several pruning tree techniques which not only reduce the complexity of data but also enhance the performance of classifiers in terms of computational time and accuracy. We survey many machine learning algorithms on different types of program representations including software metrics, sequences, and tree structures. The approaches are evaluated based on classifying 52000 programs written in C language into 104 target labels. The experiments show that the tree-based classifiers dramatically achieve high performance in comparison with those of metrics-based or sequences-based; and two proposed models {TBCNN} + {SVM} and {TBCNN} + {kNN} rank as the top and the second classifiers. Pruning redundant {AST} branches leads to not only a substantial reduction in execution time but also an increase in accuracy.},
	pages = {12--25},
	journaltitle = {Data \& Knowledge Engineering},
	author = {Phan, Anh Viet and Chau, Phuong Ngoc and Nguyen, Minh Le and Bui, Lam Thu},
	date = {2018},
	keywords = {Abtract Syntax Tree ({AST}), K-Nearest Neighbors ({kNN}), Support Vector Machines ({SVMs}), Tree-based convolutional neural networks({TBCNN}), xyes},
	file = {Phan et al. - 2018 - Automatically classifying source code using tree-b.pdf:C\:\\Users\\michalm\\Zotero\\storage\\LWB2FQJU\\Phan et al. - 2018 - Automatically classifying source code using tree-b.pdf:application/pdf}
}

@article{qiu_understanding_2017,
	title = {Understanding the syntactic rule usage in java},
	volume = {123},
	issn = {0164-1212},
	url = {https://www.sciencedirect.com/science/article/pii/S0164121216302126},
	doi = {https://doi.org/10.1016/j.jss.2016.10.017},
	abstract = {Context: Syntax is fundamental to any programming language: syntax defines valid programs. In the 1970s, computer scientists rigorously and empirically studied programming languages to guide and inform language design. Since then, language design has been artistic, driven by the aesthetic concerns and intuitions of language architects. Despite recent studies on small sets of selected language features, we lack a comprehensive, quantitative, empirical analysis of how modern, real-world source code exercises the syntax of its programming language. Objective: This study aims to understand how programming language syntax is employed in actual development and explore their potential applications based on the results of syntax usage analysis. Method: We present our results on the first such study on Java, a modern, mature, and widely-used programming language. Our corpus contains over 5000 open-source Java projects, totalling 150 million source lines of code ({SLoC}). We study both independent (i.e. applications of a single syntax rule) and dependent (i.e. applications of multiple syntax rules) rule usage, and quantify their impact over time and project size. Results: Our study provides detailed quantitative information and yields insight, particularly (i) confirming the conventional wisdom that the usage of syntax rules is Zipfian; (ii) showing that the adoption of new rules and their impact on the usage of pre-existing rules vary significantly over time; and (iii) showing that rule usage is highly contextual. Conclusions: Our findings suggest potential applications across language design, code suggestion and completion, automatic syntactic sugaring, and language restriction.},
	pages = {160--172},
	journaltitle = {Journal of Systems and Software},
	author = {Qiu, Dong and Li, Bixin and Barr, Earl T. and Su, Zhendong},
	date = {2017},
	keywords = {Empirical study, Language syntax, Practical language usage, xyes}
}

@article{palomba_large-scale_2018,
	title = {A large-scale empirical study on the lifecycle of code smell co-occurrences},
	volume = {99},
	issn = {0950-5849},
	url = {https://www.sciencedirect.com/science/article/pii/S0950584918300211},
	doi = {https://doi.org/10.1016/j.infsof.2018.02.004},
	abstract = {Context Code smells are suboptimal design or implementation choices made by programmers during the development of a software system that possibly lead to low code maintainability and higher maintenance costs. Objective Previous research mainly studied the characteristics of code smell instances affecting a source code file, while only few studies analyzed the magnitude and effects of smell co-occurrence, i.e., the co-occurrence of different types of smells on the same code component. This paper aims at studying in details this phenomenon. Method We analyzed 13 code smell types detected in 395 releases of 30 software systems to firstly assess the extent to which code smells co-occur, and then we analyze (i) which code smells co-occur together, and (ii) how and why they are introduced and removed by developers. Results 59\% of smelly classes are affected by more than one smell, and in particular there are six pairs of smell types (e.g., Message Chains and Spaghetti Code) that frequently co-occur. Furthermore, we observed that method-level code smells may be the root cause for the introduction of class-level smells. Finally, code smell co-occurrences are generally removed together as a consequence of other maintenance activities causing the deletion of the affected code components (with a consequent removal of the code smell instances) as well as the result of a major restructuring or scheduled refactoring actions. Conclusions Based on our findings, we argue that more research aimed at designing co-occurrence-aware code smell detectors and refactoring approaches is needed.},
	pages = {1--10},
	journaltitle = {Information and Software Technology},
	author = {Palomba, Fabio and Bavota, Gabriele and Penta, Massimiliano Di and Fasano, Fausto and Oliveto, Rocco and Lucia, Andrea De},
	date = {2018},
	keywords = {Mining software repositories, Empirical study, Code smells co-occurrences, xyes},
	file = {Palomba et al. - 2018 - A large-scale empirical study on the lifecycle of .pdf:C\:\\Users\\michalm\\Zotero\\storage\\AUFLGEZ2\\Palomba et al. - 2018 - A large-scale empirical study on the lifecycle of .pdf:application/pdf}
}